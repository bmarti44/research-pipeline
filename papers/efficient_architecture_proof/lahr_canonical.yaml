# LAHR Canonical Configuration
# This is the authoritative config for all experiments

# =============================================================================
# Model Architecture
# =============================================================================

model:
  name: "lahr"
  version: "v4"  # Use lahr_v4.py - the canonical implementation

  # Base model dimensions
  vocab_size: 50257  # GPT-2 tokenizer
  d_model: 256  # Default for "small" scale
  n_heads: 8
  ff_mult: 4
  dropout: 0.1
  max_seq_len: 512

  # Mixture-of-Depths
  n_layers: 12
  mod_capacity: 0.125  # 12.5% of tokens processed per MoD layer
  mod_every_n: 2  # Apply MoD to layers 1, 3, 5, 7, 9, 11

  # Latent Reasoning (COCONUT-style)
  use_latent_reasoning: true
  n_latent_iterations: 4

  # Memory
  use_memory: true
  n_memory_slots: 128
  memory_top_k: 8

# =============================================================================
# Model Scales
# =============================================================================

scales:
  tiny:
    d_model: 128
    n_heads: 4
    n_layers: 6
    max_seq_len: 256
    n_memory_slots: 64
    estimated_params: "~1M"

  small:
    d_model: 256
    n_heads: 8
    n_layers: 8
    max_seq_len: 512
    n_memory_slots: 128
    estimated_params: "~10M"

  medium:
    d_model: 512
    n_heads: 8
    n_layers: 12
    max_seq_len: 1024
    n_memory_slots: 256
    estimated_params: "~50M"

  base:
    d_model: 768
    n_heads: 12
    n_layers: 12
    max_seq_len: 1024
    n_memory_slots: 512
    estimated_params: "~125M"

# =============================================================================
# Ablation Conditions (2^3 Factorial Design)
# =============================================================================

ablation:
  # Full factorial: 2^3 = 8 conditions
  conditions:
    - name: "full"
      mod: true
      latent: true
      memory: true
      description: "Full LAHR with all components"

    - name: "no_mod"
      mod: false
      latent: true
      memory: true
      description: "Without Mixture-of-Depths"

    - name: "no_latent"
      mod: true
      latent: false
      memory: true
      description: "Without latent reasoning"

    - name: "no_memory"
      mod: true
      latent: true
      memory: false
      description: "Without memory module"

    - name: "mod_only"
      mod: true
      latent: false
      memory: false
      description: "Only Mixture-of-Depths"

    - name: "latent_only"
      mod: false
      latent: true
      memory: false
      description: "Only latent reasoning"

    - name: "memory_only"
      mod: false
      latent: false
      memory: true
      description: "Only memory module"

    - name: "baseline"
      mod: false
      latent: false
      memory: false
      description: "Standard transformer (no innovations)"

  # Statistical requirements
  seeds_per_condition: 5  # Minimum for variance estimation
  primary_metric: "val_perplexity"
  secondary_metrics:
    - "throughput_tokens_per_sec"
    - "flops_per_token"
    - "latent_iterations"
    - "memory_utilization"

# =============================================================================
# Training Configuration
# =============================================================================

training:
  # Hardware (MacBook M2/M3 Max with 36GB unified memory)
  device: "mps"
  dtype: "float32"  # MPS doesn't support bf16 well

  # Batch settings
  batch_size: 4
  gradient_accumulation: 8  # Effective batch = 32
  seq_len: 512

  # Optimizer
  learning_rate: 3.0e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  grad_clip: 1.0

  # Schedule
  warmup_steps: 1000
  max_steps: 50000
  lr_schedule: "cosine"

  # Checkpointing
  eval_interval: 1000
  save_interval: 5000
  log_interval: 100

  # Reproducibility
  seed: 42

# =============================================================================
# Data Configuration
# =============================================================================

data:
  # Primary dataset
  dataset: "TinyStories"  # Small, well-studied, good for architecture validation
  tokenizer: "gpt2"

  # Data paths (relative to paper directory)
  train_path: "data/train.bin"
  val_path: "data/val.bin"

  # Processing
  max_tokens: 10_000_000  # 10M tokens for initial experiments

# =============================================================================
# Evaluation Configuration
# =============================================================================

evaluation:
  datasets:
    - name: "val_perplexity"
      split: "validation"
      metric: "perplexity"

  # KNOWN LIMITATION: Latent reasoning fails on arithmetic
  # Document but don't evaluate on GSM8K - would be misleading
  excluded:
    - "gsm8k"  # COCONUT achieves 34% vs 42% CoT - not competitive

# =============================================================================
# Statistical Analysis Plan
# =============================================================================

statistics:
  # Primary hypothesis
  primary:
    hypothesis: "Full LAHR achieves lower perplexity per FLOP than baseline"
    test: "paired_t_test"
    comparison: ["full", "baseline"]
    alpha: 0.05
    effect_size_threshold: 0.3  # Cohen's d

  # Secondary hypotheses (Holm-Bonferroni correction)
  secondary:
    - hypothesis: "MoD provides FLOP efficiency"
      test: "paired_t_test"
      comparison: ["no_mod", "full"]

    - hypothesis: "Latent reasoning improves logical tasks"
      test: "paired_t_test"
      comparison: ["no_latent", "full"]

    - hypothesis: "Memory provides benefit"
      test: "paired_t_test"
      comparison: ["no_memory", "full"]

  # Interaction effects (exploratory, not confirmatory)
  exploratory:
    - "3-way ANOVA for component interactions"
    - "Effect size confidence intervals"

  # Multiple comparison correction
  correction: "holm_bonferroni"

# =============================================================================
# Reproducibility Checklist
# =============================================================================

reproducibility:
  checklist:
    - "requirements.txt with pinned versions"
    - "Random seeds set before data loading"
    - "Model initialization seeds documented"
    - "Data preparation is deterministic"
    - "Checkpoints saved at regular intervals"
    - "Full config logged with each run"

  code_version: "v4"
  canonical_model: "code/models/lahr_v4.py"
  canonical_config: "lahr_canonical.yaml"
