title: "The Curriculum Is the Mechanism: Dissecting COCONUT's Latent Thought Gains on ProsQA"
short_title: "COCONUT: Curriculum vs Mechanism"

authors:
  - name: "Brian Martin"
    affiliation: "Independent"

abstract: >
  Meta's COCONUT (Chain of Continuous Thought) replaces explicit chain-of-thought
  with continuous hidden states recycled as input embeddings, achieving 97% accuracy
  on ProsQA graph traversal. We train a compute-matched pause-token control (M5) that
  receives the same 7-stage curriculum and same number of thought positions but uses a
  single learned embedding instead of recycled hidden states. M5 matches COCONUT on
  in-distribution accuracy (97.3% vs 97.3% validation; 95.6% vs 98.0% test) and shows
  identical corruption profiles: same degradation cliff at position 4, zero permutation
  sensitivity, and successful cross-problem transplantation. On out-of-distribution
  generalization, M5 outperforms COCONUT on 3 of 4 test sets by 7-9 percentage points
  (McNemar p=0.001 on 7-hop after Bonferroni correction). Linear probes reveal zero
  step-specific selectivity in both models, with COCONUT encoding more task information
  at thought positions (10.5% vs 4.0% advantage over input positions) that does not
  translate to better performance. The training curriculum drives COCONUT's gains on
  ProsQA; the continuous thought mechanism does not.

keywords:
  - latent reasoning
  - chain of thought
  - continuous thought
  - COCONUT
  - ProsQA
  - graph traversal
  - curriculum learning
  - pause tokens

venue: "Preprint"
date: "2026-02-12"
version: "1.0"

research_question: >
  Does COCONUT's performance on ProsQA come from the continuous thought mechanism
  (hidden states recycled as inputs) or from the training curriculum (progressive
  CoT removal providing extra compute)?

hypothesis:
  primary: >
    A compute-matched pause-token control trained with the same curriculum will match
    COCONUT's in-distribution accuracy and corruption profiles, demonstrating that
    the curriculum, not the mechanism, drives the gains.
  outcome: "Supported"

models:
  - id: M1
    name: "CoT Baseline"
    architecture: "GPT-2 124M"
    description: "Standard chain-of-thought fine-tuning"
  - id: M2
    name: "COCONUT"
    architecture: "Coconut-wrapped GPT-2 124M"
    description: "Meta's continuous thought with hidden-state recycling"
  - id: M3
    name: "Pause-Curriculum"
    architecture: "GPT-2 124M"
    description: "Learned pause embedding with same 7-stage curriculum"
  - id: M4
    name: "Pause-Multipass"
    architecture: "GPT-2 124M"
    description: "Learned pause embedding with 6 sequential passes (same processing structure as COCONUT)"

data:
  training:
    dataset: "ProsQA (Meta's COCONUT repo)"
    samples: 17886
    format: "Graph traversal, 3-6 hops, nonsense entities"
  validation:
    samples: 300
  test:
    samples: 500
  ood:
    - name: "7-hop"
      samples: 1000
      novelty: "Hop count beyond training range"
    - name: "8-hop"
      samples: 1000
      novelty: "Hop count beyond training range"
    - name: "DAG"
      samples: 1000
      novelty: "DAG topology (training uses trees)"
    - name: "Dense"
      samples: 1000
      novelty: "Higher connectivity"

training:
  base_model: "openai-community/gpt2"
  parameters: 124000000
  optimizer: "AdamW"
  learning_rate: 0.0001
  weight_decay: 0.01
  batch_size_effective: 128
  epochs: 50
  precision: "fp32"
  seed: 0
  curriculum:
    stages: 7
    epochs_per_stage: 5
    max_latent_stage: 6

results:
  in_distribution:
    M1_test: 0.830
    M2_test: 0.980
    M3_test: 0.956
    M4_test: 0.948
    M2_val: 0.973
    M3_val: 0.973
    M4_val: 0.967
    M4_best_epoch: 30
  corruption:
    permutation_sensitivity_M2: 0.0
    permutation_sensitivity_M3: 0.0
    cross_transplant_M2: 0.970
    cross_transplant_M3: 0.965
    cliff_position: 4
    M4_all_positions_critical: true  # 2.4% accuracy when any single position corrupted
  probing:
    M2_peak_accuracy: 0.554
    M3_peak_accuracy: 0.571
    M2_peak_layer: 0
    M3_peak_layer: 12
    M2_selectivity: 0.52
    M3_selectivity: 0.52
    M2_thought_advantage: 0.105
    M3_thought_advantage: 0.040
    M4_note: "Non-reportable â€” extraction artifact (cold-start forward pass without KV-cache)"
  ood:
    7hop: { M1: 0.107, M2: 0.660, M3: 0.754, M4: 0.769 }
    8hop: { M1: 0.082, M2: 0.675, M3: 0.751, M4: 0.752 }
    dag:  { M1: 0.282, M2: 0.592, M3: 0.519, M4: 0.598 }
    dense: { M1: 0.141, M2: 0.612, M3: 0.684, M4: 0.648 }
  factorial_decomposition:
    # M4 vs M2 isolates recycled content (same sequential processing, different content)
    M4_vs_M2_7hop: { diff_pp: 10.9, p_bonf: 0.000, significant: true, direction: "M4>M2" }
    M4_vs_M2_8hop: { diff_pp: 7.7, p_bonf: 0.000, significant: true, direction: "M4>M2" }
    M4_vs_M2_dag:  { diff_pp: 0.6, p_bonf: 1.000, significant: false }
    M4_vs_M2_dense: { diff_pp: 3.6, p_bonf: 0.280, significant: false }
    # M4 vs M3 isolates sequential processing (same fixed content, different processing)
    M4_vs_M3_7hop: { diff_pp: 1.5, p_bonf: 1.000, significant: false }
    M4_vs_M3_8hop: { diff_pp: 0.1, p_bonf: 1.000, significant: false }
    M4_vs_M3_dag:  { diff_pp: 7.9, p_bonf: 0.000, significant: true, direction: "M4>M3" }
    M4_vs_M3_dense: { diff_pp: -3.6, p_bonf: 0.306, significant: false }
  wilcoxon_teacher_forced:
    M2_vs_M4_id: { r: 0.678, p: 5.52e-52, direction: "M2>M4" }
    M3_vs_M4_id: { r: 0.286, p: 1.61e-10, direction: "M3>M4" }

statistical_tests:
  # M3 vs M2 (original two-model comparison)
  mcnemar_7hop:
    chi2: 10.107
    p_raw: 0.00148
    p_bonferroni: 0.00739
    significant: true
  mcnemar_8hop:
    chi2: 6.643
    p_raw: 0.00995
    p_bonferroni: 0.04977
    significant: true
  mcnemar_dag:
    chi2: 5.076
    p_raw: 0.02425
    p_bonferroni: 0.12126
    significant: false
  mcnemar_dense:
    chi2: 5.340
    p_raw: 0.02084
    p_bonferroni: 0.10420
    significant: false
  # M4 vs M2 ID (recycled content test)
  mcnemar_M4_vs_M2_id:
    diff_pp: -2.2
    p_raw: 0.071
    p_bonferroni: 0.354
    significant: false
  # M4 vs M3 ID (sequential processing test)
  mcnemar_M4_vs_M3_id:
    diff_pp: -1.8
    p_raw: 0.136
    p_bonferroni: 0.680
    significant: false

conclusion: >
  The training curriculum, not the continuous thought mechanism, drives COCONUT's
  in-distribution performance on ProsQA. Factorial decomposition via M4 (pause-multipass)
  reveals that recycled content actively hurts chain-length extrapolation (M4 outperforms
  M2 on 7-hop by 10.9pp) while sequential processing drives topological generalization
  (M4 outperforms M3 on DAG by 7.9pp). Researchers building on COCONUT should invest
  in curriculum design and processing architecture, not hidden-state recycling.

references:
  - id: hao2024
    authors: "Hao et al."
    year: 2024
    title: "Training Large Language Models to Reason in a Continuous Latent Space"
    venue: "arXiv:2412.06769"
  - id: zhang2025
    authors: "Zhang et al."
    year: 2025
    title: "On the Causal Role of Continuous Thought Tokens"
    venue: "arXiv:2512.21711"
  - id: zhu2025
    authors: "Zhu et al."
    year: 2025
    title: "On the Expressiveness of Continuous Thought"
    venue: "ICML 2025"
  - id: meng2022
    authors: "Meng et al."
    year: 2022
    title: "Locating and Editing Factual Associations in GPT"
    venue: "NeurIPS 2022"
  - id: ravichander2021
    authors: "Ravichander et al."
    year: 2021
    title: "Probing the Probing Paradigm"
    venue: "EACL 2021"
  - id: goyal2024
    authors: "Goyal et al."
    year: 2024
    title: "Think before you speak: Training Language Models With Pause Tokens"
    venue: "ICLR 2024"
  - id: wei2022
    authors: "Wei et al."
    year: 2022
    title: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
    venue: "NeurIPS 2022"
  - id: radford2019
    authors: "Radford et al."
    year: 2019
    title: "Language Models are Unsupervised Multitask Learners"
    venue: "OpenAI Technical Report"
