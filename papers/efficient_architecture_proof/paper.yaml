title: "The Curriculum Is the Mechanism: Dissecting COCONUT's Latent Thought Gains on ProsQA"
short_title: "COCONUT: Curriculum vs Mechanism"

authors:
  - name: "Brian Martin"
    affiliation: "Independent"

abstract: >
  Meta's COCONUT (Chain of Continuous Thought) replaces explicit chain-of-thought
  with continuous hidden states recycled as input embeddings, achieving 97% accuracy
  on ProsQA graph traversal. We construct two curriculum-matched controls: M3, a
  single-pass pause-token baseline, and M4, a multi-pass control matching COCONUT's
  sequential processing structure while using fixed embeddings, enabling factorial
  decomposition of recycled-content and sequential-processing factors. M3 reaches
  96.6% test accuracy, not significantly different from COCONUT's 97.0% (McNemar
  p = 0.845); M4 reaches 94.8% (p = 0.354 after Bonferroni correction). Three
  converging experiments probe the distinction: corruption analysis reveals identical
  degradation profiles for M2 and M3, linear probes reveal identical selectivity
  profiles despite COCONUT encoding information more broadly across layers (29/78
  vs. 11/78 significant probing cells), and cross-model thought transplantation
  succeeds bidirectionally. On out-of-distribution generalization, factorial
  decomposition via M4 cleanly separates two confounded factors: recycled content
  actively hurts chain-length extrapolation (M4 outperforms M2 by 10.9pp on 7-hop,
  p < 0.001), while sequential processing drives DAG generalization (M4 outperforms
  M3 by 7.9pp, p < 0.001). At GPT-2 124M scale, the training curriculum drives
  COCONUT's accuracy on ProsQA; the continuous thought mechanism contributes
  measurably higher confidence (Wilcoxon r = 0.678 on in-distribution data) but
  does not improve accuracy.

keywords:
  - latent reasoning
  - chain of thought
  - continuous thought
  - COCONUT
  - ProsQA
  - graph traversal
  - curriculum learning
  - pause tokens

venue: "Preprint"
date: "2026-02-12"
version: "1.0"

research_question: >
  Does COCONUT's performance on ProsQA come from the continuous thought mechanism
  (hidden states recycled as inputs) or from the training curriculum (progressive
  CoT removal providing extra compute)?

hypothesis:
  primary: >
    A compute-matched pause-token control trained with the same curriculum will match
    COCONUT's in-distribution accuracy and corruption profiles, demonstrating that
    the curriculum, not the mechanism, drives the gains.
  outcome: "Supported"

models:
  - id: M1
    name: "CoT Baseline"
    architecture: "GPT-2 124M"
    description: "Standard chain-of-thought fine-tuning"
  - id: M2
    name: "COCONUT"
    architecture: "Coconut-wrapped GPT-2 124M"
    description: "Meta's continuous thought with hidden-state recycling"
  - id: M3
    name: "Pause-Curriculum"
    architecture: "GPT-2 124M"
    description: "Learned pause embedding with same 7-stage curriculum"
  - id: M4
    name: "Pause-Multipass"
    architecture: "GPT-2 124M"
    description: "Learned pause embedding with 6 sequential passes (same processing structure as COCONUT)"

data:
  training:
    dataset: "ProsQA (Meta's COCONUT repo)"
    samples: 17886
    format: "Graph traversal, 3-6 hops, nonsense entities"
  validation:
    samples: 300
  test:
    samples: 500
  ood:
    - name: "7-hop"
      samples: 1000
      novelty: "Hop count beyond training range"
    - name: "8-hop"
      samples: 1000
      novelty: "Hop count beyond training range"
    - name: "DAG"
      samples: 1000
      novelty: "DAG topology (training uses trees)"
    - name: "Dense"
      samples: 1000
      novelty: "Higher connectivity"

training:
  base_model: "openai-community/gpt2"
  parameters: 124000000
  optimizer: "AdamW"
  learning_rate: 0.0001
  weight_decay: 0.01
  batch_size_effective: 128
  epochs: 50
  precision: "fp32"
  seed: 0
  curriculum:
    stages: 7
    epochs_per_stage: 5
    max_latent_stage: 6

results:
  in_distribution:
    M1_test: 0.830
    M2_test: 0.970
    M3_test: 0.966
    M4_test: 0.948
    M2_val: 0.973
    M3_val: 0.973
    M4_val: 0.967
    M4_best_epoch: 30
  corruption:
    permutation_sensitivity_M2: 0.0
    permutation_sensitivity_M3: 0.0
    cross_transplant_M2: 0.970
    cross_transplant_M3: 0.965
    cliff_position: 4
    M4_all_positions_critical: true  # 2.4% accuracy when any single position corrupted
  probing:
    M2_peak_accuracy: 0.554
    M3_peak_accuracy: 0.570
    M2_peak_layer: 0
    M3_peak_layer: 12
    M2_selectivity: 0.52
    M3_selectivity: 0.52
    M2_thought_advantage: 0.105
    M3_thought_advantage: 0.040
    M4_note: "Non-reportable â€” extraction artifact (cold-start forward pass without KV-cache)"
  ood:
    7hop: { M1: 0.107, M2: 0.660, M3: 0.754, M4: 0.769 }
    8hop: { M1: 0.082, M2: 0.675, M3: 0.751, M4: 0.752 }
    dag:  { M1: 0.282, M2: 0.592, M3: 0.519, M4: 0.598 }
    dense: { M1: 0.141, M2: 0.612, M3: 0.684, M4: 0.648 }
  factorial_decomposition:
    # M4 vs M2 isolates recycled content (same sequential processing, different content)
    M4_vs_M2_7hop: { diff_pp: 10.9, p_bonf: 0.000, significant: true, direction: "M4>M2" }
    M4_vs_M2_8hop: { diff_pp: 7.7, p_bonf: 0.000, significant: true, direction: "M4>M2" }
    M4_vs_M2_dag:  { diff_pp: 0.6, p_bonf: 1.000, significant: false }
    M4_vs_M2_dense: { diff_pp: 3.6, p_bonf: 0.280, significant: false }
    # M4 vs M3 isolates sequential processing (same fixed content, different processing)
    M4_vs_M3_7hop: { diff_pp: 1.5, p_bonf: 1.000, significant: false }
    M4_vs_M3_8hop: { diff_pp: 0.1, p_bonf: 1.000, significant: false }
    M4_vs_M3_dag:  { diff_pp: 7.9, p_bonf: 0.000, significant: true, direction: "M4>M3" }
    M4_vs_M3_dense: { diff_pp: -3.6, p_bonf: 0.306, significant: false }
  wilcoxon_teacher_forced:
    M2_vs_M4_id: { r: 0.678, p: 5.52e-52, direction: "M2>M4" }
    M3_vs_M4_id: { r: 0.286, p: 1.61e-10, direction: "M3>M4" }

statistical_tests:
  _NOTE: "All McNemar tests are exact (two-sided binomial on discordant pairs), matching manuscript Tables 5b/5c."
  # M3 vs M2 (original two-model comparison, exact McNemar)
  mcnemar_prosqa_id:
    b: 14
    c: 12
    p_exact: 0.845
    p_bonferroni: 1.000
    significant: false
  mcnemar_7hop:
    b: 120
    c: 214
    p_exact: 0.000
    p_bonferroni: 0.000
    significant: true
  mcnemar_8hop:
    b: 122
    c: 198
    p_exact: 0.000
    p_bonferroni: 0.000
    significant: true
  mcnemar_dag:
    b: 235
    c: 162
    p_exact: 0.000291
    p_bonferroni: 0.0015
    significant: true
  mcnemar_dense:
    b: 139
    c: 211
    p_exact: 0.000
    p_bonferroni: 0.000
    significant: true
  # M4 vs M2 ID (recycled content test)
  mcnemar_M4_vs_M2_id:
    diff_pp: -2.2
    p_raw: 0.071
    p_bonferroni: 0.354
    significant: false
  # M4 vs M3 ID (sequential processing test)
  mcnemar_M4_vs_M3_id:
    diff_pp: -1.8
    p_raw: 0.136
    p_bonferroni: 0.680
    significant: false

conclusion: >
  The training curriculum, not the continuous thought mechanism, drives COCONUT's
  in-distribution performance on ProsQA. Factorial decomposition via M4 (pause-multipass)
  reveals that recycled content actively hurts chain-length extrapolation (M4 outperforms
  M2 on 7-hop by 10.9pp) while sequential processing drives topological generalization
  (M4 outperforms M3 on DAG by 7.9pp). Researchers building on COCONUT should invest
  in curriculum design and processing architecture, not hidden-state recycling.

references:
  - id: hao2024
    authors: "Hao et al."
    year: 2024
    title: "Training Large Language Models to Reason in a Continuous Latent Space"
    venue: "arXiv:2412.06769"
  - id: zhang2025
    authors: "Zhang et al."
    year: 2025
    title: "On the Causal Role of Continuous Thought Tokens"
    venue: "arXiv:2512.21711"
  - id: zhu2025
    authors: "Zhu et al."
    year: 2025
    title: "On the Expressiveness of Continuous Thought"
    venue: "ICML 2025"
  - id: meng2022
    authors: "Meng et al."
    year: 2022
    title: "Locating and Editing Factual Associations in GPT"
    venue: "NeurIPS 2022"
  - id: ravichander2021
    authors: "Ravichander et al."
    year: 2021
    title: "Probing the Probing Paradigm"
    venue: "EACL 2021"
  - id: goyal2024
    authors: "Goyal et al."
    year: 2024
    title: "Think before you speak: Training Language Models With Pause Tokens"
    venue: "ICLR 2024"
  - id: wei2022
    authors: "Wei et al."
    year: 2022
    title: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
    venue: "NeurIPS 2022"
  - id: radford2019
    authors: "Radford et al."
    year: 2019
    title: "Language Models are Unsupervised Multitask Learners"
    venue: "OpenAI Technical Report"
