title: "The Curriculum Is the Mechanism: Dissecting COCONUT's Latent Thought Gains on ProsQA"
short_title: "COCONUT: Curriculum vs Mechanism"

authors:
  - name: "Brian Martin"
    affiliation: "Independent"

abstract: >
  Meta's COCONUT (Chain of Continuous Thought) replaces explicit chain-of-thought
  with continuous hidden states recycled as input embeddings, achieving 97% accuracy
  on ProsQA graph traversal. We train a compute-matched pause-token control (M5) that
  receives the same 7-stage curriculum and same number of thought positions but uses a
  single learned embedding instead of recycled hidden states. M5 matches COCONUT on
  in-distribution accuracy (97.3% vs 97.3% validation; 95.6% vs 98.0% test) and shows
  identical corruption profiles: same degradation cliff at position 4, zero permutation
  sensitivity, and successful cross-problem transplantation. On out-of-distribution
  generalization, M5 outperforms COCONUT on 3 of 4 test sets by 7-9 percentage points
  (McNemar p=0.001 on 7-hop after Bonferroni correction). Linear probes reveal zero
  step-specific selectivity in both models, with COCONUT encoding more task information
  at thought positions (10.5% vs 4.0% advantage over input positions) that does not
  translate to better performance. The training curriculum drives COCONUT's gains on
  ProsQA; the continuous thought mechanism does not.

keywords:
  - latent reasoning
  - chain of thought
  - continuous thought
  - COCONUT
  - ProsQA
  - graph traversal
  - curriculum learning
  - pause tokens

venue: "Preprint"
date: "2026-02-12"
version: "1.0"

research_question: >
  Does COCONUT's performance on ProsQA come from the continuous thought mechanism
  (hidden states recycled as inputs) or from the training curriculum (progressive
  CoT removal providing extra compute)?

hypothesis:
  primary: >
    A compute-matched pause-token control trained with the same curriculum will match
    COCONUT's in-distribution accuracy and corruption profiles, demonstrating that
    the curriculum, not the mechanism, drives the gains.
  outcome: "Supported"

models:
  - id: M1
    name: "CoT Baseline"
    architecture: "GPT-2 124M"
    description: "Standard chain-of-thought fine-tuning"
  - id: M3
    name: "COCONUT"
    architecture: "Coconut-wrapped GPT-2 124M"
    description: "Meta's continuous thought with hidden-state recycling"
  - id: M5
    name: "Pause-Curriculum"
    architecture: "GPT-2 124M"
    description: "Learned pause embedding with same 7-stage curriculum"

data:
  training:
    dataset: "ProsQA (Meta's COCONUT repo)"
    samples: 17886
    format: "Graph traversal, 3-6 hops, nonsense entities"
  validation:
    samples: 300
  test:
    samples: 500
  ood:
    - name: "7-hop"
      samples: 1000
      novelty: "Hop count beyond training range"
    - name: "8-hop"
      samples: 1000
      novelty: "Hop count beyond training range"
    - name: "DAG"
      samples: 1000
      novelty: "DAG topology (training uses trees)"
    - name: "Dense"
      samples: 1000
      novelty: "Higher connectivity"

training:
  base_model: "openai-community/gpt2"
  parameters: 124000000
  optimizer: "AdamW"
  learning_rate: 0.0001
  weight_decay: 0.01
  batch_size_effective: 128
  epochs: 50
  precision: "fp32"
  seed: 0
  curriculum:
    stages: 7
    epochs_per_stage: 5
    max_latent_stage: 6

results:
  in_distribution:
    M1_test: 0.830
    M3_test: 0.980
    M5_test: 0.956
    M3_val: 0.973
    M5_val: 0.973
  corruption:
    permutation_sensitivity_M3: 0.0
    permutation_sensitivity_M5: 0.0
    cross_transplant_M3: 0.970
    cross_transplant_M5: 0.965
    cliff_position: 4
  probing:
    M3_peak_accuracy: 0.554
    M5_peak_accuracy: 0.571
    M3_peak_layer: 0
    M5_peak_layer: 12
    M3_selectivity: 0.0
    M5_selectivity: 0.0
    M3_thought_advantage: 0.105
    M5_thought_advantage: 0.040
  ood:
    7hop: { M1: 0.107, M3: 0.660, M5: 0.754 }
    8hop: { M1: 0.082, M3: 0.675, M5: 0.751 }
    dag:  { M1: 0.282, M3: 0.592, M5: 0.519 }
    dense: { M1: 0.141, M3: 0.612, M5: 0.684 }

statistical_tests:
  mcnemar_7hop:
    chi2: 10.107
    p_raw: 0.00148
    p_bonferroni: 0.00739
    significant: true
  mcnemar_8hop:
    chi2: 6.643
    p_raw: 0.00995
    p_bonferroni: 0.04977
    significant: true
  mcnemar_dag:
    chi2: 5.076
    p_raw: 0.02425
    p_bonferroni: 0.12126
    significant: false
  mcnemar_dense:
    chi2: 5.340
    p_raw: 0.02084
    p_bonferroni: 0.10420
    significant: false

conclusion: >
  The training curriculum, not the continuous thought mechanism, drives COCONUT's
  performance on ProsQA. A single learned pause embedding with the same curriculum
  achieves equivalent in-distribution accuracy and superior out-of-distribution
  generalization on 3 of 4 test sets. Researchers building on COCONUT should invest
  in curriculum design, not hidden-state recycling architectures.

references:
  - id: hao2024
    authors: "Hao et al."
    year: 2024
    title: "Training Large Language Models to Reason in a Continuous Latent Space"
    venue: "arXiv:2412.06769"
  - id: zhang2025
    authors: "Zhang et al."
    year: 2025
    title: "On the Causal Role of Continuous Thought Tokens"
    venue: "arXiv:2512.21711"
  - id: zhu2025
    authors: "Zhu et al."
    year: 2025
    title: "On the Expressiveness of Continuous Thought"
    venue: "ICML 2025"
  - id: meng2022
    authors: "Meng et al."
    year: 2022
    title: "Locating and Editing Factual Associations in GPT"
    venue: "NeurIPS 2022"
  - id: ravichander2021
    authors: "Ravichander et al."
    year: 2021
    title: "Probing the Probing Paradigm"
    venue: "EACL 2021"
  - id: goyal2024
    authors: "Goyal et al."
    year: 2024
    title: "Think before you speak: Training Language Models With Pause Tokens"
    venue: "ICLR 2024"
  - id: wei2022
    authors: "Wei et al."
    year: 2022
    title: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
    venue: "NeurIPS 2022"
  - id: radford2019
    authors: "Radford et al."
    year: 2019
    title: "Language Models are Unsupervised Multitask Learners"
    venue: "OpenAI Technical Report"
