## 7. Conclusion

We asked whether COCONUT's continuous thought tokens perform latent reasoning or serve as computational buffers. A compute-matched pause-token baseline (M5), trained under COCONUT's own 7-stage curriculum, closes 85% of the gap to COCONUT on ProsQA without recycling any hidden states. Three converging experiments -- corruption analysis, representational probing, and cross-model transplantation -- fail to distinguish the two systems on any diagnostic where reasoning and buffering make divergent predictions. On out-of-distribution generalization, the simpler pause baseline outperforms COCONUT on 3 of 4 test sets, suggesting that the recycling mechanism constrains rather than enables generalization.

These results indicate that COCONUT's performance on ProsQA is primarily attributable to its training curriculum, not to the continuous latent mechanism. The curriculum -- which progressively removes explicit chain-of-thought tokens and forces the model to internalize multi-step computation -- is the shared factor between M3 and M5, and it is the factor that separates both from the chain-of-thought baseline. For researchers developing latent reasoning architectures, this work suggests a concrete reallocation of effort: invest in curriculum design rather than hidden-state recycling. The simpler mechanism achieves comparable accuracy at lower architectural and computational cost, and it generalizes more robustly to out-of-distribution reasoning depths.
