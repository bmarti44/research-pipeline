# M1: CoT baseline (single GPU, grad_accum=4 to match Meta's 4-GPU effective batch)
project: coconut
save_path: ../results
name: prosqa-cot

only_eval: False

coconut: False
cot: True
no_thoughts: False
no_cot: False

c_thought: 1
epochs_per_stage: 5
max_latent_stage: 6
pad_latent_to_max: True

save_only_improve: False
uniform_prob: 0.0
model_id: openai-community/gpt2
load_model_path: None
seed: 0
resume: 0
bf16: False
train_path: data/prosqa_train.json
val_path: data/prosqa_valid.json
reset_optimizer: True
batch_size_training: 32
debug: False
gradient_accumulation_steps: 4
num_epochs: 50
lr: !!float "1e-4"
weight_decay: 0.01
