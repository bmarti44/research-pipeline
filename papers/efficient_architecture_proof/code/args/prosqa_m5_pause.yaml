# M5: Pause-Curriculum (single GPU)
# Compute-matched control: same curriculum as M3, same forward pass count in training,
# but uses a single learned <pause> embedding at thought positions instead of
# hidden-state recycling. Single forward pass (no multi-pass loop).
# M3 vs M5 is the PRIMARY comparison in every experiment.
project: coconut
save_path: ../results
name: prosqa-m5-pause

only_eval: False

coconut: True
cot: False
no_thoughts: False
no_cot: False
feedback_mode: pause_curriculum

c_thought: 1
epochs_per_stage: 5
max_latent_stage: 6
pad_latent_to_max: True

save_only_improve: False
uniform_prob: 0.0
model_id: openai-community/gpt2
load_model_path: None
seed: 0
resume: 0
bf16: False
train_path: data/prosqa_train.json
val_path: data/prosqa_valid.json
reset_optimizer: True
batch_size_training: 32
debug: False
gradient_accumulation_steps: 4
num_epochs: 50
lr: !!float "1e-4"
weight_decay: 0.01
