"""
Data preparation pipeline for LAHR experiments.

Prepares TinyStories dataset for training, which is:
- Small enough for MacBook training (~500MB)
- Well-studied in the literature
- Good for validating architecture ideas before scaling

Usage:
    python prepare_data.py --dataset tinystories --output data/
    python prepare_data.py --dataset tinystories --max_tokens 10000000
"""

import argparse
import os
from pathlib import Path
from typing import Optional

import numpy as np


def prepare_tinystories(
    output_dir: str,
    max_tokens: Optional[int] = None,
    seq_len: int = 512,
) -> dict:
    """
    Prepare TinyStories dataset for training.

    TinyStories is a dataset of simple children's stories generated by GPT-4,
    specifically designed for training small language models.

    Returns:
        dict with train/val stats
    """
    try:
        from datasets import load_dataset
        import tiktoken
    except ImportError:
        raise ImportError(
            "Please install datasets and tiktoken:\n"
            "  pip install datasets tiktoken"
        )

    print("Loading TinyStories dataset...")
    dataset = load_dataset("roneneldan/TinyStories", split="train")

    print("Loading GPT-2 tokenizer...")
    enc = tiktoken.get_encoding("gpt2")

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Tokenize
    print("Tokenizing...")
    all_tokens = []
    for i, example in enumerate(dataset):
        tokens = enc.encode(example["text"])
        all_tokens.extend(tokens)

        # Progress
        if i % 10000 == 0:
            print(f"  Processed {i:,} examples, {len(all_tokens):,} tokens")

        # Early stop if max_tokens reached
        if max_tokens and len(all_tokens) >= max_tokens:
            all_tokens = all_tokens[:max_tokens]
            print(f"  Reached max_tokens={max_tokens:,}")
            break

    print(f"Total tokens: {len(all_tokens):,}")

    # Split into train/val (95/5)
    n_val = int(len(all_tokens) * 0.05)
    val_tokens = all_tokens[:n_val]
    train_tokens = all_tokens[n_val:]

    print(f"Train tokens: {len(train_tokens):,}")
    print(f"Val tokens: {len(val_tokens):,}")

    # Save as memory-mapped files for efficient loading
    train_path = output_path / "train.bin"
    val_path = output_path / "val.bin"

    train_arr = np.array(train_tokens, dtype=np.uint16)
    val_arr = np.array(val_tokens, dtype=np.uint16)

    train_arr.tofile(train_path)
    val_arr.tofile(val_path)

    print(f"Saved to {train_path} and {val_path}")

    # Calculate number of sequences
    n_train_seq = len(train_tokens) // seq_len
    n_val_seq = len(val_tokens) // seq_len

    stats = {
        "dataset": "TinyStories",
        "total_tokens": len(all_tokens),
        "train_tokens": len(train_tokens),
        "val_tokens": len(val_tokens),
        "train_sequences": n_train_seq,
        "val_sequences": n_val_seq,
        "seq_len": seq_len,
        "train_path": str(train_path),
        "val_path": str(val_path),
    }

    # Save stats
    import json
    stats_path = output_path / "stats.json"
    with open(stats_path, "w") as f:
        json.dump(stats, f, indent=2)

    print(f"\nDataset ready!")
    print(f"  Train sequences: {n_train_seq:,}")
    print(f"  Val sequences: {n_val_seq:,}")
    print(f"  Sequence length: {seq_len}")

    return stats


def prepare_synthetic(
    output_dir: str,
    n_tokens: int = 1_000_000,
    vocab_size: int = 50257,
    seed: int = 42,
) -> dict:
    """
    Create synthetic random data for testing the pipeline.

    This is NOT for real experiments - only for verifying the code works.
    """
    print("Creating synthetic data (for testing only)...")

    np.random.seed(seed)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Generate random tokens
    all_tokens = np.random.randint(0, vocab_size, size=n_tokens, dtype=np.uint16)

    # Split
    n_val = int(n_tokens * 0.05)
    val_tokens = all_tokens[:n_val]
    train_tokens = all_tokens[n_val:]

    # Save
    train_path = output_path / "train.bin"
    val_path = output_path / "val.bin"

    train_tokens.tofile(train_path)
    val_tokens.tofile(val_path)

    stats = {
        "dataset": "synthetic",
        "total_tokens": n_tokens,
        "train_tokens": len(train_tokens),
        "val_tokens": len(val_tokens),
        "vocab_size": vocab_size,
        "seed": seed,
        "WARNING": "Synthetic data - for testing only, not for real experiments",
    }

    import json
    with open(output_path / "stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    print(f"Saved synthetic data to {output_path}")
    return stats


def main():
    parser = argparse.ArgumentParser(description="Prepare data for LAHR training")
    parser.add_argument(
        "--dataset",
        type=str,
        choices=["tinystories", "synthetic"],
        default="tinystories",
        help="Dataset to prepare",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data",
        help="Output directory",
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=None,
        help="Maximum tokens to use (default: all)",
    )
    parser.add_argument(
        "--seq_len",
        type=int,
        default=512,
        help="Sequence length for calculating stats",
    )

    args = parser.parse_args()

    if args.dataset == "tinystories":
        prepare_tinystories(args.output, args.max_tokens, args.seq_len)
    elif args.dataset == "synthetic":
        prepare_synthetic(args.output, n_tokens=args.max_tokens or 1_000_000)


if __name__ == "__main__":
    main()
