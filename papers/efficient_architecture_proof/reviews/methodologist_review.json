{
  "reviewer": "Methodologist",
  "timestamp": "2026-02-13T22:00:00Z",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/output/manuscript.md",
    "papers/efficient_architecture_proof/results/experiments/corruption/results.json",
    "papers/efficient_architecture_proof/results/experiments/probing/results.json",
    "papers/efficient_architecture_proof/results/experiments/mcnemar/results.json",
    "papers/efficient_architecture_proof/results/experiments/ood/results.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/results/statistical_analysis.json",
    "papers/efficient_architecture_proof/results/m3_test_eval.json",
    "papers/efficient_architecture_proof/results/m5_test_eval.json",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json"
  ],
  "findings": [
    {
      "id": "M001",
      "severity": "critical",
      "category": "confounds",
      "description": "Uncontrolled confound: number of forward passes. M3 executes 6 sequential forward passes during inference while M5 executes a single forward pass. The manuscript acknowledges this asymmetry (Section 3.2) and frames it as favoring the paper's argument because M5 achieves comparable accuracy with less computation. However, this confound is BIDIRECTIONAL. The number of forward passes is a major architectural difference that could explain not only M3's DAG advantage but also M5's OOD advantages. With 6x more forward passes, M3 has 6x more opportunities for error accumulation (compounding errors through the recurrence chain), which could explain its inferior OOD performance on longer chains -- not the content of the hidden states per se, but the serialization of computation. Conversely, M5's single forward pass means it sees all pause tokens simultaneously through self-attention, giving it an architectural advantage for parallel information integration. The paper conflates 'recycling mechanism' with 'multi-pass sequential computation' and cannot distinguish which factor drives the OOD results. A proper control would be M5 with 6 forward passes (pause tokens fed through the same recurrence loop but without hidden-state recycling), or M3 with a single forward pass (all 6 recycled states computed in one pass through concatenation).",
      "location": "Section 3.2 (Models), Section 5.3 (Sequential Bottleneck)",
      "evidence": "Manuscript states: 'M5 requires approximately one-sixth the inference-time FLOPs of M3, because it executes a single forward pass rather than the six sequential passes required by hidden-state recycling.' Yet the paper attributes M5's OOD advantage to the CONTENT of thought tokens rather than to the computational architecture.",
      "recommendation": "Either (a) add a control condition that matches forward-pass count while removing hidden-state content (e.g., M5 run through 6 sequential forward passes with pause tokens), or (b) substantially weaken the causal claims about the recycling mechanism's content vs. the serialization of computation. The current framing overstates what can be concluded."
    },
    {
      "id": "M002",
      "severity": "critical",
      "category": "internal_validity",
      "description": "Inconsistent model checkpoints across experiments. Table 2 reports M3 test accuracy = 98.0% (epoch 49) and M5 test accuracy = 95.6% (epoch 43), sourced from m3_test_eval.json and m5_test_eval.json respectively. However, the corruption experiments (Table 3), probing experiments (Table 4), and OOD experiments (Table 5) all report M3 clean/baseline accuracy = 97.0% and M5 = 96.6%, as confirmed by per_sample_correctness.json and corruption/results.json. This 1 percentage-point discrepancy for M3 (98.0% vs 97.0%) and 1 percentage-point discrepancy in the OPPOSITE direction for M5 (95.6% vs 96.6%) indicates that different model checkpoints were used for Table 2 vs. all subsequent experiments. The inconsistency means the 'gap closure' narrative (M5 closes 85% of the M3-M1 gap) may be based on one set of checkpoints while the mechanistic experiments were run on different checkpoints. This undermines internal validity because the models being probed and corrupted are not the same models whose headline accuracy is reported.",
      "location": "Table 2 vs. Tables 3, 4, 5",
      "evidence": "m3_test_eval.json: accuracy=0.98 (epoch 49). per_sample_correctness.json: M3=485/500=0.970. corruption/results.json: m3 clean_accuracy=0.97. Table 2 says 98.0% but Table 3 says 97.0% clean.",
      "recommendation": "Identify which checkpoint was used for which experiments and ensure consistency. Either (a) rerun all experiments with the same checkpoint, or (b) clearly document which checkpoint was used where and verify that the 1pp difference does not alter any conclusions. At minimum, Table 2 and Tables 3-5 must reference the same models."
    },
    {
      "id": "M003",
      "severity": "major",
      "category": "confounds",
      "description": "Single training seed (seed 0). All results derive from a single random initialization. The 2.4pp in-distribution gap (98.0% vs 95.6% per Table 2, or 0.4pp per Table 5 using different checkpoints) and all OOD differences (7-10pp) could be artifacts of the specific initialization rather than systematic architectural effects. This is acknowledged in the Limitations section but its severity is understated. With a single seed, there is no way to estimate variance, construct confidence intervals around accuracy differences, or test whether the OOD advantages are robust. The McNemar tests address within-test-set variance (whether models differ on the same samples) but cannot address between-seed variance (whether the same architectural difference would produce the same OOD pattern under a different initialization).",
      "location": "Section 6 (Limitations), throughout",
      "evidence": "statistical_analysis.json confirms n_seeds=1 with null CIs throughout. All effect sizes are labeled 'single_seed_difference'.",
      "recommendation": "Run at minimum 3-5 training seeds for each model. Report mean +/- std across seeds for all accuracy metrics. The OOD results are the most seed-sensitive because they depend on which generalizable representations the model happened to learn, which is highly initialization-dependent."
    },
    {
      "id": "M004",
      "severity": "major",
      "category": "construct_validity",
      "description": "The corruption experiment does not distinguish 'sequential reasoning' from 'position-specific information encoding.' The paper frames corruption sensitivity as a test of sequential reasoning: if M3 reasons sequentially, corrupting early positions should cascade. But the finding that position 3 is critical for BOTH models and that permutation has zero effect is equally consistent with both models learning to encode the answer at a specific position (position 3) via the curriculum, with no sequential reasoning occurring in either model. The corruption experiment cannot distinguish between (a) 'M3 does not reason sequentially' and (b) 'both M3 and M5 encode the answer at the same position because the curriculum trains both to do so.' The authors acknowledge this in Discussion 5.2 but still frame the result as evidence against sequential reasoning in M3 specifically, when it is equally evidence that the curriculum creates a strong position-specific encoding bias regardless of mechanism.",
      "location": "Section 3.4 (Experiment 1), Section 4.2, Section 5.1 (Table 6)",
      "evidence": "Single-position corruption at position 3 causes catastrophic accuracy drop for BOTH models (M3: 57.6%, M5: 57.8%). Positions 0-2 corruption has no effect on either model.",
      "recommendation": "Reframe the corruption experiment's conclusions. The finding is that both models have identical position-sensitivity profiles, which is evidence for curriculum-driven encoding. It is not direct evidence that M3 fails to reason sequentially -- M3 could perform some sequential computation that happens to converge on the same position-specific encoding as M5. The convergent evidence framing in Table 6 should be more conservative."
    },
    {
      "id": "M005",
      "severity": "major",
      "category": "construct_validity",
      "description": "Permutation insensitivity at 0% flip rate may reflect evaluation insensitivity rather than true order invariance. With temperature=0, the model always selects the argmax token. A permutation that changes the internal activations slightly but does not change which token has the highest probability would produce zero flips even if the model IS using order information. The binary correct/incorrect outcome is a very coarse measure. The paper needs to examine whether permutation changes the probability DISTRIBUTION (e.g., the logit gap between the top-1 and top-2 tokens) even if it does not change the argmax. If permuting thought tokens reduces the confidence margin from, say, 95% to 55%, the model IS using order information even though the flip rate is 0%. The 0% flip rate with 5,000 trials is strong evidence against large effects, but it cannot detect subtle effects that do not cross the decision boundary.",
      "location": "Section 4.2 (Permutation sensitivity)",
      "evidence": "permutation_power.json: 5000 trials, 0 flips, excludes true flip rate > 0.06%. But no analysis of logit or probability changes under permutation.",
      "recommendation": "Supplement the flip rate analysis with (a) the distribution of logit differences (correct - incorrect) under permutation vs. clean, (b) a continuous measure such as KL-divergence or mean absolute logit change, and (c) analysis of whether permutation affects only wrong-answer samples (where the margin is already thin). This would strengthen the 0%-flip finding by confirming it reflects genuine insensitivity rather than a ceiling effect on a binary measure."
    },
    {
      "id": "M006",
      "severity": "major",
      "category": "construct_validity",
      "description": "Probing selectivity measures presence of decodable information, not functional use. The paper acknowledges this (Section 5.2, citing Ravichander et al. 2021) but draws strong conclusions from the selectivity profiles nonetheless. The key claim -- that 'identical selectivity profiles' demonstrate 'step-specific encoding arises from the shared curriculum rather than from the recycling mechanism' -- does not follow. Two models can have identical PATTERNS of selectivity (peaking at position 3) while the underlying MECHANISM producing that pattern is entirely different. In M3, the selectivity at position 3 could arise from genuine sequential computation that deposits its output at position 3; in M5, the selectivity could arise from a completely different computational pathway (attention to input tokens routed through positional encoding). The probing analysis cannot distinguish these cases.",
      "location": "Section 4.3, Section 5.2",
      "evidence": "M3 peak at layer 0 position 3 (55.4%), M5 peak at layer 12 position 3 (57.0%). The LOCATION differs dramatically (layer 0 vs layer 12), suggesting different computational pathways, yet the paper emphasizes the similarity of selectivity values.",
      "recommendation": "Acknowledge that identical selectivity values at different layers actually suggest different computational mechanisms converging on similar outputs, not identical mechanisms. The layer-0 peak for M3 (where recycled states are injected) vs. layer-12 peak for M5 (standard transformer processing) indicates fundamentally different information processing pathways. Consider causal probing (ablating position 3 activations at the probe-peak layer) to test whether this information is used."
    },
    {
      "id": "M007",
      "severity": "major",
      "category": "alternative_explanations",
      "description": "The M5 OOD advantage may reflect an unfair comparison due to different effective model capacity utilization. M3 uses 6 forward passes and must allocate capacity across all 6 passes, while M5 uses a single forward pass and can allocate all capacity to that one pass. On OOD tasks requiring novel generalization, M5 may benefit from having its full representational capacity available in one shot, while M3 must distribute capacity across 6 serialized passes. This is a capacity-allocation confound, distinct from the sequential-bottleneck explanation offered in the paper. Under this alternative, the DAG advantage for M3 would arise because DAGs have convergent paths that benefit from iterative refinement across multiple passes (more opportunities to correct errors), while longer chains suffer from error accumulation.",
      "location": "Section 4.4, Section 5.3",
      "evidence": "M5 outperforms M3 on 7-hop (+9.4pp), 8-hop (+7.6pp), and dense (+7.2pp), while M3 wins on DAG (+7.3pp). The manuscript attributes this to 'sequential bottleneck' but does not rule out capacity-allocation explanations.",
      "recommendation": "Discuss the capacity-allocation alternative explanation alongside the sequential-bottleneck interpretation. Consider whether the number of forward passes (rather than hidden-state content) could explain the pattern. A control with M5 run through multiple forward passes (reprocessing the same input+pause tokens) would distinguish these accounts."
    },
    {
      "id": "M008",
      "severity": "major",
      "category": "operationalization",
      "description": "The transplant experiment's design may be insensitive to problem-specific information. If thought tokens carry problem-specific information that is redundant with the input tokens (which remain unchanged during transplantation), the model could reconstruct the correct answer from the input alone regardless of what the thought tokens contain. The transplant experiment tests whether thought tokens are NECESSARY for correct answers, not whether they carry problem-specific information. At 97% baseline accuracy, most test samples are presumably easy enough that the model can solve them from the input alone, making the transplant test insensitive. A more sensitive test would transplant thought tokens while also corrupting a subset of input tokens, forcing the model to rely on thought-token content.",
      "location": "Section 3.4 (Cross-problem transplant), Section 4.2",
      "evidence": "Transplant accuracy: M3=97.0% (matched), M3=97.5% (unmatched) -- identical to clean baseline. This is consistent with both (a) thought tokens carrying no information and (b) thought tokens being redundant with input.",
      "recommendation": "Run a transplant experiment with partial input corruption to test whether thought tokens carry information that becomes useful when input is degraded. Also consider transplanting thoughts between problems with different answer types (e.g., different number of hops) where the model must rely on thought-token content to distinguish."
    },
    {
      "id": "M009",
      "severity": "major",
      "category": "controls",
      "description": "Missing curriculum-only control condition. The paper identifies the curriculum as the likely driver of performance but does not test a curriculum-only condition in which removed CoT tokens are simply deleted (producing shorter sequences with no replacement tokens). This is acknowledged in the Limitations section, but it is a more severe gap than the paper suggests. Without this control, the paper cannot distinguish between three hypotheses: (a) the curriculum alone drives performance, (b) the curriculum + additional computation positions drive performance, (c) the curriculum + specific token content drives performance. The paper's central claim -- that curriculum is primary -- requires ruling out (b), which requires the missing control.",
      "location": "Section 6 (Limitations)",
      "evidence": "The manuscript states: 'We therefore cannot distinguish whether the curriculum alone drives the gains or whether the curriculum requires additional attention positions as a computational budget.'",
      "recommendation": "Add M6: a curriculum-trained model where removed CoT tokens are deleted entirely (no thought tokens, pause tokens, or placeholders). If M6 performs comparably to M3 and M5, the curriculum alone is sufficient. If M6 performs worse, the additional attention positions (compute budget) are necessary regardless of content."
    },
    {
      "id": "M010",
      "severity": "major",
      "category": "internal_validity",
      "description": "Discrepancy between approximate and exact McNemar test results. The statistical_analysis.json file contains approximate McNemar tests (computed from aggregate accuracy rather than per-sample data, as flagged by its own 'warning' field) that produce DIFFERENT significance conclusions than the exact McNemar tests in mcnemar/results.json (computed from per-sample paired predictions). Specifically, the approximate tests find DAG (p_corr=0.121) and Dense (p_corr=0.104) as NOT significant, while the exact tests find both as significant (DAG p_bonf=0.00146, Dense p_bonf=0.00070). The manuscript reports the exact per-sample results, which is methodologically correct. However, the existence of contradictory results in the data directory raises a provenance concern: were the exact McNemar tests computed post-hoc after seeing that the approximate tests were non-significant?",
      "location": "Table 5, data files",
      "evidence": "statistical_analysis.json: DAG p_corr=0.12126 (NOT significant). mcnemar/results.json: DAG p_bonf=0.00146 (significant). The approximate version explicitly warns: 'Approximate: computed from aggregate accuracy, not per-sample data.'",
      "recommendation": "Document the timeline of when each analysis was run. Clarify that the exact McNemar test was always the planned analysis (per the preregistered analysis plan, if any), and the approximate version was a preliminary estimate. The manuscript should note that the exact test was prespecified and the approximate version was superseded."
    },
    {
      "id": "M011",
      "severity": "minor",
      "category": "operationalization",
      "description": "The manuscript reports DAG Bonferroni-corrected p = 0.001 (Table 5, Section 5.3), but the actual value is 0.00146 (from mcnemar/results.json). While this is within rounding tolerance for p < 0.002, scientific convention is to report the first significant digit that differs, which would be p = .0015 or p < .002 rather than p = .001. The appendix_data.json file explicitly labels this as 'dag_bonferroni_rounded', suggesting intentional rounding. However, rounding p = .00146 down to p = .001 overstates the significance by making it appear more significant than it actually is.",
      "location": "Table 5, Section 4.4",
      "evidence": "mcnemar/results.json: p_bonferroni = 0.00145697. appendix_data.json: dag_bonferroni_rounded = 0.001. Manuscript: 'p = 0.001'.",
      "recommendation": "Report as p = .0015 or p = .001 with explicit notation that this is rounded. Alternatively, report as p < .002 to avoid ambiguity."
    },
    {
      "id": "M012",
      "severity": "minor",
      "category": "replicability",
      "description": "Probing analysis has zero entries for positions 4 and 5 across all layers for both models, with n=81 and n=12 samples respectively. The paper correctly notes the n=12 limitation for position 5, but position 4 (n=81) also shows 0.0% accuracy across all cells for both models. With 81 samples and ~38+ target classes (species names), the expected accuracy from a balanced classifier would be ~2.6%. The 0.0% result for position 4 is suspiciously low and may indicate a data processing error (e.g., label extraction failing for position 4 samples, or the probe receiving constant inputs). The paper draws no conclusions from positions 4-5 but their inclusion as zeros in summary statistics (e.g., mean selectivity, mean thought-vs-input advantage) deflates these averages.",
      "location": "Section 4.3, Tables A5-A6",
      "evidence": "probing/results.json: All cells at positions 4 and 5 report exactly 0.0 for both linear and nonlinear probes. Position 4 has n=81 samples -- enough for meaningful probe accuracy if labels are present.",
      "recommendation": "Investigate whether the 0.0% for position 4 reflects genuine absence of information or a data processing artifact. If the 81 samples at position 4 all have the same label (or if label extraction failed), this should be documented. Consider excluding positions 4-5 from summary statistics or computing them separately."
    },
    {
      "id": "M013",
      "severity": "minor",
      "category": "ecological_validity",
      "description": "ProsQA is a synthetic benchmark with perfectly deterministic, unambiguous reasoning paths and a closed vocabulary of 38 species and 17 person names. The paper's conclusions about COCONUT's mechanism are drawn entirely from this single task. While the Limitations section acknowledges this, the paper's title and abstract do not qualify the claims to ProsQA-specific findings. The abstract states 'These results indicate that the curriculum, not the continuous latent mechanism, drives COCONUT's in-distribution performance' -- this should be qualified to 'on ProsQA' rather than stated as a general conclusion about COCONUT.",
      "location": "Title, Abstract, Section 7 (Conclusion)",
      "evidence": "All experiments use ProsQA (and OOD variants of ProsQA). The paper has no results on natural language tasks, MMLU, HotpotQA, or other benchmarks where COCONUT was evaluated.",
      "recommendation": "Qualify the title and abstract to indicate ProsQA-specific findings. Consider softening the causal language: 'on ProsQA, the curriculum appears to be the primary driver' rather than 'the curriculum drives COCONUT's performance' which implies generality."
    },
    {
      "id": "M014",
      "severity": "minor",
      "category": "construct_validity",
      "description": "The cross-corruption analysis (applying M3-magnitude noise to M5) does not control for the direction of the noise. M3's thought tokens have high variance because they encode different hidden states across problems. M5's thought tokens have near-zero variance because they are copies of the same learned embedding. When M3-magnitude random noise is added to M5's low-variance embeddings, it overwhelms the original signal completely (L2 of noise ~ 203 vs. original L2 ~ 4). When M3-magnitude noise is added to M3's high-variance embeddings, it adds variance-matched noise that partially preserves the original signal's direction. The cross-corruption experiment therefore does not equalize the perturbation in a meaningful sense -- the effective signal-to-noise ratio is dramatically different between the two conditions.",
      "location": "Section 4.2, Appendix A.2",
      "evidence": "cross_corruption.json: M3 noise L2 = 202.8, M5 noise L2 = 4.09. When applying M3 noise to M5, the noise completely dominates the original embedding.",
      "recommendation": "Acknowledge that the cross-corruption condition replaces rather than perturbs M5's embeddings. This actually strengthens the buffering interpretation (M5 works even when its thought tokens are replaced with random high-variance noise), but the framing should be adjusted to reflect that this is a replacement rather than a perturbation of M5's representations."
    },
    {
      "id": "M015",
      "severity": "minor",
      "category": "alternative_explanations",
      "description": "The 'broadcast-then-attend' explanation for anti-selectivity at positions 0-1 is speculative and not tested. The paper proposes that both models broadcast later-step (answer-relevant) information to early positions, making it accessible via self-attention. This is a plausible mechanism for M5 (which uses self-attention across all positions in one pass) but less plausible for M3, where positions are filled sequentially by separate forward passes. In M3, position 0 is filled BEFORE positions 1-5 exist; it cannot be 'broadcasting' information to later positions because those positions have not been computed yet. For M3, the anti-selectivity at position 0 more likely reflects that the first recycled hidden state (from processing just the input) already encodes the problem's answer-relevant information from the input tokens, not that it was placed there for later positions to attend to.",
      "location": "Section 4.3, Section 5.2",
      "evidence": "M3 positions are filled sequentially (each forward pass generates the next position's input). M5 positions are processed simultaneously. The 'broadcast-then-attend' explanation applies differently to each architecture.",
      "recommendation": "Differentiate the mechanistic explanation between M3 and M5. For M3, anti-selectivity at position 0 reflects the recycled hidden state from the input-processing pass, which naturally encodes the full problem representation. For M5, anti-selectivity may genuinely reflect a broadcast strategy. The convergence of the pattern across models may be coincidental rather than indicating a shared mechanism."
    },
    {
      "id": "M016",
      "severity": "minor",
      "category": "replicability",
      "description": "The nonlinear probe results (0.0 across all 78 cells for both models) are likely a training failure rather than a genuine null result. The manuscript acknowledges this possibility: 'the MLP training procedure (default scikit-learn MLPClassifier hyperparameters) may warrant further tuning to rule out convergence failure.' If the MLP probes failed to converge, reporting '0/78 cells where MLP > linear' is misleading because it implies the MLPs achieved comparable accuracy to linear probes but never exceeded them. In reality, 0.0 accuracy means the MLPs failed entirely, not that they matched linear probe accuracy. This inflates the evidence for the claim that 'all encoded information is linearly decodable.'",
      "location": "Section 4.3, Appendix A.7",
      "evidence": "probing/results.json: All nonlinear_probe_accuracy values are exactly 0.0 across all 78 cells. Linear probes achieve up to 55.4% (M3) and 57.0% (M5).",
      "recommendation": "Either (a) retrain MLP probes with proper hyperparameter tuning (learning rate search, longer training, different architectures) and report the actual MLP accuracy, or (b) remove the nonlinear probe claim entirely and note only that the MLP probes failed to converge, making no claim about nonlinear decodability."
    },
    {
      "id": "M017",
      "severity": "suggestion",
      "category": "controls",
      "description": "M1 (CoT) is included as a baseline but never directly compared to M3 or M5 in the corruption, probing, or transplant experiments. The paper runs M1 only on the OOD test sets. If the corruption, probing, and transplant experiments were also run on M1's intermediate CoT tokens, it would provide a positive control: CoT tokens SHOULD be order-sensitive, problem-specific, and carry step-specific information. Confirming this would validate the experimental paradigm and strengthen the negative results for M3 and M5.",
      "location": "Section 3.4",
      "evidence": "No corruption, probing, or transplant results for M1 in any data files.",
      "recommendation": "Run the corruption (permutation of CoT tokens), probing (decode intermediate steps from CoT token positions), and transplant (cross-problem CoT injection) experiments on M1. If M1's CoT tokens show order-sensitivity, problem-specificity, and step-specific encoding, this validates that the experimental methods can detect sequential reasoning when it exists."
    },
    {
      "id": "M018",
      "severity": "suggestion",
      "category": "operationalization",
      "description": "The claim that 'M5 closes 85% of the gap between M1 and M3' depends on which accuracy numbers are used. With Table 2 numbers (M1=83.0%, M3=98.0%, M5=95.6%): gap closure = (95.6 - 83.0) / (98.0 - 83.0) = 84%. With experiment numbers (M1=83.0%, M3=97.0%, M5=96.6%): gap closure = (96.6 - 83.0) / (97.0 - 83.0) = 97.1%. The experiment-checkpoint numbers suggest M5 closes 97% of the gap, making the 85% figure an underestimate that depends on the specific checkpoints used.",
      "location": "Abstract, Section 1, Section 4.1",
      "evidence": "Table 2: M3=98.0%, M5=95.6%. Per-sample correctness: M3=97.0%, M5=96.6%.",
      "recommendation": "Report gap closure using consistent checkpoints. If the experiment checkpoints (97.0% and 96.6%) are the definitive numbers, the gap closure is ~97%, not 85%. This actually strengthens the paper's argument."
    },
    {
      "id": "M019",
      "severity": "suggestion",
      "category": "design",
      "description": "The OOD test sets vary multiple factors simultaneously. The 7-hop and 8-hop sets change path length but preserve tree topology. The DAG set changes topology but preserves path length range (3-6). The Dense set changes branching factor but preserves tree topology and path length range. This design enables some factorial reasoning but misses key combinations: there is no 'long DAG' (7+ hop DAG) or 'dense 7-hop' condition, which would help disentangle length-dependent vs. topology-dependent effects of the recycling mechanism. The claim that M3's DAG advantage reflects 'path convergence' would be strengthened by testing DAGs at longer path lengths.",
      "location": "Section 3.4 (Experiment 3), Table A7",
      "evidence": "OOD datasets: 7-hop (tree, 7), 8-hop (tree, 8), DAG (DAG, 3-6), Dense (tree, 3-6, bf 5-8). No crossed conditions.",
      "recommendation": "Generate additional OOD test sets that cross path length and topology (e.g., 7-hop DAG, 8-hop DAG, dense 7-hop) to enable a fuller factorial analysis of how the recycling mechanism interacts with both axes of generalization."
    },
    {
      "id": "M020",
      "severity": "suggestion",
      "category": "external_validity",
      "description": "The paper's scale limitation (GPT-2 124M) is more consequential than presented. Zhu et al. (2025) proved that continuous thought tokens are more expressive than discrete CoT. This expressiveness advantage is an asymptotic result that may require sufficient model capacity to materialize. At 124M parameters with 12 layers, the model may simply lack the capacity to exploit the theoretical expressiveness of the continuous space. The negative results at this scale cannot rule out that the mechanism becomes effective at larger scales. The paper appropriately cites Zhang et al. (2025) who found similar inertness at 7-8B scale, but the convergence of evidence is limited to two studies at very different scales with different tasks.",
      "location": "Section 6 (Limitations)",
      "evidence": "All experiments on GPT-2 124M. Zhang et al. (2025) found inertness on LLaMA 7-8B on different tasks (MMLU, HotpotQA). Neither study tests on ProsQA at scale.",
      "recommendation": "Acknowledge more explicitly that the curriculum-vs-mechanism question at ProsQA-optimal scale (where COCONUT achieves 97%) remains open. The 124M scale may be simultaneously (a) sufficient for curriculum-driven learning and (b) insufficient for mechanism-driven learning, creating a floor effect for the mechanism."
    }
  ],
  "overall_assessment": "pass_with_conditions",
  "summary": "This paper makes a well-motivated and important contribution by constructing a curriculum-matched control (M5) for COCONUT, addressing a clear confound in the original work. The experimental design is creative and multi-pronged, and the convergent evidence framework is commendable. The statistical analysis of OOD results (exact McNemar with Bonferroni correction) is sound, and the per-sample verification of contingency tables adds credibility.\n\nHowever, the paper has two critical issues that must be addressed. First, the number of forward passes (6 for M3 vs 1 for M5) is an uncontrolled confound that the paper acknowledges but whose implications it underestimates. The OOD results are attributed to the CONTENT of hidden-state recycling, but they could equally reflect the ARCHITECTURE of serialized computation. This confound limits the causal specificity of the paper's central claims. Second, the inconsistent model checkpoints across experiments (Table 2 accuracy figures differ from those used in Tables 3-5 by 1pp in opposite directions for M3 and M5) is a data integrity issue that must be resolved.\n\nAdditionally, several major issues weaken the strength of individual experiments. The single training seed prevents estimation of seed-dependent variance. The permutation and transplant experiments may have insufficient sensitivity to detect subtle effects. The probing analysis reveals different computational pathways (layer 0 vs layer 12 peaks) while the paper emphasizes the similarity of selectivity values. The nonlinear probe results (all zeros) likely reflect training failure rather than genuine absence of nonlinear encoding. The missing curriculum-only control means the paper cannot distinguish curriculum effects from curriculum + compute-budget effects.\n\nDespite these issues, the paper's core contribution -- demonstrating that a simple pause baseline can match COCONUT's in-distribution performance under the same curriculum -- is robust and important. The conditions for passing are: (1) resolve the checkpoint inconsistency, (2) explicitly discuss the forward-pass-count confound as an alternative explanation for OOD results, (3) qualify causal language throughout, and (4) address or acknowledge the major issues noted above."
}
