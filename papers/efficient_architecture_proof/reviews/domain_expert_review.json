{
  "reviewer": "Domain Expert (LLM/Mechanistic Interpretability)",
  "timestamp": "2026-02-13T22:30:00Z",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/output/manuscript.md",
    "papers/efficient_architecture_proof/results/experiments/corruption/results.json",
    "papers/efficient_architecture_proof/results/experiments/probing/results.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/experiments/mcnemar/results.json",
    "papers/efficient_architecture_proof/results/experiments/ood/results.json",
    "papers/efficient_architecture_proof/results/statistical_analysis.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json",
    "papers/efficient_architecture_proof/results/pause_embedding_architecture.txt"
  ],
  "findings": [
    {
      "id": "D001",
      "severity": "major",
      "category": "interpretation",
      "description": "The selectivity values reported in Table 4 and Section 4.3 (+52.0pp and +52.3pp at position 3) genuinely indicate step-specific encoding, which is more consistent with a structured representational strategy than with pure 'buffering'. The paper frames this as arising from the curriculum, but the fact that position 3 specifically encodes the entity corresponding to the final hop -- with 52 percentage points of selectivity over the best control position -- suggests that these representations are not 'generic compute buffers' in any standard sense of the term. A compute buffer, by definition, carries no task-relevant structure; these positions clearly do. The paper's own probing results contradict the headline 'buffering' framing. The paper partially acknowledges this tension (Section 5.2) but does not fully resolve it: the convergent evidence table (Table 6) lists probing selectivity under 'general broadcast' for the buffering claim, but +52pp selectivity at a specific position with anti-selectivity at others is precisely the opposite of a broadcast pattern. This is a structured, position-specific encoding strategy, shared between M3 and M5. The correct conclusion is not 'buffering vs reasoning' but rather 'curriculum-driven structured computation that does not require the recycling mechanism.' The binary framing obscures what is actually a more interesting and nuanced finding.",
      "location": "Section 4.3, Table 4, Table 6 (Section 5.1)",
      "evidence": "Table 4: Position 3 selectivity = +52.0pp (M3), +52.3pp (M5). Table 6 labels this as 'General broadcast' under the buffering claim, but the data show position-specific encoding with a clear hierarchy (positions 0-1 anti-selective, position 2 mildly selective, position 3 strongly selective).",
      "recommendation": "Reframe the paper away from the binary 'reasoning vs buffering' dichotomy. The more precise conclusion is that both models learn a structured representational strategy driven by the curriculum, with step-relevant information concentrated at specific positions, but this strategy does not require the hidden-state recycling mechanism. Consider replacing 'buffering' with 'curriculum-driven structured computation' or similar language that does not imply the absence of task-relevant organization."
    },
    {
      "id": "D002",
      "severity": "major",
      "category": "interpretation",
      "description": "The corruption cliff at position 4 does not straightforwardly mean that the first 3 positions (0-2) are 'redundant'. Combined with the probing data showing that positions 0-1 encode later-step (answer-relevant) information with anti-selectivity, and position 2 encodes mild step-specific content, these positions may serve a preparatory or routing function. In causal attention, positions 0-2 are visible to positions 3-5. If the model's strategy is to place answer-relevant information at early positions where it can be attended to from all later positions, then corrupting those positions should not matter -- until the model loses the positions that actually use that information (position 3+). This is not 'redundancy' but a deliberate two-phase computation: distribute information early, then integrate at position 3. The single-position corruption data supports this: corrupting position 3 alone causes the cliff, but corrupting positions 0-2 individually does not. The paper partially captures this with the 'broadcast-then-attend' language in Section 4.3 but then reverts to calling the positions 'redundant' in the corruption discussion.",
      "location": "Section 4.2, Section 4.3",
      "evidence": "Table A4: Corrupting position 0, 1, or 2 alone causes no accuracy drop (<1pp). Corrupting position 3 alone drops accuracy to ~57.6%. Positions 0-1 show anti-selectivity (encode step 2 better than their own matched step). This pattern is consistent with a distributed computation strategy, not redundancy.",
      "recommendation": "Replace the language of 'redundancy' for positions 0-2 with a more precise characterization. These positions appear to serve a distributional role (broadcasting answer-relevant information across the sequence for causal attention access) while position 3 serves as the integration/decision point. This is a specific computational strategy, not mere redundancy."
    },
    {
      "id": "D003",
      "severity": "major",
      "category": "interpretation",
      "description": "The permutation insensitivity result (0% flip rate across 5000 trials) is presented as strong evidence against sequential encoding. However, the paper's own probing analysis shows that position-specific information is concentrated at position 3, with positions 0-2 encoding qualitatively similar content (later-step entities with anti-selectivity). Permuting positions that encode similar content would not produce prediction flips even if the model were performing position-sensitive computation, because the swapped activations carry approximately interchangeable information. The critical test would be to permute position 3 with a non-critical position (e.g., position 0), which should disrupt the computation if position 3 is functionally special. But this is exactly what the permutation test does (random permutations include such swaps), and the result is still zero flips. The paper should more carefully reason through why zero flips occurs even when position 3 (which is demonstrably critical, per single-position corruption) is moved. One possible explanation: the model attends based on activation content rather than absolute position, so moving position 3's activation to slot 0 does not matter because the attention mechanism can still find it by content matching. This would be consistent with 'structured computation that is position-agnostic in its attention routing' rather than 'buffering'.",
      "location": "Section 4.2 (Permutation sensitivity)",
      "evidence": "Permutation flip rate = 0.0% across 5000 trials for both M3 and M5. But single-position corruption at position 3 causes catastrophic failure (~57% accuracy). This creates a puzzle: position 3 is critical to corrupt but irrelevant to move. The resolution likely involves attention-based content retrieval rather than positional dependence, which is a more specific mechanism than 'buffering'.",
      "recommendation": "Add a paragraph discussing the tension between permutation insensitivity and single-position corruption criticality. The most parsimonious explanation is that the model routes information via content-based attention rather than position-indexed lookup, making the physical position of the critical activation irrelevant. This is an important architectural insight that gets lost in the 'buffering' framing."
    },
    {
      "id": "D004",
      "severity": "major",
      "category": "technical_accuracy",
      "description": "The paper states that M5 performs 'a single forward pass over the entire sequence' (Section 3.1, bullet 3), while M3 performs 'six sequential forward passes'. This is a fundamental computational asymmetry that goes beyond the hidden-state recycling mechanism. M3 gets 6x the transformer depth (72 effective layers vs 12) for the thought-token region of the sequence. The paper acknowledges the FLOP difference (Section 3.2, final paragraph) but frames it as favoring the paper's argument. However, this confounds two separate claims: (1) M5 matches M3 with less compute (fair), and (2) M3's recycled hidden states do not carry useful information (the corruption/probing claims). For claim (2), the computational asymmetry is problematic: M3 may be using those 6 forward passes to build up the representation at position 3 that the probes detect, and the recycling may be essential for that build-up process even though the final result is phenomenologically similar to what M5 achieves in a single pass via a different route. The transplantation experiment partially addresses this (M5 can use M3's thought representations and vice versa), but the 6x depth difference makes the two models less directly comparable than the paper implies.",
      "location": "Section 3.1, Section 3.2 (final paragraph)",
      "evidence": "M3: 6 sequential forward passes of 12 layers each = 72 effective layers for thought computation. M5: 1 forward pass of 12 layers = 12 effective layers. Paper acknowledges 'approximately one-sixth the inference-time FLOPs' but does not discuss implications for the probing and corruption comparisons.",
      "recommendation": "Discuss the depth asymmetry more explicitly in the methods and interpretation. Consider framing it as: 'M3 builds representations through 6x the computational depth but arrives at a functionally equivalent result, suggesting that the additional depth is not leveraged for qualitatively different computation.' Also note that the probing comparison at 'layer 0' for M3 (where recycled states are injected) versus 'layer 12' for M5 reflects this asymmetry -- M3's layer 0 states have already been through 12 layers of processing in the previous pass."
    },
    {
      "id": "D005",
      "severity": "major",
      "category": "missing_experiments",
      "description": "The paper's central claim -- that the curriculum drives performance rather than the recycling mechanism -- would be substantially strengthened by a 'curriculum-only' ablation that the Limitations section (Section 6) acknowledges is missing. Without this ablation, the paper cannot distinguish between two hypotheses: (a) the curriculum is sufficient (no extra attention positions needed), and (b) the curriculum requires additional attention positions to deposit intermediate computations, and pause tokens provide those positions. Hypothesis (b) is still consistent with 'buffering' but would mean that the number and presence of thought positions matters, which is a more specific claim than 'the curriculum drives everything.' This is a significant gap because the Pfau et al. (2024) result (filler tokens expand computational class) suggests that additional positions are likely necessary, not just the curriculum.",
      "location": "Section 6 (Limitations, 'Curriculum isolation' paragraph)",
      "evidence": "The paper states: 'we do not test a curriculum-only condition in which removed reasoning tokens are simply deleted, producing shorter sequences with no additional attention positions.' This is a major missing ablation that would resolve the paper's central ambiguity.",
      "recommendation": "If feasible, run a 'curriculum-only' ablation where the CoT tokens are removed at each stage but no pause tokens are inserted (the sequence simply gets shorter). If this model performs comparably to M5, the curriculum alone is sufficient. If it performs much worse, the additional attention positions are a necessary computational resource, which would support a 'compute buffering' interpretation more precisely. At minimum, acknowledge this more prominently than in the limitations section -- it is a core interpretive ambiguity, not a peripheral limitation."
    },
    {
      "id": "D006",
      "severity": "minor",
      "category": "related_work",
      "description": "The paper does not cite or discuss several relevant lines of work in mechanistic interpretability. Specifically: (1) Nostalgebraist's work on interpreting GPT-2 internals, which is directly relevant since the base model is GPT-2. (2) The information bottleneck perspective from Shwartz-Ziv and Tishby, which provides a theoretical framework for understanding why intermediate representations might compress information. (3) Olsson et al. (2022) on induction heads, which is relevant to how the model might be routing information across thought positions via attention. (4) Conmy et al. (2023) on automated circuit discovery (ACDC), which would provide a more targeted methodology for identifying which circuits use the thought positions. (5) Bills et al. (2023) on automated interpretability, relevant to understanding what the thought position representations encode. None of these are strictly required, but the paper's probing methodology would benefit from being situated in the broader mechanistic interpretability toolkit.",
      "location": "Section 2.4, Section 2 generally",
      "evidence": "The related work section cites Meng et al. (2022) for causal analysis and Ravichander et al. (2021) for probing, but does not engage with the broader mechanistic interpretability literature.",
      "recommendation": "Add a brief paragraph in Section 2.4 situating the probing methodology within the broader mechanistic interpretability literature. Consider citing Olsson et al. (2022) on induction heads and attention-based information routing, and note that circuit-level analysis (e.g., via ACDC or path patching) would provide a more targeted decomposition than linear probing."
    },
    {
      "id": "D007",
      "severity": "minor",
      "category": "related_work",
      "description": "The paper does not discuss Deng et al. (2024) 'Explicit CoT Training for Implicit CoT Reasoning' or related work on distilling chain-of-thought into implicit reasoning, which is closely related to the COCONUT curriculum. This line of work suggests that models can learn to perform implicit reasoning when trained with explicit supervision that is gradually removed -- which is exactly the curriculum mechanism that this paper identifies as the key driver.",
      "location": "Section 2.1",
      "evidence": "The paper discusses Wei et al. (2022) and the general question of whether verbalization is necessary, but does not cite the growing literature on explicit-to-implicit CoT distillation.",
      "recommendation": "Add a brief mention of work on CoT distillation and implicit reasoning training, which supports the paper's thesis that progressive curriculum design (rather than the specific latent mechanism) drives performance."
    },
    {
      "id": "D008",
      "severity": "minor",
      "category": "technical_accuracy",
      "description": "The manuscript's abstract states M5 outperforms on '3 of 4 OOD test sets', but the OOD section (4.4) and Table 5 report 5 comparisons (ProsQA ID + 4 OOD). The abstract should be consistent: M5 outperforms M3 on 3 of 4 OOD sets (correct -- 7-hop, 8-hop, dense), while the in-distribution comparison shows no difference. The body text is consistent but the abstract's '3 of 4' phrasing accurately reflects the 4 OOD-only comparisons. However, the introduction says '3 of 4 test sets' (paragraph 4) which is slightly ambiguous -- a reader could interpret this as 3 of 4 total comparisons. This is a minor wording issue.",
      "location": "Abstract, Section 1 (paragraph 4)",
      "evidence": "Abstract: 'M5 outperforms COCONUT on 3 of 4 test sets (all statistically significant).' Body: Table 5 shows 5 comparisons. 3 of 4 OOD comparisons favor M5. The phrasing is technically correct but potentially confusing since the total number of comparisons varies by context.",
      "recommendation": "Clarify in the abstract and introduction: 'M5 outperforms COCONUT on 3 of 4 out-of-distribution test sets' to avoid ambiguity about whether the in-distribution comparison is included in the count."
    },
    {
      "id": "D009",
      "severity": "minor",
      "category": "technical_accuracy",
      "description": "The nonlinear probe results (Appendix A.7) show 0.0 accuracy for ALL MLP probes across ALL 78 cells for both models. This is not a null result indicating 'MLP probes do not exceed linear probes' -- it indicates that the MLP probes completely failed to learn. An MLP probe that achieves 0.0% accuracy on a classification task (where chance is above 0% for most target distributions) has convergence or implementation issues. The paper acknowledges this in the final sentence of A.7 ('the MLP training procedure... may warrant further tuning to rule out convergence failure'), but this understatement obscures the severity: zero accuracy across 156 probing configurations is almost certainly a bug in the MLP training, not a genuine null finding. The paper should either fix the MLP probing or remove the claim that 'the encoded information is linearly decodable' (Section 4.3), since the nonlinear baseline is broken.",
      "location": "Appendix A.7, Section 4.3 (Table 4, 'Cells where MLP > linear: 0 / 78')",
      "evidence": "probing/results.json: nonlinear_probe_accuracy is 0.0 for all 78 cells in both M3 and M5. This is not a null result but a failure of the MLP training procedure.",
      "recommendation": "Either (1) fix the MLP probe training (likely needs learning rate tuning, longer training, or proper cross-validation) and re-run, or (2) remove the nonlinear probe claims entirely and note that the linear-vs-nonlinear comparison was not successfully conducted. Do not present a broken experiment as a null result."
    },
    {
      "id": "D010",
      "severity": "minor",
      "category": "technical_accuracy",
      "description": "The selectivity values reported in Table 4 are described as computed at each model's 'peak probe accuracy layer'. For M3, positions 0, 1, 3 have peak at layer 0, and position 2 at layer 12. For M5, all positions peak at layer 12. However, the selectivity_recomputed.json data shows selectivity values at ALL layers, not just the peak layer. The reported values in the manuscript (+52.0pp for M3 at position 3) correspond to the selectivity at the peak layer for that position. This is methodologically sound but should be stated more explicitly: the selectivity is measured at the single layer where the matched-step probe achieves its highest accuracy, which biases the selectivity estimate upward (cherry-picking the best layer). A more conservative approach would report the mean selectivity across all layers for each position.",
      "location": "Section 4.3, Table 4",
      "evidence": "selectivity_recomputed.json shows that M3 position 3 selectivity ranges from 0.22 (layer 8) to 0.52 (layers 0 and 12), and M5 position 3 selectivity ranges from -0.007 (layer 8) to 0.52 (layer 12). Reporting only the peak layer value overstates the consistency of the selectivity signal.",
      "recommendation": "Add a note that selectivity is reported at the peak probing layer for each position. Consider also reporting the mean selectivity across layers for each position, which would give a more conservative estimate. For M5, position 3 selectivity is only high at layers 11-12, suggesting that the step-specific encoding is a late-stage phenomenon in M5."
    },
    {
      "id": "D011",
      "severity": "major",
      "category": "missing_experiments",
      "description": "The paper lacks attention pattern analysis. Given that the key claim is about how information flows (or does not flow) through thought positions, examining attention weights would provide direct evidence. Specifically: (1) Does position 3 receive more attention from the answer-generating position than other thought positions? (2) Do early thought positions (0-2) attend heavily to input tokens, suggesting they are building representations from the input rather than from prior thought tokens? (3) In M3, does the sequential forward-pass structure produce different attention patterns than M5's single-pass architecture? Attention patterns would distinguish between the 'broadcast-then-attend' strategy the paper proposes and alternative information routing strategies. This is standard methodology in mechanistic interpretability and its absence is notable.",
      "location": "Missing from the manuscript",
      "evidence": "The paper proposes a 'broadcast-then-attend' strategy (Section 4.3, Section 5.2) but provides no attention analysis to support it. The probing results show where information is stored but not how it flows.",
      "recommendation": "Add an attention pattern analysis for both M3 and M5. Visualize attention from the answer-generating position to thought positions, and from thought positions to input positions. This would directly test the 'broadcast-then-attend' interpretation and could reveal important differences between M3 and M5 that the current methodology misses."
    },
    {
      "id": "D012",
      "severity": "minor",
      "category": "novelty",
      "description": "The paper's core finding -- that COCONUT's continuous thought tokens are largely inert -- has been independently established by Zhang et al. (2025) on different tasks (MMLU, HotpotQA) and at larger scale (LLaMA 7B/8B). The paper acknowledges this and positions itself as extending the finding to ProsQA (COCONUT's strongest task). However, the novelty is somewhat reduced by the concurrent work. The paper's unique contributions are: (a) the curriculum-matched M5 baseline, which provides a constructive alternative rather than just ablation; (b) the OOD generalization analysis showing a task-dependent tradeoff; and (c) the probing analysis showing identical selectivity patterns. Of these, (a) is the strongest novel contribution. The paper should be clearer about which findings are confirmatory (extending Zhang et al.) and which are genuinely new.",
      "location": "Section 2.2, Section 5.4",
      "evidence": "Zhang et al. (2025) found causal inertness of COCONUT thoughts on MMLU and HotpotQA. This paper finds the same on ProsQA. The novel contribution is the M5 baseline and the OOD tradeoff analysis.",
      "recommendation": "Restructure the contributions to more clearly delineate: (1) confirmatory replication of Zhang et al.'s causal inertness finding on ProsQA (the strongest-case domain for COCONUT), (2) novel M5 curriculum-matched baseline methodology (applicable beyond COCONUT), and (3) novel finding that the recycling mechanism introduces a task-dependent generalization tradeoff rather than uniform benefit or deficit."
    },
    {
      "id": "D013",
      "severity": "minor",
      "category": "interpretation",
      "description": "The DAG result (M3 outperforms M5 by 7.3pp) is interpreted as suggesting that 'sequential accumulation of state through recycling may provide a useful inductive bias for tracking path convergence.' This is speculative and the paper does not provide evidence for this specific mechanism. An equally plausible explanation is that M3's 6x computational depth simply provides more representational capacity for handling novel graph structures, and DAGs happen to require more computation than extended chains. Without targeted ablations (e.g., varying the number of forward passes in M3, or providing M5 with more attention positions for DAG problems), the 'path convergence' interpretation is underdetermined.",
      "location": "Section 4.4, Section 5.3",
      "evidence": "M3 outperforms M5 on DAG by 7.3pp (p = 0.001). The interpretation invokes 'path convergence' but this is post-hoc. DAGs also have other structural differences from trees (multiple parents, convergent paths, potential cycles in the general case) that could explain the advantage through different mechanisms.",
      "recommendation": "Present the DAG advantage more cautiously. State that M3 outperforms on DAG structures but that the mechanistic explanation (path convergence benefiting from sequential state accumulation) is speculative. Note alternative explanations (computational depth, structural novelty)."
    },
    {
      "id": "D014",
      "severity": "minor",
      "category": "framing",
      "description": "The title 'Does COCONUT Reason or Buffer?' sets up a binary that the paper's own evidence complicates. The probing results show structured, step-specific encoding that is neither pure 'reasoning' (in the sense of sequential BFS) nor pure 'buffering' (in the sense of content-free computation). The evidence is most consistent with 'curriculum-driven structured computation' where both models learn a specific representational strategy for the task, but this strategy does not require the recycling mechanism. The binary framing may attract readers but risks oversimplifying the contribution.",
      "location": "Title, throughout",
      "evidence": "The paper's evidence shows: (1) thought positions are not content-free buffers (they encode step-specific entities with 52pp selectivity); (2) they are not sequential reasoning chains (permutation insensitive, transplant tolerant); (3) the representational strategy is identical between architectures, arising from the curriculum.",
      "recommendation": "Consider a title that better captures the nuance, e.g., 'Dissecting Latent Thought Tokens: Curriculum, Not Mechanism, Drives COCONUT's Performance on ProsQA' or similar. Alternatively, keep the title but address the false dichotomy explicitly in the introduction."
    },
    {
      "id": "D015",
      "severity": "suggestion",
      "category": "missing_experiments",
      "description": "The paper would benefit from a per-hop-count breakdown of in-distribution accuracy for M3 vs M5. ProsQA contains paths of 3-6 hops, and the performance difference between models may concentrate at specific hop counts. If M3 outperforms M5 primarily on 5-6 hop problems (where the sequential pipeline has more room to contribute), this would provide additional evidence about when the recycling mechanism matters. The OOD results show this pattern extrapolates to 7-8 hops, but the ID breakdown would be informative.",
      "location": "Section 4.1",
      "evidence": "Table 2 reports aggregate accuracy only. The 2.4pp gap between M3 (98.0%) and M5 (95.6%) on the test set might concentrate at specific hop counts.",
      "recommendation": "Add a per-hop-count accuracy breakdown for the in-distribution test set. This would connect the ID results to the OOD findings and clarify whether the recycling mechanism provides a marginal benefit even in-distribution for longer chains."
    },
    {
      "id": "D016",
      "severity": "suggestion",
      "category": "missing_experiments",
      "description": "The probing analysis tests whether thought positions encode the entity at the corresponding step of the reasoning path. This tests for BFS-like sequential encoding. However, Zhu et al. (2025) proved that continuous thought tokens can encode superposition states representing multiple frontier nodes simultaneously. The paper's probes would not detect such superposition encoding because they classify a single entity label. A probe designed to decode whether a position contains information about multiple entities (e.g., a multi-label classification or a representational similarity analysis comparing thought-position activations to embeddings of all entities in the graph) would provide a more targeted test of the BFS superposition hypothesis. The paper briefly acknowledges this in Section 5.4 but does not pursue it.",
      "location": "Section 5.4",
      "evidence": "Section 5.4 states: 'A probe designed to decode multiple frontier nodes simultaneously would provide a more targeted test of the BFS hypothesis and could reveal representational differences between M3 and M5 that our current analysis does not capture.'",
      "recommendation": "This is correctly identified as a limitation. Consider adding a representational similarity analysis (RSA) that compares thought-position activations to the full set of entity embeddings, which could detect superposition encoding without requiring explicit multi-label classification."
    },
    {
      "id": "D017",
      "severity": "suggestion",
      "category": "concurrent_work",
      "description": "The paper should check for and discuss any concurrent work on COCONUT analysis that may have appeared since Zhang et al. (2025). The COCONUT paper (Hao et al., 2024) has generated significant interest, and there may be other concurrent analyses of its thought tokens from different research groups. Additionally, concurrent work on latent reasoning in other architectures (e.g., Quiet-STaR from Zelikman et al., 2024) may provide useful context for understanding when latent reasoning mechanisms do vs. do not provide benefits.",
      "location": "Section 2.2, Section 5.4",
      "evidence": "The paper cites Zhang et al. (2025) and Zhu et al. (2025) as concurrent/follow-up work on COCONUT. Other concurrent analyses may exist.",
      "recommendation": "Conduct a thorough literature search for concurrent COCONUT analyses. Consider citing Zelikman et al. (2024) Quiet-STaR as an alternative approach to latent reasoning that provides a useful comparison point."
    },
    {
      "id": "D018",
      "severity": "minor",
      "category": "technical_accuracy",
      "description": "The paper states that M3 and M5 share 'the same number of attention positions occupied by thought tokens during both training and inference' (Section 3.2). However, the computational structure is fundamentally different. In M3, each thought position in forward pass k can only attend to (a) all input tokens and (b) the single recycled hidden state at the current position from the previous pass. In M5, each thought position can attend to all input tokens AND all other thought positions simultaneously via standard causal attention. M5's thought positions have access to a richer attention context (they can attend to each other), while M3's thought positions are informationally isolated from each other except through the sequential recycling chain. This is an important architectural difference that the paper should discuss more carefully, as it may explain why M5 can achieve comparable performance with 1/6 the FLOPs: M5's attention structure is strictly more connected than M3's for the thought-token region.",
      "location": "Section 3.2",
      "evidence": "M3: thought position k in pass k can attend to input tokens and the recycled hidden state at position k (from pass k-1). M5: thought position k can attend to input tokens and all thought positions 0 through k-1 via causal self-attention.",
      "recommendation": "Add a discussion of the attention structure difference. M5's thought positions can attend to each other through standard causal attention, while M3's are informationally linked only through the sequential recycling chain. This connectivity difference is likely important for understanding the OOD generalization results."
    },
    {
      "id": "D019",
      "severity": "suggestion",
      "category": "impact",
      "description": "The paper's practical implications section (5.5) correctly identifies that curriculum design is the higher-leverage investment. However, it could strengthen its impact by providing more concrete guidance for researchers building latent reasoning systems. Specifically: (1) What properties of the curriculum drive the performance? Is it the progressive removal, the number of stages, the epoch budget per stage? (2) Would a 3-stage curriculum work as well as a 7-stage one? (3) Is the specific initialization of the pause embedding important, or would random initialization work? These are actionable research questions that follow directly from the paper's findings and would increase its practical value.",
      "location": "Section 5.5",
      "evidence": "The paper identifies the curriculum as the key ingredient but does not decompose which properties of the curriculum matter.",
      "recommendation": "Add a brief discussion of which curriculum properties are likely to matter based on the results, and flag specific ablations (number of stages, epoch budget, initialization) as high-value follow-up experiments."
    },
    {
      "id": "D020",
      "severity": "minor",
      "category": "technical_accuracy",
      "description": "The paper reports M3's test accuracy as 98.0% (Table 2) but the corruption results (Table 3, results.json) show clean accuracy of 97.0%. This 1pp discrepancy likely reflects a difference between evaluation methods (the corruption experiment may use a different evaluation procedure or a different subset). The paper should note and explain this discrepancy.",
      "location": "Table 2 vs Table 3",
      "evidence": "Table 2: M3 test accuracy = 98.0%. Table 3: M3 clean accuracy = 97.0%. corruption/results.json: clean_accuracy = 0.97. The corruption experiments may use the trained model at a different checkpoint or with a different decoding strategy.",
      "recommendation": "Explain the 1pp discrepancy between Table 2 and Table 3 for M3's accuracy. If the corruption experiments use a different evaluation procedure (e.g., greedy decoding vs. the evaluation used for Table 2), state this explicitly."
    },
    {
      "id": "D021",
      "severity": "suggestion",
      "category": "framing",
      "description": "The paper's M5 baseline is a strong and well-motivated control. However, the paper could better contextualize what M5 represents theoretically. M5 is not just a 'pause token' model -- it is a model that performs the same curriculum-driven training as COCONUT but routes all inter-step computation through standard self-attention rather than through an explicit recurrence loop. This makes M5 architecturally similar to a standard transformer with extra padding tokens, trained with a specific curriculum. The fact that this achieves 95.6% accuracy on ProsQA is itself a significant result: it demonstrates that standard transformer attention is sufficient for multi-hop reasoning when the training curriculum is appropriate, without any architectural modifications beyond adding learnable tokens. This relates to the broader debate about whether transformers need architectural augmentation for reasoning tasks.",
      "location": "Section 3.2, Section 5.5",
      "evidence": "M5 = GPT-2 + 1 learned embedding + standard causal attention + COCONUT's 7-stage curriculum = 95.6% on ProsQA. This is itself a strong result that deserves more emphasis.",
      "recommendation": "Add a brief discussion framing M5 as evidence that standard transformer attention, combined with appropriate curriculum design, is sufficient for multi-hop reasoning on ProsQA. This connects to the broader debate about whether transformers need architectural modifications for reasoning."
    }
  ],
  "overall_assessment": "pass_with_conditions",
  "summary": "This paper makes a genuine and valuable contribution by constructing a well-controlled curriculum-matched baseline (M5) that isolates the role of COCONUT's hidden-state recycling mechanism from its training curriculum. The experimental methodology is thorough: the corruption, probing, and OOD generalization experiments converge on a consistent picture, and the statistical analysis is rigorous (McNemar's test with Bonferroni correction, independently verified). The single-seed limitation is honestly acknowledged, and the data files show meticulous verification.\n\nHowever, the paper's central framing -- the binary 'reasoning vs buffering' dichotomy -- is not well-supported by its own evidence. The probing results show +52pp selectivity at position 3 in both models, which is not 'buffering' in any conventional sense. Both models learn structured, position-specific representational strategies driven by the shared curriculum. The correct conclusion is not that COCONUT 'buffers' but that its curriculum -- not its recycling mechanism -- drives a structured computational strategy that does not require inter-step hidden-state propagation. This is a more nuanced and arguably more interesting finding than the binary framing suggests. The nonlinear probe results (0.0% accuracy everywhere) indicate a clear implementation failure that should be fixed or removed. The missing curriculum-only ablation and attention pattern analysis would substantially strengthen the interpretive claims.\n\nThe paper's strongest contributions are: (1) the M5 control methodology, which is applicable beyond COCONUT to any architecture claiming gains from a mechanism confounded with a curriculum; (2) the OOD generalization finding that the recycling mechanism introduces a task-dependent tradeoff rather than a uniform benefit; and (3) the demonstration that curriculum design is the higher-leverage intervention. With the framing adjusted and the nonlinear probe issue resolved, this paper would make a solid contribution to the understanding of latent reasoning architectures."
}
