{
  "reviewer": "Methodologist",
  "checkpoint": "manuscript_review_round2",
  "round": 2,
  "timestamp": "2026-02-14T18:30:00Z",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/manuscript.md",
    "papers/efficient_architecture_proof/code/coconut.py",
    "papers/efficient_architecture_proof/code/exp_corruption.py",
    "papers/efficient_architecture_proof/code/exp_probing.py",
    "papers/efficient_architecture_proof/code/args/prosqa_coconut_1gpu.yaml",
    "papers/efficient_architecture_proof/code/args/prosqa_m5_pause.yaml",
    "papers/efficient_architecture_proof/results/experiments/mcnemar/results.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/statistical_analysis.json",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json",
    "papers/efficient_architecture_proof/results/m3_linear_perm.json",
    "papers/efficient_architecture_proof/results/m3_test_eval.json",
    "papers/efficient_architecture_proof/results/m5_test_eval.json",
    "papers/efficient_architecture_proof/reviews/methodologist_review.json",
    "papers/efficient_architecture_proof/reviews/skeptic_review.json",
    "papers/efficient_architecture_proof/reviews/statistician_review.json"
  ],
  "findings": [
    {
      "id": "F001",
      "severity": "critical",
      "category": "design",
      "description": "Forward-pass asymmetry remains the single most consequential uncontrolled confound. M3 executes 6 sequential forward passes through 12 transformer layers (72 effective layers of computation), while M5 executes a single forward pass (12 effective layers). The manuscript acknowledges this in Section 3.2 and Section 6, framing it as directionally favorable to the paper's argument because M5 achieves comparable accuracy with 6x fewer FLOPs. However, the confound is bidirectional. For the in-distribution comparison, the argument is that M3's additional compute provides no benefit -- supporting the paper's thesis. But for the OOD comparison, the direction reverses: M5's advantages on 7-hop, 8-hop, and dense tests could reflect the benefits of single-pass parallel attention (which can integrate information across all positions simultaneously) versus M3's serialized computation (which forces a bottleneck through sequential passes). Conversely, M3's DAG advantage could reflect 6x greater computation depth rather than meaningful hidden-state recycling. The paper's 'sequential bottleneck' interpretation (Section 5.3) is one hypothesis, but an equally valid alternative is that 72 effective layers overfit to training-distribution patterns while 12 effective layers generalize better -- a capacity/depth regularization effect having nothing to do with the content of recycled states. The manuscript now explicitly acknowledges this limitation and proposes a third control (6 forward passes with fixed re-injected embeddings) as future work. This is appropriate but does not resolve the interpretive ambiguity in the current paper. The paper would benefit from explicitly framing the OOD results as reflecting 'the combined effect of recycling mechanism and multi-pass depth' rather than attributing them specifically to the sequential bottleneck of recycled content.",
      "location": "Section 3.2 (paragraph 3), Section 5.3, Section 6 (Forward-pass asymmetry)",
      "recommendation": "Weaken causal claims in Section 5.3 by replacing 'COCONUT's hidden-state recycling imposes a sequential bottleneck' with language that explicitly names the two confounded factors: 'COCONUT's architecture combines hidden-state recycling with multi-pass sequential computation, imposing both a content-dependent and a depth-dependent bottleneck that cannot be disentangled in the current design.' Add a sentence to the OOD discussion acknowledging the capacity-regularization alternative: M5's single pass through 12 layers may benefit from implicit regularization that prevents overfitting to training-distribution patterns, independent of thought-token content.",
      "resolution_required": true
    },
    {
      "id": "F002",
      "severity": "critical",
      "category": "design",
      "description": "The manuscript has now resolved the checkpoint inconsistency from the round 1 review (M002). Table 2 reports 'Training-time evaluation at best epoch yielded slightly higher estimates for M3 (98.0%) and lower for M5 (95.6%), a discrepancy of 5 samples per model attributable to differences in the inference code path; we use the experiment-pipeline numbers for consistency with all subsequent analyses.' Table 2 now reports M3 = 97.0% and M5 = 96.6% as the primary numbers, with the training-time numbers footnoted. This is a substantial improvement. However, the '85% gap closure' figure that appeared in the round 1 manuscript has been removed from the abstract and introduction. The abstract now states 'M5 reaches 96.6% test accuracy, statistically indistinguishable from COCONUT's 97.0%' rather than framing it as gap closure. This is appropriate. RESOLVED from round 1.",
      "location": "Table 2, Abstract",
      "recommendation": "No further action needed on the checkpoint issue. The resolution is adequate.",
      "resolution_required": false
    },
    {
      "id": "F003",
      "severity": "major",
      "category": "design",
      "description": "Single training seed (seed 0) for all three models. This was flagged in round 1 (M003) and acknowledged in Section 6. The manuscript now includes appropriate caveats about seed-specific effects. However, the severity remains major because the OOD results -- the most novel and impactful finding in the paper -- are the most seed-sensitive. The disagreement patterns (32-40% of OOD samples have exactly one model correct) suggest the two architectures develop genuinely different strategies, but whether M5 systematically outperforms M3 on longer chains or whether this is an accident of the specific random initialization cannot be determined from a single seed. The McNemar tests address within-sample variance but not between-seed variance. The statistical_analysis.json file confirms n_seeds=1 with null confidence intervals throughout. This is a fundamental limitation that the paper handles honestly but that significantly constrains the strength of the OOD claims.",
      "location": "Section 6 (Single seed), statistical_analysis.json",
      "recommendation": "If additional training runs are feasible, run at minimum 3 seeds for M3 and M5 and report mean +/- std for OOD accuracy. If not feasible, add a sentence to the OOD results section (not just Limitations) acknowledging that the OOD advantages are preliminary single-seed observations. Consider softening the OOD-related conclusion from 'M5 outperforms COCONUT on 3 of 4 test sets' to 'In a single-seed comparison, M5 outperforms COCONUT on 3 of 4 test sets; multi-seed replication is needed to confirm robustness.'",
      "resolution_required": true
    },
    {
      "id": "F004",
      "severity": "major",
      "category": "design",
      "description": "Missing curriculum-only control. The paper identifies the curriculum as the primary driver of performance but does not test a condition where removed CoT tokens are simply deleted (no thought tokens, no pause tokens). This was flagged in round 1 (M009) and is now acknowledged in Section 6 (Curriculum isolation). Without this control, the paper cannot distinguish: (a) the curriculum alone drives performance, (b) the curriculum + additional attention positions (compute budget) drive performance. The paper's central claim is that curriculum, not mechanism, is primary -- but the curriculum is not tested alone. M5 demonstrates that the mechanism is unnecessary, but it cannot demonstrate that the attention positions are unnecessary. The current evidence is consistent with 'curriculum + any filler tokens' being the operative combination, which is a weaker claim than 'curriculum is primary.'",
      "location": "Section 6 (Curriculum isolation)",
      "recommendation": "Reframe the central claim to 'the specific content of thought tokens (recycled hidden states vs. fixed pause embeddings) does not drive in-distribution performance' rather than 'the curriculum drives performance.' The paper's evidence supports the former claim strongly but the latter only indirectly. If a curriculum-only ablation is feasible, it would be the highest-impact additional experiment.",
      "resolution_required": true
    },
    {
      "id": "F005",
      "severity": "major",
      "category": "design",
      "description": "The corruption noise calibration creates a fundamentally different perturbation regime for M3 and M5, undermining direct comparison of degradation curves. M3's thought tokens are recycled hidden states with standard deviation 7.31 and mean L2 = 202.8. M5's thought tokens are near-identical copies of a single learned embedding with standard deviation 0.15 and mean L2 = 4.09. Per-model calibrated noise preserves the SNR relative to each model's own scale, but produces incomparable absolute perturbations. The cross-corruption analysis (applying M3-magnitude noise to M5) is helpful but has a different interpretation than intended: for M5, it is effectively a REPLACEMENT of thought tokens with random vectors (since the noise magnitude is 50x the signal), not a perturbation. The paper acknowledges this in Section 6 (Corruption noise calibration). The key finding -- both models exhibit the same position-4 cliff -- is robust because it appears under both per-model calibrated and cross-scale noise. However, the progressive degradation curves (Table 3) are not directly comparable between models: M3's gradual decline from 97.0% to 96.8% over positions 0-3 could reflect noise partially corrupting meaningful signal, while M5's decline from 96.6% to 95.8% could reflect a different effect of noise on the attention mechanism's use of those positions.",
      "location": "Section 3.4 (Experiment 1), Section 4.2, Appendix A.2",
      "recommendation": "Add a clarifying sentence to Section 4.2 noting that the degradation curves above the cliff (positions 0-3) are not directly comparable between models due to the 50x difference in perturbation scale, and that the key finding is the structural cliff location (position 4), which is robust across noise scales. Consider testing corruption at multiple noise levels (e.g., 0.5x, 1x, 2x, 5x, 10x each model's SD) to map the full sensitivity landscape rather than using a single noise level per condition.",
      "resolution_required": false
    },
    {
      "id": "F006",
      "severity": "major",
      "category": "design",
      "description": "The permutation experiment tests only argmax (binary correct/incorrect) sensitivity, not continuous sensitivity. With temperature=0 decoding, the model always outputs the argmax token. A permutation that changes internal activations but does not change which token has the highest logit will register as 0% flip rate even if the model IS using order information. This was flagged in round 1 (M005). The manuscript now includes Appendix A.4 with a formal power analysis showing that 5,000 trials excludes true flip rates above 0.06% at 95% confidence, which is appropriate for the binary outcome. However, the deeper concern is not about statistical power for the binary test -- it is about the sensitivity of the binary measure itself. If permutation changes the logit margin from 99.5% confidence to 51% confidence but the argmax remains correct, the 0% flip rate is misleading. The paper does not analyze logit changes or probability distributions under permutation. The per_sample_logprobs.json file exists in the results directory but is not referenced in the permutation analysis.",
      "location": "Section 4.2 (Permutation sensitivity), Appendix A.4",
      "recommendation": "Analyze the per-sample log-probability difference (correct token logit minus next-best token logit) under clean vs. permuted conditions. If the mean logit margin does not change under permutation (or changes by less than, say, 0.1 nats), this would substantially strengthen the 0% flip rate finding. If the margin narrows significantly under permutation even though the argmax does not flip, this would indicate that the model IS using order information and the 0% flip rate reflects a ceiling effect on a coarse measure.",
      "resolution_required": false
    },
    {
      "id": "F007",
      "severity": "major",
      "category": "design",
      "description": "The transplant experiment may be insensitive because thought tokens are redundant with input tokens. Both M3 and M5 receive the full problem input (all graph rules and the question) as input tokens. The thought tokens occupy positions after the input. If the model can solve the problem from the input tokens alone (attending to them through standard self-attention), then the content of thought tokens is irrelevant, and transplanting thoughts from a different problem will have no effect regardless of whether they carry problem-specific information. The transplant accuracy (M3: 97.0% matched, 97.5% unmatched; M5: 96.5% matched, 96.5% unmatched) matches clean accuracy exactly, which is consistent with both (a) thoughts carrying no problem-specific information and (b) thoughts being redundant with input. This ambiguity was flagged in round 1 (M008). The unmatched transplant results (unmatched_transplant.json) show that even randomly pairing donor-recipient problems with no hop-count matching produces identical accuracy, which strengthens interpretation (a) but does not rule out (b). A decisive test would transplant thoughts while also degrading input tokens, forcing the model to rely on thought-token content.",
      "location": "Section 3.4 (Cross-problem transplant), Section 4.2",
      "recommendation": "Acknowledge the redundancy interpretation in the text. Add a sentence noting that the transplant result demonstrates thoughts are not necessary for correct answers (given intact input) but cannot demonstrate they carry no information. Consider a combined corruption+transplant experiment where some input tokens are masked and the model must rely on thought tokens to maintain accuracy -- if transplanted foreign thoughts do not help recover accuracy that corrupt inputs remove, this would more strongly support the 'no information' interpretation.",
      "resolution_required": false
    },
    {
      "id": "F008",
      "severity": "major",
      "category": "design",
      "description": "Probing selectivity measures information presence, not functional use. The paper correctly cites Ravichander et al. (2021) on this distinction (Section 5.2) and uses it to interpret the finding that M3's 29/78 significant probing cells vs. M5's 11/78 do not translate into behavioral advantages. However, the same logic applies to the paper's positive claims about selectivity. The finding that both models show +52pp selectivity at position 3 demonstrates that step-specific information is PRESENT at position 3, not that the model USES position-3 information. Single-position corruption shows that corrupting position 3 collapses accuracy, but this could reflect attention-pattern disruption (the model learned to attend to position 3, so any disturbance there is catastrophic) rather than the model extracting step-specific reasoning information from position 3. The distinction matters because the paper interprets the selectivity pattern as revealing the model's 'representational strategy' (Section 5.2), implying functional use rather than mere presence.",
      "location": "Section 4.3, Section 5.2",
      "recommendation": "Add a caveat acknowledging that the selectivity pattern reveals which positions encode step information, not that the model uses that information for step-level reasoning. The single-position corruption result demonstrates that position 3 is functionally important, but the probe result demonstrates only that step information is decodable there -- the model may use position 3 for a different purpose (e.g., encoding the final answer directly) that happens to be correlated with the last reasoning step.",
      "resolution_required": false
    },
    {
      "id": "F009",
      "severity": "major",
      "category": "design",
      "description": "The probing methodology uses a cross-position selectivity measure that differs from the standard Ravichander et al. (2021) selectivity control. Ravichander et al. use random label baselines (probing with shuffled labels) to establish a selectivity floor. This paper instead compares matched-step accuracy against the best cross-position accuracy: selectivity(l,t) = probe_acc(target=step_t) - max_{s!=t} probe_acc(target=step_s). The paper argues this is 'a stricter test' (Section 3.4). While cross-position comparison is informative, the max over alternative steps introduces selection bias: by taking the maximum accuracy across alternative targets, the control accuracy is inflated by the strongest alternative, making it harder to achieve positive selectivity. This may explain the anti-selectivity at positions 0-1: these positions decode step 2 (the final hop for many problems) better than their matched step, but this may partly reflect that step 2 is the most decodable target everywhere due to having the most training signal (all 500 samples have step 2, while step 0 has 500, step 3 has 298, step 4 has 81). The choice of max rather than mean for the control baseline biases toward finding anti-selectivity at positions that happen to decode one specific alternative step well.",
      "location": "Section 3.4 (Experiment 2), selectivity_recomputed.json",
      "recommendation": "Report selectivity using both the max control (as currently done) and the mean control: selectivity_mean(l,t) = probe_acc(target=step_t) - mean_{s!=t} probe_acc(target=step_s). If the selectivity profiles differ meaningfully between max-control and mean-control, discuss the implications. Also report the Ravichander et al. random-label baseline for comparison. The combination of three selectivity metrics (max-control, mean-control, random-baseline) would provide a more complete picture.",
      "resolution_required": false
    },
    {
      "id": "F010",
      "severity": "major",
      "category": "design",
      "description": "The nonlinear probe results (0/78 cells with MLP > linear) are almost certainly a training failure artifact rather than a genuine null finding. The manuscript acknowledges this concern in Appendix A.7 but still includes the finding in the main text and Table 4 ('Cells where MLP > linear: 0/78' for both models). Default scikit-learn MLPClassifier with early_stopping=True, validation_fraction=0.1 on samples as small as n=12 (position 5) or n=81 (position 4) will have severe convergence problems. Even at position 3 (n=298) with 38+ target classes, the default learning rate schedule may be inadequate. The manuscript concedes this but draws the conclusion 'no evidence of nonlinearly encoded step information' -- which is not warranted when the MLP probes produced 0.0% accuracy everywhere (including at cells where linear probes achieve 55%). If the MLP probes had achieved accuracy EQUAL to linear probes (confirming convergence) but not exceeding them, the null finding would be informative. Achieving 0.0% everywhere indicates the MLPs failed to learn at all.",
      "location": "Section 4.3, Table 4, Appendix A.7",
      "recommendation": "Remove the '0/78 cells where MLP > linear' from Table 4 and the main text. Replace with a note that MLP probes failed to converge under default hyperparameters and that the presence of nonlinear encoding remains untested. Alternatively, retrain MLP probes with a proper hyperparameter search (grid over learning rates {1e-2, 1e-3, 1e-4}, hidden sizes {64, 128, 256, 512}, max_iter {500, 1000, 2000}) and report results only after confirming convergence (MLP accuracy >= linear accuracy at the same cell).",
      "resolution_required": true
    },
    {
      "id": "F011",
      "severity": "minor",
      "category": "design",
      "description": "The OOD test sets vary single factors against the training distribution but do not cross factors, limiting the ability to disentangle topology-dependent and length-dependent effects. The 7-hop and 8-hop sets extend path length but preserve tree topology and training-range branching factor. The DAG set changes topology but preserves training-range path lengths (3-6). The Dense set changes branching factor but preserves tree topology and training-range path lengths. There is no '7-hop DAG' or 'dense 8-hop' condition. The paper's claim that M3's DAG advantage reflects 'path convergence' (Section 5.3) would be testable with longer DAGs: if M3's DAG advantage persists or grows at 7-8 hops, convergent-path reasoning is sustained under extrapolation; if it reverses, the sequential bottleneck dominates at longer chains even for DAGs.",
      "location": "Section 3.4 (Experiment 3), Table A7",
      "recommendation": "Acknowledge this design limitation and suggest crossed OOD conditions (e.g., 7-hop DAG, 8-hop DAG, dense 7-hop) as future work. This would enable a 2x2 analysis of path-length extrapolation vs. topology generalization.",
      "resolution_required": false
    },
    {
      "id": "F012",
      "severity": "minor",
      "category": "design",
      "description": "Position 4 (n=81) probe accuracy of 0.0% for all layers across both models is suspicious and may indicate a data processing issue. With 81 samples and ~32 unique target classes (since not all 38 species appear at position 4), chance accuracy should be roughly 3% (1/32). 5-fold cross-validation with 81 samples across 32 classes gives approximately 16 samples per fold, with fewer than 1 sample per class per fold on average, making stratified splits unreliable. Examining the selectivity_recomputed.json data confirms all position-4 entries are exactly 0.0 for both matched accuracy and all cross-position accuracies. This is consistent with a systematic failure (e.g., the stratified CV fails because some classes have only 1 sample and cannot be split across folds, causing scikit-learn to silently return 0 or NaN). The paper correctly excludes position 4-5 from quantitative claims but includes them in the full grids (Tables A5-A6) with zeros that could mislead readers.",
      "location": "Tables A5-A6, Appendix A.1",
      "recommendation": "Investigate whether the 0.0% at position 4 reflects a CV failure (too few samples per class for stratified 5-fold) or genuine absence of signal. If the former, use leave-one-out CV or reduce to 3 folds for positions 4-5. If genuinely 0.0%, report it with a note explaining that the sample/class ratio precludes meaningful evaluation. Consider marking position 4-5 cells as 'N/A' rather than 0.0 in the tables.",
      "resolution_required": false
    },
    {
      "id": "F013",
      "severity": "minor",
      "category": "design",
      "description": "M1 (CoT) is included only in OOD evaluation but not in corruption, probing, or transplant experiments. Running these experiments on M1's explicit CoT tokens would provide a positive control: CoT tokens SHOULD show permutation sensitivity (reordering 'Alex is a jompus. Jompus is a zhorpus.' should disrupt the reasoning chain), problem-specificity (CoT for problem A should not help solve problem B), and step-specific encoding (position k should encode step k by construction). If these diagnostics detect sequential reasoning in M1, they are validated as methods that CAN detect reasoning when present. Without this positive control, the null results for M3 and M5 could reflect insensitive methods rather than absent reasoning.",
      "location": "Section 3.4, across all experiments",
      "recommendation": "Run the permutation, transplant, and probing experiments on M1's CoT tokens as a positive control. Report the results in a supplementary table. If M1's CoT tokens show the expected sensitivity patterns, note this as validation of the experimental paradigm.",
      "resolution_required": false
    },
    {
      "id": "F014",
      "severity": "minor",
      "category": "design",
      "description": "The paper reports DAG Bonferroni-corrected p = 0.0015 in Table 5 and p = 0.001 in Section 5.3. The actual value from mcnemar/results.json is p_bonferroni = 0.00145697 (exact) or 0.00151001 (chi-square approximation). Both are rounded to p = 0.0015 in Table 5, which is accurate. However, Section 5.3 reports 'p = 0.001, Bonferroni-corrected' which rounds DOWN, overstating significance. The appendix_data.json file labels this as 'dag_bonferroni_rounded = 0.001'. This inconsistency between Table 5 (p = 0.0015) and Section 5.3 (p = 0.001) should be resolved.",
      "location": "Table 5 vs. Section 5.3",
      "recommendation": "Use p = .0015 consistently throughout, matching Table 5 and the verified data.",
      "resolution_required": false
    },
    {
      "id": "F015",
      "severity": "minor",
      "category": "design",
      "description": "The manuscript describes M5's pause embedding as 'a single learned embedding vector of 768 dimensions (nn.Parameter), repeated identically at all six thought positions' (Section 3.2). Confirmed in coconut.py: self.pause_embedding = nn.Parameter(self.embedding.weight.data[latent_token_id].clone()). This is initialized as a clone of the <latent> token embedding. During training, this parameter is updated by gradient descent, meaning it can learn to encode task-relevant information. The manuscript does not report what the final learned pause embedding looks like -- whether it converges to something meaningfully different from its initialization, or whether it remains close to the <latent> token embedding. Since M5 achieves 96.6% accuracy using this single learned vector at all 6 positions, the content of this vector could be important: it might encode a task-specific 'reasoning mode' signal. The paper claims the pause embedding carries 'no information from one reasoning step to the next,' which is true architecturally (no inter-step information flow), but the embedding itself could carry substantial task-level information that aids reasoning.",
      "location": "Section 3.2, coconut.py line 50-52",
      "recommendation": "Report the L2 distance between the initial pause embedding (at <latent> token initialization) and the final learned pause embedding after training. If this distance is large, the model learned a task-specific embedding, which is a mild form of 'information in the thought tokens' (just not step-specific information). Report this in the methods or appendix for completeness.",
      "resolution_required": false
    },
    {
      "id": "F016",
      "severity": "minor",
      "category": "design",
      "description": "The OOD test sets were all generated using ProsQA's exact vocabulary (38 species, 17 person names) with seed 42. This means the OOD tests vary graph topology and path length but not vocabulary. A model that learned vocabulary-specific associations during training (e.g., 'jompus typically appears at step 2') could exploit these associations on OOD tests in ways that would not transfer to genuinely novel vocabularies. The vocabulary overlap between training and OOD sets means the generalization being tested is structural (graph topology, path length) but not lexical. This is appropriate for the paper's claims about structural generalization but limits the scope of 'out-of-distribution' characterization.",
      "location": "Section 3.4 (Experiment 3), Table A7",
      "recommendation": "Note that OOD tests share vocabulary with the training set and that vocabulary generalization remains untested. This constrains the OOD conclusions to structural generalization only.",
      "resolution_required": false
    },
    {
      "id": "F017",
      "severity": "suggestion",
      "category": "design",
      "description": "The paper does not test whether M5's positional encodings are necessary for its performance. M5's thought tokens are identical at all 6 positions; the only position-distinguishing signal is GPT-2's learned positional encoding. The paper hypothesizes (Section 5.2) that M5's selectivity at position 3 is 'mediated by GPT-2's learned positional encodings.' A simple ablation would confirm this: run M5 inference with all 6 thought positions receiving the same positional encoding (e.g., all mapped to the same absolute position). If accuracy drops significantly, positional encoding is necessary for M5's strategy. If it does not, even positional encoding is unnecessary, and the curriculum alone -- acting through the model's attention patterns over input tokens -- drives performance.",
      "location": "Section 5.2",
      "recommendation": "Test M5 with uniform positional encoding across thought positions as a supplementary ablation. This would test whether the model needs to distinguish thought positions to achieve high accuracy.",
      "resolution_required": false
    },
    {
      "id": "F018",
      "severity": "suggestion",
      "category": "design",
      "description": "The corruption experiment uses Gaussian noise matched to each model's thought-token statistics (mean and standard deviation). This is a blunt perturbation that destroys all structure. If M3's thought tokens encode reasoning information in structured ways (e.g., specific directions in the hidden-state space that correspond to specific reasoning operations), random Gaussian noise would destroy this structure while preserving the overall scale. A more targeted corruption method -- such as projecting thought-token activations onto a random subspace of matching dimensionality, or adding structured noise that preserves some activation statistics while disrupting others -- could reveal whether specific aspects of the thought-token representation are functionally important. The current Gaussian corruption treats all dimensions equally and cannot distinguish between 'all information is noise-like' and 'the information is there but robust to undirected perturbation.'",
      "location": "Section 3.4 (Experiment 1)",
      "recommendation": "Consider supplementary corruption experiments using structured perturbations: (1) project thought-token activations onto the top-k principal components and zero out the remaining dimensions, testing whether the dominant variance directions carry the critical information; (2) add noise only along specific subspaces (e.g., the subspace spanned by the probe weights for step classification); (3) replace thought tokens with the mean thought-token activation across all test samples, which preserves global statistics while removing sample-specific variation.",
      "resolution_required": false
    },
    {
      "id": "F019",
      "severity": "suggestion",
      "category": "design",
      "description": "The paper does not test whether M3's 6 sequential forward passes produce meaningfully different hidden states at each pass, or whether later passes converge to a fixed point. If the recycled hidden states converge after 2-3 passes (i.e., the hidden state at pass k is nearly identical to the hidden state at pass k-1 for k >= 3), this would explain why permutation has no effect and why the model behaves like a buffer: only the first few passes matter, and later passes are redundant repetitions. This would also explain the position-3 selectivity peak: position 3 corresponds to the point at which the hidden states have converged, making it the first position with fully processed information. Testing convergence is straightforward: compute the cosine similarity between successive recycled hidden states across passes.",
      "location": "Section 4.2 (Permutation), Section 5.1",
      "recommendation": "Compute the cosine similarity or L2 distance between M3's recycled hidden states at consecutive passes (pass k vs. pass k+1) for each test sample. Report the convergence curve. If hidden states converge by pass 3-4, this provides a mechanistic explanation for the position-3 selectivity peak and the permutation insensitivity.",
      "resolution_required": false
    },
    {
      "id": "F020",
      "severity": "suggestion",
      "category": "design",
      "description": "The M3 vs. M5 comparison confounds the training dynamics with the final representation. M3 and M5 undergo the same curriculum, but the gradient signals they receive differ: M3 receives gradients that flow through the recycling mechanism (the chain of forward passes), while M5 receives gradients only through a single forward pass. This means M3's curriculum optimization landscape differs from M5's, even though the curriculum schedule is identical. The two models may converge to similar performance for different reasons: M3 through a landscape shaped by multi-pass gradients, M5 through a landscape shaped by single-pass gradients. The paper implicitly assumes that matching the curriculum schedule matches the training experience, but the gradient dynamics are fundamentally different between the two training regimes. This is not a fatal flaw -- it is inherent to any comparison between architecturally distinct models -- but it should be acknowledged as a limitation of the 'curriculum-matched' framing.",
      "location": "Section 3.2, Section 3.3",
      "recommendation": "Add a note acknowledging that while the curriculum schedule is matched, the gradient dynamics during training differ between M3 (multi-pass backpropagation through the recycling chain) and M5 (single-pass backpropagation). This means the 'same curriculum' framing is approximate: both models see the same training data in the same sequence, but the optimization signals they receive from that data differ.",
      "resolution_required": false
    }
  ],
  "overall_assessment": "pass_with_conditions",
  "summary": "This is a well-designed and carefully executed study that addresses a genuine confound in the COCONUT literature. The curriculum-matched control (M5) is a creative and appropriate experimental intervention, and the convergent evidence framework across three experimental paradigms is commendable. The manuscript has improved substantially since round 1: the checkpoint inconsistency is resolved, the gap-closure framing is removed, and the limitations section is honest and substantive. Three issues require resolution before proceeding: (1) the forward-pass asymmetry language in Section 5.3 should be weakened to explicitly acknowledge that depth and content are confounded in the OOD results; (2) the central claim should be reframed from 'curriculum drives performance' to 'thought-token content does not drive performance,' since the curriculum-only condition is not tested; (3) the nonlinear probe result (0/78) should be removed from the main text or reported as a training failure rather than a null finding. The remaining major findings are legitimate methodological limitations that are appropriately handled through caveats rather than additional experiments: the single-seed constraint, the noise calibration asymmetry, the permutation sensitivity floor, the transplant redundancy ambiguity, and the probing-as-presence issue. The paper's core contribution -- demonstrating that a pause baseline matches COCONUT under matched curriculum -- is robust and important regardless of these limitations."
}
