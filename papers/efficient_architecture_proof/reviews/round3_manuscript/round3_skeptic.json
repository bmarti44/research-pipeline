{
  "reviewer": "Skeptic / Devil's Advocate",
  "checkpoint": "manuscript_review_round3",
  "round": 4,
  "timestamp": "2026-02-14T23:45:00Z",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/manuscript.md",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/experiments/ood/results.json",
    "papers/efficient_architecture_proof/results/experiments/corruption/results.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/reviews/skeptic_review.json",
    "papers/efficient_architecture_proof/reviews/round_4_skeptic.json"
  ],
  "findings": [
    {
      "id": "F001",
      "severity": "critical",
      "category": "design",
      "description": "The forward-pass asymmetry remains the paper's most damaging uncontrolled confound, and the manuscript's handling of it -- while now extensive -- still does not fully reckon with its implications for every claim. M3 processes thought tokens through 6 sequential incremental forward passes (each adding one token via KV-cache decoding), giving it approximately 6x the effective computation depth for thought processing. M5 processes all thought tokens in a single forward pass. The paper acknowledges this and describes a missing third control (6-pass with fixed re-injected embedding). However, the paper continues to draw affirmative conclusions from this confounded comparison. Consider the implications systematically: (1) The in-distribution null result (p=0.845) is presented as 'directionally favorable' because M5 matches M3 with less compute. But this is only favorable if the extra compute in M3 is wasted. An equally valid interpretation: M3's extra compute IS useful but ProsQA is easy enough that both models are at ceiling, and the additional computation provides no marginal benefit at 97% accuracy. (2) The corruption experiment showing 'identical degradation profiles' conflates two different corruption operations: for M3, corrupting a thought position disrupts one step of a sequential chain AND removes one forward pass of computation; for M5, it removes one attention position from a single-pass computation. The degradation profile being 'identical' under these structurally different perturbations is NOT expected under either the reasoning or buffering hypotheses -- it requires explanation. (3) The probing experiment's finding that M3 has 29/78 significant cells vs. M5's 11/78 could be entirely attributable to M3 having 6x more computation in which to encode information, rather than to the information content of recycled states. Until the 6-pass fixed-embedding control is run, every cross-model comparison in this paper is a joint test of (recycling content) AND (sequential processing structure), and the paper cannot attribute any result to either factor alone.",
      "location": "Section 3.2 paragraphs 4-5, Section 6 'Forward-pass asymmetry' limitation, all cross-model comparisons in Sections 4.1-4.4",
      "recommendation": "The paper must choose between two paths: (a) Run the missing control -- a model that executes 6 sequential forward passes through the KV-cache incremental loop but re-injects the same fixed pause embedding at each step rather than the recycled hidden state. This would isolate recycled content from sequential compute structure. (b) If this control is infeasible, systematically weaken every cross-model comparison to acknowledge the confound as an interpretation qualifier, not just a limitation. The current approach -- acknowledging the confound in Section 6 while drawing unqualified conclusions in Sections 4-5 -- is inconsistent. Specifically, the convergent evidence table (Table 6) should have a column noting which results are affected by the confound, and the 'Our result' column should note 'confounded with sequential processing structure' for each row.",
      "resolution_required": true
    },
    {
      "id": "F002",
      "severity": "critical",
      "category": "logic",
      "description": "The paper's argument is fundamentally structured as a chain of null results (no permutation sensitivity, no transplant failure, 'identical' corruption profiles, 'identical' selectivity, in-distribution accuracy difference not significant), and no formal equivalence testing is performed for any of them. The in-distribution McNemar test (p=0.845, 95% CI [-2.4, +1.6]) is the only comparison with a reported confidence interval. For every other 'identical' comparison, the paper provides only point estimates and informal characterization. For example: the corruption cliff is described as 'identical' but the actual accuracies are M3=96.8% vs M5=95.8% at 3 positions corrupted (a 1.0pp difference); selectivity at position 3 is 52.0pp vs 52.3pp (a 0.3pp difference); thought-vs-input advantage is 10.5% vs 4.0% (a 6.5pp difference that is NOT small and NOT 'identical'). The thought-vs-input advantage is actually a substantial difference (2.6x ratio) that the paper treats as a secondary observation rather than as potential disconfirming evidence. The paper needs equivalence bounds: what magnitude of difference in corruption profiles, selectivity, or transplant accuracy would the authors consider evidence AGAINST the curriculum hypothesis? Without pre-specified bounds, the hypothesis is unfalsifiable -- any similarity confirms the thesis, and any difference (DAG, 29 vs 11 significant cells, 10.5% vs 4.0% thought-vs-input advantage) is explained away. This was raised as F002 in the round 2 review and remains unresolved.",
      "location": "Section 4.2 (corruption comparisons), Section 4.3 (probing comparisons), Section 5.1 (Table 6), Section 4.4 (in-distribution comparison)",
      "recommendation": "For each null comparison, provide one of the following: (a) A formal equivalence test (TOST) with pre-specified equivalence bounds, reporting whether the difference is small enough to establish practical equivalence. (b) A confidence interval for the difference, allowing readers to judge whether practically meaningful differences are consistent with the data. (c) At minimum, a sensitivity analysis: 'Given our sample size and measurement precision, the smallest detectable difference for this comparison is X.' The current approach of reporting point estimates and calling them 'identical' is scientifically insufficient for a paper whose thesis rests on these null results.",
      "resolution_required": true
    },
    {
      "id": "F003",
      "severity": "major",
      "category": "logic",
      "description": "The OOD results present a paradox that the paper does not adequately confront. If M5 is 'just buffering' (as the paper's thesis implies), why does it outperform M3 on 3 of 4 OOD test sets by statistically significant margins (7.2-9.4pp)? And conversely, if M3 is also 'just buffering' (same curriculum, similar in-distribution performance), why does it hold a significant 7.3pp advantage on DAGs? The paper explains these as a 'task-dependent generalization tradeoff,' but this explanation is deeply problematic for the thesis. Buffers are generic compute resources -- by definition, they should not produce task-specific generalization patterns. If two models both 'buffer,' they should generalize similarly. The fact that they generalize DIFFERENTLY (and significantly so, with 32-40% disagreement rates on OOD) is strong evidence that they are doing something different -- which is precisely what the paper denies on in-distribution data. The paper attempts to attribute these differences to the sequential processing structure (the confound from F001), which is a reasonable hypothesis, but then it cannot simultaneously claim that the in-distribution null result proves the curriculum is the only driver. The in-distribution null result is consistent with both models being at ceiling; the OOD results are the tests that actually have discriminative power, and they show the two architectures behave very differently. The paper cannot have it both ways: using the in-distribution null to support the thesis while dismissing the OOD divergences as confounded tradeoffs.",
      "location": "Section 4.4 (Table 5), Section 5.3 (Architectural Differences), Section 5.1 (Table 6 last row), Section 7 (Conclusion paragraph 2)",
      "recommendation": "The OOD results should be promoted from 'nuanced caveat' to co-equal finding. The paper's central conclusion should be reformulated as: 'On in-distribution ProsQA, both models achieve near-ceiling accuracy and no diagnostic distinguishes them, consistent with curriculum being the dominant driver. On out-of-distribution tests, the architectures diverge significantly, with neither uniformly dominating the other. This divergence demonstrates that the recycling mechanism produces functionally different computation from pause tokens, even though the difference does not manifest at in-distribution ceiling.' This is a stronger, more honest, and more interesting finding than 'curriculum drives everything.'",
      "resolution_required": true
    },
    {
      "id": "F004",
      "severity": "major",
      "category": "logic",
      "description": "The selectivity paradox remains underexplored. Both M3 and M5 show +52pp selectivity at position 3, meaning position 3 encodes the matched reasoning step with 52 percentage points higher accuracy than any other step. The paper interprets this as 'structured, position-specific encoding arising from the shared curriculum rather than from the recycling mechanism.' But structured position-specific encoding of reasoning step identity IS reasoning -- or at least, it is a form of computation far more specific than 'buffering.' A buffer, by definition, provides generic compute capacity without encoding task-specific information at specific positions. Position 3 encoding step 3 with 52pp selectivity over alternative steps is the opposite of generic buffering. The paper acknowledges this tension in one sentence ('We note that the evidence does not support pure buffering... the strong selectivity at position 3 reveals structured, position-specific encoding') but does not follow through on its implications. If both models show structured reasoning-like representations, and these arise from the curriculum, the correct conclusion is not 'curriculum drives buffering' but 'curriculum drives structured computation that resembles step-specific reasoning.' The paper's title question ('Reason or Buffer?') thus has a third answer that the paper identifies but underweights: 'Curriculum-driven structured computation.' This is neither reasoning (because it does not depend on sequential hidden-state recycling) nor buffering (because it is structured and step-specific). The paper's binary framing obscures its own most interesting finding.",
      "location": "Section 4.3 (Table 4, selectivity rows), Section 5.1 (paragraph 1, sentence beginning 'We note that'), Section 5.2 (paragraph 1)",
      "recommendation": "Reframe the core finding. The paper's evidence supports a three-part conclusion: (1) the curriculum produces structured, step-specific computation in thought positions, (2) this structure is sufficient for in-distribution ProsQA performance regardless of whether hidden states are recycled, and (3) the recycling mechanism adds quantitative representational richness (29/78 vs 11/78 significant cells) that manifests as task-specific OOD effects but not in-distribution benefits. This is more precise than 'curriculum, not mechanism' and avoids the false 'reasoning vs buffering' dichotomy.",
      "resolution_required": false
    },
    {
      "id": "F005",
      "severity": "major",
      "category": "design",
      "description": "All results derive from a single training seed (seed 0). The round 1 review (K010) and round 2 review (F004) both flagged this. The manuscript's limitations section acknowledges it. But consider what this means concretely for the paper's claims: the in-distribution comparison rests on 26 discordant samples out of 500 (14 where only M3 is correct, 12 where only M5 is correct). A different training seed could easily shift 5-10 of these samples, potentially widening the gap to 2-3pp (which would reach significance) or reversing it. The OOD results, while showing larger effects (120-235 discordant pairs), are similarly seed-dependent because a different seed would produce different model weights, different decision boundaries, and potentially different OOD generalization profiles. The paper makes architectural claims ('the recycling mechanism is not the causal source of COCONUT's in-distribution performance') from a single instantiation of each architecture. This is analogous to testing whether Drug A and Drug B differ by treating one patient with each. The McNemar test is valid within the instance, but the generalization to the architecture class requires multiple instances. At minimum, the paper should explicitly state that all claims are about these specific model instances, not about the COCONUT architecture or the pause architecture in general.",
      "location": "Section 6 (Single seed limitation), all results sections, abstract, conclusion",
      "recommendation": "Two options: (a) Train 3-5 seeds of each model and report between-seed variability for all primary metrics (accuracy, corruption profiles, selectivity, OOD accuracy). This would establish whether the findings are robust to initialization. (b) If multi-seed training is infeasible within the paper's scope, systematically change all architectural claims to instance-level claims. Replace 'the continuous latent mechanism is not the causal source' with 'in these trained instances, the continuous latent mechanism does not appear to be the causal source.' This scoping change must propagate to the abstract, conclusion, and section 5.5 (practical implications).",
      "resolution_required": false
    },
    {
      "id": "F006",
      "severity": "major",
      "category": "design",
      "description": "The paper acknowledges that it cannot distinguish whether the curriculum alone drives the gains, or whether the curriculum requires additional attention positions as a computational budget (Section 6, 'Curriculum isolation' limitation). This is not merely a minor limitation -- it directly undermines the paper's central claim that 'the curriculum is the active ingredient.' Consider: M1 (CoT) achieves 83%, M3 (COCONUT) achieves 97%, and M5 (Pause) achieves 96.6%. The paper attributes the 13.6-14pp gap between M1 and the curriculum-trained models to the curriculum. But M1 does not have thought positions. M3 and M5 have 6 thought positions that provide additional self-attention targets. A curriculum-only control (M6), which removes CoT tokens at each curriculum stage WITHOUT replacing them with thought/pause positions (i.e., shorter sequences), would test whether the curriculum alone can close the gap. If M6 achieves 83% (like M1), the curriculum is necessary but insufficient -- the additional positions are required. If M6 achieves 96-97%, the curriculum truly is the active ingredient. Without M6, the paper cannot distinguish 'curriculum drives performance' from 'curriculum + extra attention positions drive performance.' The paper's title question should perhaps be 'Curriculum or Attention Budget?' rather than 'Reason or Buffer?'",
      "location": "Section 6 (Curriculum isolation limitation), Section 5.2, Section 5.5 (Practical Implications), Section 7 (Conclusion)",
      "recommendation": "Train an M6 model: same 7-stage curriculum as M3/M5, but at each stage, removed CoT tokens are simply deleted (no replacement tokens). If M6 matches M3/M5, the curriculum alone drives the gains. If M6 collapses, the additional attention positions are necessary, and the 'active ingredient' claim must be reframed as 'curriculum + compute budget.' This ablation is architecturally trivial (no new code needed, just shorter sequences) and would substantially strengthen the paper's causal claims.",
      "resolution_required": false
    },
    {
      "id": "F007",
      "severity": "major",
      "category": "logic",
      "description": "The cross-problem transplantation experiment is non-diagnostic for the reasoning vs. buffering question, as argued in the round 2 review (F008). The experiment transplants thought tokens from problem A into problem B and measures accuracy on problem B. Both M3 (97.0%) and M5 (96.5%) achieve clean-accuracy performance under transplantation, including unmatched transplantation (M3: 97.5%). The paper concludes that 'thought representations carry no problem-specific content.' However, the model retains access to its own input tokens (the graph facts for problem B) through self-attention. The thought tokens are ADDITIONAL context, and if the model can solve the problem from input alone, transplanted foreign thoughts are simply irrelevant noise that the model learns to ignore. The unmatched transplant result for M5 is trivially explained: M5's pause embeddings carry no content, so transplanting them changes nothing meaningful -- it replaces one fixed embedding with the same fixed embedding from a different forward pass. For M3, the transplanted hidden states from problem A are from a different input context, but the model's transformer layers at thought positions have already seen problem B's input tokens via self-attention in the KV-cache. The high transplant accuracy is entirely consistent with: (a) thoughts carry no problem-specific content (buffering hypothesis), (b) thoughts carry problem-specific content but the model has redundant pathways through input tokens (reasoning hypothesis with robustness), or (c) thoughts carry generic computation-facilitating structure that is problem-independent (structured computation hypothesis). The experiment cannot distinguish these alternatives.",
      "location": "Section 4.2 (Cross-problem transplantation, last two paragraphs), Appendix A.3",
      "recommendation": "Either run a stronger transplant experiment that masks/corrupts input tokens so the model must rely solely on thought tokens to answer, or explicitly acknowledge that the transplant experiment is non-diagnostic and remove it from the convergent evidence table (Table 6). A transplant experiment where input tokens are replaced with random embeddings, leaving only thought tokens and the question, would be genuinely diagnostic: if M3 can still answer from foreign thoughts alone, the thoughts carry problem-specific information; if not, they do not.",
      "resolution_required": false
    },
    {
      "id": "F008",
      "severity": "major",
      "category": "analysis",
      "description": "The probing analysis reveals a quantitative asymmetry that the paper dismisses too readily. M3 has 29/78 significant probing cells (37%) versus M5's 11/78 (14%) -- a 2.6x ratio. M3's thought-vs-input advantage is 10.5% versus M5's 4.0% -- also a 2.6x ratio. The paper frames these as showing that 'recycling adds quantitative signal without altering the qualitative representational strategy.' But a 2.6x difference in the number of significant probing cells is not a minor quantitative variation -- it means M3's thought positions carry substantially more decodable task-relevant information across a broader range of layers. The dismissal relies on the claim that this 'does not translate to a behavioral advantage,' but as argued in F003, the in-distribution comparison is ceiling-limited (both at ~97%) and the OOD comparisons show significant behavioral differences. The 29/78 vs 11/78 disparity may be EXACTLY what produces M3's DAG advantage (where it needs broadly distributed representations to handle convergent paths) and M5's chain-length advantages (where the sequential bottleneck in M3 constrains extrapolation). The paper treats the representational difference as irrelevant because it does not affect in-distribution accuracy, but this is a circular argument: the representational difference does not matter where both models are at ceiling, therefore the representational difference does not matter. The OOD results show it does matter.",
      "location": "Section 4.3 (last paragraph), Section 5.2 (paragraph 2)",
      "recommendation": "Remove the claim that the representational difference has no behavioral consequence. Replace with: 'M3's representations are substantially richer and more broadly distributed (29/78 vs 11/78 significant cells, thought-vs-input advantage of 10.5% vs 4.0%). These differences do not affect in-distribution accuracy, where both models are near ceiling, but may contribute to the significant OOD divergences reported in Section 4.4. The functional significance of the representational asymmetry remains an open question that ceiling-limited in-distribution comparisons cannot resolve.'",
      "resolution_required": false
    },
    {
      "id": "F009",
      "severity": "major",
      "category": "framing",
      "description": "The paper's title and abstract frame the inquiry as a binary question: 'Does COCONUT Reason or Buffer?' The evidence, by the paper's own admission, supports neither endpoint of this dichotomy. Both models show structured, step-specific encoding (+52pp selectivity) that is inconsistent with generic buffering. The recycling mechanism produces richer representations (29/78 vs 11/78) that are inconsistent with functional equivalence. The OOD results show architecture-specific generalization patterns that are inconsistent with both models doing 'the same thing.' Yet the paper's conclusion cleaves to the binary framing: 'COCONUT's in-distribution performance on ProsQA is primarily attributable to its training curriculum, not to the content of the recycled hidden states.' This conclusion is technically defensible for in-distribution accuracy alone, but the paper presents evidence spanning in-distribution, OOD, probing, corruption, and transplantation -- and the totality of evidence does not support such a clean verdict. A more accurate summary would be: 'The curriculum is the dominant driver of in-distribution performance, but the recycling mechanism produces measurably different internal representations and architecture-specific OOD generalization, and both models exhibit structured step-specific encoding that transcends simple buffering.' The binary framing served the paper well as a hook, but the evidence demands a ternary answer.",
      "location": "Title, Abstract (first and last sentences), Section 7 (Conclusion, paragraph 1)",
      "recommendation": "Consider revising the title to something like 'Curriculum, Not Mechanism, Drives COCONUT's In-Distribution Performance on ProsQA' -- which is both more accurate and still has a clear thesis. Alternatively, keep the question format but deliver a nuanced answer in the abstract: 'Neither fully: both models exhibit curriculum-driven structured computation that resembles step-specific reasoning, but the recycling mechanism produces distinct representational and generalization characteristics.' The current framing oversells the paper's ability to adjudicate the binary question.",
      "resolution_required": false
    },
    {
      "id": "F010",
      "severity": "minor",
      "category": "analysis",
      "description": "The MLP probe results remain inconclusive (0/78 cells where MLP exceeds linear for both models), and the manuscript now acknowledges this with appropriate caveats. However, the inconclusive MLP result is still listed in Table 4 ('Cells where MLP > linear: 0/78 (inconclusive*)') and discussed in the text as if it provides information. A measurement instrument that returns 0% accuracy everywhere has failed -- it provides exactly zero bits of information about the underlying representation. Listing '0/78' in a table, even with a caveat, creates a misleading impression that nonlinear encoding was tested and found absent. The asterisk and footnote mitigate this, but the entry still occupies a row in the summary table alongside valid measurements. The round 2 review (F009) recommended changing the language; the manuscript added caveats but retained the finding in the table.",
      "location": "Section 4.3 (Table 4, last row), Appendix A.7",
      "recommendation": "Replace the '0/78 (inconclusive*)' entry in Table 4 with 'N/A (probe convergence failure)' and move the discussion entirely to the appendix. Alternatively, retrain MLPs with a proper hyperparameter search. The current presentation -- even with caveats -- implies that something was measured.",
      "resolution_required": false
    },
    {
      "id": "F011",
      "severity": "minor",
      "category": "analysis",
      "description": "The selectivity data in selectivity_recomputed.json contains two grids: 'selectivity_raw_grid' (non-zero values used in the manuscript) and 'selectivity_aligned_grid' (all zeros for both models). The manuscript reports the 'raw' values but never explains what the 'aligned' grid computes or why it is zero. The round 2 review (F005) flagged this data provenance issue. The manuscript's Appendix A.1 describes the correction (using each position's full sample count) but does not explain the two-grid structure in the data file. Furthermore, the 'outcome' field in selectivity_recomputed.json is set to 'A' with interpretation: 'Selectivity remains near zero even with correct pairwise alignment... The n_common=12 bug existed but did not affect the conclusion.' This interpretation ('selectivity remains near zero') contradicts the manuscript's reporting of +52pp selectivity. The data file's own interpretation says selectivity is near zero; the manuscript says it is +52pp. This discrepancy, while likely reflecting different stages of analysis, is confusing and should be resolved for auditability.",
      "location": "Appendix A.1, selectivity_recomputed.json (outcome and interpretation fields)",
      "recommendation": "Update the selectivity_recomputed.json file to reconcile the 'interpretation' field with the actual values in the 'selectivity_raw_grid'. Alternatively, add a note in the manuscript explaining that the 'raw' grid uses one computation method (which produces the +52pp values) while the 'aligned' grid uses another (which produces zeros), and explain why the raw computation is the correct one. The current state of the data file is self-contradictory.",
      "resolution_required": false
    },
    {
      "id": "F012",
      "severity": "minor",
      "category": "analysis",
      "description": "The M3 layer-by-layer probe accuracy trajectory at position 3 reveals a striking U-shaped pattern that is underreported and underinterpreted. From Table A5: layer 0 = 55.4%, layers 1-9 decline monotonically to 25.8% (layer 8), then recover sharply to 55.0% at layer 12. This U-shape means that the recycled hidden state injected at layer 0 carries highly decodable step information, the middle transformer layers partially overwrite it (accuracy drops by nearly 30pp), and the final layers reconstruct it. For M5, the trajectory at position 3 is qualitatively different: layers 0-10 hover between 2.7% and 11.4% (essentially noise for a 38-class problem), then spike to 53.0% and 57.0% at layers 11-12. This means M5 builds its step representation de novo in the final two layers from positional encoding and self-attention, rather than refining injected information. These dramatically different trajectories -- M3's U-shape (inject, degrade, reconstruct) vs. M5's hockey stick (nothing, nothing, sudden construction) -- are not just 'where information is injected' differences. They reveal fundamentally different computational strategies. M3's middle layers actively process the recycled representation (otherwise it would not degrade and recover). M5's computation is concentrated in the final two layers. This qualitative difference in HOW the models compute, visible in the probe trajectories, deserves a dedicated analysis paragraph and figure, not just a table in the appendix.",
      "location": "Section 4.3, Tables A5-A6",
      "recommendation": "Add a figure plotting probe accuracy vs. layer for position 3, comparing M3 and M5. Discuss the U-shaped trajectory for M3 and the hockey-stick trajectory for M5 as evidence that the two models employ qualitatively different computational strategies despite arriving at similar outputs. This analysis could be used to strengthen (or qualify) the 'both models do the same thing' claim.",
      "resolution_required": false
    },
    {
      "id": "F013",
      "severity": "minor",
      "category": "logic",
      "description": "The paper asserts that anti-selectivity at positions 0-1 ('both models decode later reasoning steps better than their own matched steps from these positions') reflects a 'broadcast-then-attend strategy' where the curriculum trains models to propagate answer-relevant information to early positions. However, this interpretation assumes that the probes are detecting information actively placed at positions 0-1. An alternative explanation is probe leakage: in a transformer with causal attention, hidden states at thought position 0 can attend to all input tokens. A linear probe trained on position 0 hidden states may decode step-2 or step-3 entity identity not because position 0 'broadcasts' that information, but because step-2/step-3 entity names are mentioned in the input facts, and the probe learns to read input-derived features from position 0's hidden state. To distinguish genuine broadcasting (model actively routes later-step information to early positions) from probe leakage (probe exploits input-derived features present at all positions), one would need to train probes on thought-position residual streams AFTER subtracting the mean input-position hidden state, or use causal intervention (zero-ablate specific attention heads and measure how anti-selectivity changes).",
      "location": "Section 4.3 (paragraph on anti-selectivity), Section 5.2 (paragraph 3)",
      "recommendation": "Acknowledge that anti-selectivity could reflect probe leakage from input tokens rather than genuine broadcasting. Consider the control experiment described above (subtracting mean input hidden state) as future work, or weaken the 'broadcast-then-attend strategy' interpretation to 'consistent with either broadcasting or probe leakage.'",
      "resolution_required": false
    },
    {
      "id": "F014",
      "severity": "minor",
      "category": "design",
      "description": "The paper raises and then drops the BFS hypothesis. Section 1 states that Hao et al. claim COCONUT 'encodes a breadth-first search strategy.' Section 5.4 acknowledges that 'our probing methodology tests for step-sequential encoding rather than for the breadth-first superposition states that Zhu et al. prove are expressible' and notes that 'a probe designed to decode multiple frontier nodes simultaneously would provide a more targeted test.' The paper then draws conclusions about whether COCONUT 'reasons' without having tested the specific form of reasoning that the original authors proposed. This is a gap between the question posed and the evidence marshaled. If the probes test for sequential-chain encoding and find it absent, this rules out one form of reasoning but not the BFS form that Hao et al. proposed and Zhu et al. theoretically justified. The paper should be explicit that its probing evidence is silent on the BFS hypothesis.",
      "location": "Section 1 (paragraph 2), Section 5.4 (paragraph 2)",
      "recommendation": "Add a clear statement to the probing results section: 'Our probes test for sequential step-by-step encoding. They do not test the BFS superposition hypothesis (Hao et al., 2024; Zhu et al., 2025). The absence of sequential encoding does not rule out breadth-first parallel encoding, which would require different probing methodology to detect.'",
      "resolution_required": false
    },
    {
      "id": "F015",
      "severity": "minor",
      "category": "design",
      "description": "The paper does not analyze per-hop-count accuracy for the in-distribution test set. ProsQA problems range from 3 to 6 hops. The aggregate in-distribution comparison (97.0% vs 96.6%, p=0.845, 26 discordant samples) may mask hop-count-specific differences. If M3 outperforms M5 specifically on 6-hop problems (the hardest in-distribution), this would narrow the gap between 'curriculum drives everything' and 'recycling helps on harder problems.' The per-sample correctness data is available, and the probing analysis already reveals that n=298 for position 3 (3-hop problems where position 3 maps to the final hop), n=81 for position 4, and n=12 for position 5, suggesting the test set has roughly 202 3-hop, 217 4-hop, 69 5-hop, and 12 6-hop problems. A per-hop McNemar analysis with these sample sizes would provide a more granular picture. This was suggested in both the round 1 (K020) and round 2 (F011) reviews.",
      "location": "Section 4.1",
      "recommendation": "Report per-hop-count accuracy for all three models on the in-distribution test set, and per-hop McNemar tests for M3 vs M5 where sample sizes permit. Even descriptive breakdowns (without formal tests on small cells) would be informative.",
      "resolution_required": false
    },
    {
      "id": "F016",
      "severity": "suggestion",
      "category": "analysis",
      "description": "The paper does not analyze the per-sample agreement structure on OOD data beyond reporting contingency table counts. The per-sample correctness data (per_sample_correctness.json) contains 1000-element boolean arrays for each model on each OOD test set. On the 7-hop test, M3 gets 660 correct and M5 gets 754 correct, with 540 both correct, 126 both wrong, 120 M3-only, and 214 M5-only. What characterizes the 214 problems that M5 solves but M3 cannot? Are they concentrated at specific graph depths, branching factors, or distractor densities? Conversely, the 120 problems that M3 alone solves -- do they share properties that suggest sequential processing helps? On DAGs, 235 problems are M3-only correct and 162 are M5-only correct. Understanding WHAT M3 solves that M5 cannot (and vice versa) on DAGs would provide mechanistic insight far richer than aggregate accuracy. The data for this analysis already exists.",
      "location": "Section 4.4, per_sample_correctness.json",
      "recommendation": "Analyze the properties of OOD problems in each contingency cell, especially for DAGs. If M3-only-correct DAG problems tend to have convergent paths (multiple paths leading to the same node), this would support the 'sequential state accumulation helps on convergent structures' hypothesis. If they are random, the DAG advantage may be a statistical fluctuation magnified by the single-seed design.",
      "resolution_required": false
    },
    {
      "id": "F017",
      "severity": "suggestion",
      "category": "analysis",
      "description": "The paper lacks any attention pattern analysis. Probing reveals what information is DECODABLE from representations; attention analysis reveals how information FLOWS between positions. The paper's central question -- whether thought positions serve as sequential reasoning steps or computational scaffolding -- would be most directly answered by examining whether the answer-generating position preferentially attends to specific thought positions (suggesting sequential dependency) or distributes attention broadly (suggesting pooling/buffering). Furthermore, comparing M3 vs M5 attention patterns could reveal whether M3's recycled hidden states alter the information flow structure. This was suggested in the round 1 (K019) and round 2 (F013) reviews.",
      "location": "General absence in experimental design",
      "recommendation": "Add an attention pattern analysis as a supplementary experiment: (1) average attention weights from the answer-generating position to each thought position for both models, (2) identify whether specific attention heads specialize in routing through thought positions, (3) compare whether M3 and M5 exhibit different information routing patterns. This analysis is computationally inexpensive and could substantially strengthen the mechanistic interpretation.",
      "resolution_required": false
    }
  ],
  "prior_findings_status": [
    {
      "original_id": "R1_K001",
      "status": "resolved",
      "assessment": "The data integrity issue (M3 accuracy reported as 98.0% in Table 2 vs 97.0% in experimental data, M5 at 95.6% vs 96.6%) has been resolved. The manuscript now consistently uses 97.0% and 96.6% from the experiment pipeline, with a transparent footnote in Table 2 explaining the training-time evaluation discrepancy (5 samples per model attributable to inference code path differences). The '85% gap closure' framing has been removed."
    },
    {
      "original_id": "R1_K002",
      "status": "partially_resolved",
      "assessment": "The paper now acknowledges that its evidence consists primarily of null results and provides the McNemar CI [-2.4, +1.6] for the primary comparison. However, formal equivalence testing (TOST) has not been added for any comparison, and the corruption, probing, and transplant comparisons remain informally characterized as 'identical' without quantified sensitivity. Reraised as F002."
    },
    {
      "original_id": "R1_K003",
      "status": "partially_resolved",
      "assessment": "The forward-pass asymmetry is now extensively discussed in Sections 3.2 and 6, with a clear description of the missing control. However, the paper continues to draw affirmative conclusions from the confounded comparison without systematically qualifying them. Reraised as F001."
    },
    {
      "original_id": "R1_K004",
      "status": "resolved",
      "assessment": "Conclusions are now better scoped to the specific experimental conditions. The limitations section explicitly acknowledges scale as a constraint, and the conclusion notes 'on ProsQA' rather than making unqualified generalization claims."
    },
    {
      "original_id": "R1_K005",
      "status": "partially_resolved",
      "assessment": "ProsQA's simplicity is acknowledged in the limitations section as 'Task complexity.' However, the conclusion still generalizes beyond ProsQA ('For researchers developing latent reasoning architectures, this work suggests...') without sufficient scoping to the specific task profile."
    },
    {
      "original_id": "R1_K006",
      "status": "partially_resolved",
      "assessment": "The OOD disagreement rates are now reported in Table 5 with proper statistical testing. The paper acknowledges that the models solve problems through 'meaningfully different strategies.' However, the per-sample disagreement patterns are not analyzed (what characterizes the disagreed-upon samples). Reraised as F016."
    },
    {
      "original_id": "R1_K007",
      "status": "partially_resolved",
      "assessment": "The selectivity computation correction is now documented in Appendix A.1 and the corrected values are reported in Table 4. However, the selectivity_recomputed.json data file still contains a self-contradictory 'interpretation' field that says selectivity is 'near zero,' and the two-grid structure (raw vs aligned) remains unexplained. Reraised as F011."
    },
    {
      "original_id": "R1_K008",
      "status": "partially_resolved",
      "assessment": "The set-encoding alternative explanation is partially addressed by the paper's acknowledgment of 'structured, position-specific encoding.' However, it is not directly tested. The paper now notes that both models show structured encoding, but does not design a probe to distinguish set-based encoding from step-specific encoding (e.g., a multi-entity probe)."
    },
    {
      "original_id": "R1_K009",
      "status": "partially_resolved",
      "assessment": "The DAG result is now presented more prominently with proper statistical testing and discussed in a dedicated subsection (5.3). However, the paper still treats it asymmetrically: M5's advantages are part of the main narrative while M3's DAG advantage is framed as a 'tradeoff' and 'confounded.' Reraised as F003."
    },
    {
      "original_id": "R1_K010",
      "status": "unresolved",
      "assessment": "Single seed limitation remains. The manuscript acknowledges it clearly in Section 6 but has not added multi-seed results. Reraised as F005."
    },
    {
      "original_id": "R1_K011",
      "status": "unresolved",
      "assessment": "The missing M3+M5-scale-noise cross-check was not addressed. Table A1 still reports only three conditions (M3+M3-noise, M5+M5-noise, M5+M3-noise). Referenced in F001."
    },
    {
      "original_id": "R1_K012",
      "status": "partially_resolved",
      "assessment": "The layer-0 probe evidence is now discussed more transparently. The paper acknowledges that M3's peak at layer 0 reflects recycled hidden state injection, and Section 5.2 notes the M3 thought-vs-input advantage. However, the 51-point difference at layer 0 between M3 (55.4%) and M5 (4.4%) is still characterized as architectural positioning rather than functional significance."
    },
    {
      "original_id": "R2_F001",
      "status": "partially_resolved",
      "assessment": "The forward-pass confound is now extensively discussed in Sections 3.2 and 6. The paper explicitly describes the missing control. However, causal conclusions in Sections 4-5 are not consistently qualified with this confound. Reraised as F001 with specific recommendations for systematic qualification."
    },
    {
      "original_id": "R2_F002",
      "status": "unresolved",
      "assessment": "Formal equivalence testing has not been added. The McNemar CI is reported, but corruption, probing, and transplant comparisons remain informally characterized. Reraised as F002."
    },
    {
      "original_id": "R2_F003",
      "status": "partially_resolved",
      "assessment": "The DAG result treatment has improved -- it is discussed in a dedicated subsection (5.3) with both the sequential-bottleneck and extra-computation interpretations offered. However, the conclusion still asymmetrically emphasizes the curriculum thesis while relegating the DAG finding to a caveat. Reraised as F003."
    },
    {
      "original_id": "R2_F004",
      "status": "unresolved",
      "assessment": "Single seed. Unchanged. Reraised as F005."
    },
    {
      "original_id": "R2_F005",
      "status": "unresolved",
      "assessment": "Selectivity data provenance (two grids, one all zeros) remains unexplained in the manuscript. Reraised as F011."
    },
    {
      "original_id": "R2_F006",
      "status": "partially_resolved",
      "assessment": "The paper now acknowledges that 'this richer encoding does not translate to a behavioral advantage' is limited by ceiling effects. However, it still makes this claim in the main text without consistent qualification. Reraised as F008."
    },
    {
      "original_id": "R2_F007",
      "status": "partially_resolved",
      "assessment": "Title framing criticism partially addressed by the abstract's more nuanced final sentence mentioning the OOD tradeoff. However, the core binary framing (title, abstract opening, conclusion) persists. Reraised as F009."
    },
    {
      "original_id": "R2_F008",
      "status": "unresolved",
      "assessment": "Transplant experiment design limitation not addressed. The experiment remains non-diagnostic as described. Reraised as F007."
    },
    {
      "original_id": "R2_F009",
      "status": "partially_resolved",
      "assessment": "MLP probe results now carry explicit caveats including the 'inconclusive' asterisk in Table 4 and detailed discussion in Appendix A.7. However, the entry in Table 4 still implies measurement rather than measurement failure. Reraised as F010."
    }
  ],
  "overall_assessment": "pass_with_conditions",
  "summary": "The manuscript has improved markedly across three review rounds. The data integrity issues are resolved, the forward-pass confound is now transparently acknowledged, the limitations section is honest and substantive, and the statistical reporting is rigorous. The core experimental design -- curriculum-matched pause baseline with converging diagnostics -- is creative and valuable. However, three issues prevent an unconditional pass. First, the forward-pass confound (F001) is acknowledged in the limitations but not consistently propagated to the results and discussion sections, where unqualified causal claims persist. Second, the chain of null results lacks formal equivalence testing (F002), meaning the paper cannot distinguish 'identical' from 'undetectably different.' Third, the OOD results (F003) demonstrate that the two architectures produce significantly different behaviors, which the paper underweights by treating M3's DAG advantage as a confounded caveat rather than a co-equal finding. With these three conditions addressed -- systematic qualification of causal claims, confidence intervals or equivalence bounds for null comparisons, and symmetric treatment of OOD results -- the paper would constitute a strong contribution to the latent reasoning literature."
}
