{
  "reviewer": "Statistician",
  "checkpoint": "manuscript_review_round3",
  "round": 3,
  "timestamp": "2026-02-14T18:30:00+00:00",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/manuscript.md",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/statistical_analysis.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/experiments/mcnemar/results.json",
    "papers/efficient_architecture_proof/results/experiments/corruption/results.json",
    "papers/efficient_architecture_proof/results/experiments/probing/results.json",
    "papers/efficient_architecture_proof/results/experiments/ood/results.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/m3_linear_perm.json",
    "papers/efficient_architecture_proof/results/m5_linear_perm.json"
  ],
  "findings": [
    {
      "id": "R3_F001",
      "severity": "minor",
      "category": "analysis",
      "description": "The 8-hop odds ratio CI lower bound is reported as 1.29 in Section 4.4 but the Wald CI computation yields 1.2952, which rounds to 1.30 at two decimal places, not 1.29. Specifically: OR = 198/122 = 1.6230, SE(log OR) = sqrt(1/122 + 1/198) = 0.1151, lower = exp(ln(1.6230) - 1.96*0.1151) = 1.2952. The manuscript reports '[1.29, 2.03]' but the correct rounding is '[1.30, 2.03]'.",
      "location": "manuscript.md:Section 4.4, paragraph beginning 'All four OOD comparisons'",
      "recommendation": "Change '95% CI [1.29, 2.03]' to '95% CI [1.30, 2.03]' for the 8-hop OR. Alternatively, if a different CI method was used (e.g., exact conditional), document which method and verify the bounds.",
      "resolution_required": false
    },
    {
      "id": "R3_F002",
      "severity": "minor",
      "category": "analysis",
      "description": "The manuscript does not specify the method used for computing the odds ratio confidence intervals. The text in Section 4.4 states 'Wald 95% confidence intervals' which is correct, but the Wald method is known to have poor coverage for small cell counts. For the ProsQA in-distribution comparison (b=14, c=12), the Wald method may under-cover. This is not a serious concern because the ProsQA comparison is clearly non-significant regardless of CI method, but for completeness, exact conditional CIs (via the binomial distribution on b/(b+c)) would be more appropriate for the small-sample case.",
      "location": "manuscript.md:Section 4.4, sentence 'Odds ratios are computed from the discordant pairs (OR = c/b) with Wald 95% confidence intervals.'",
      "recommendation": "Consider adding a brief note that Wald CIs may under-cover for the ProsQA comparison due to small discordant cell counts (b=14, c=12), but that this does not affect the conclusion since the comparison is clearly non-significant. Alternatively, report exact Clopper-Pearson CIs for the ProsQA comparison.",
      "resolution_required": false
    },
    {
      "id": "R3_F003",
      "severity": "minor",
      "category": "analysis",
      "description": "The manuscript correctly notes that the non-significant McNemar test on ProsQA does not establish equivalence, and reports the 95% CI for the difference ([-2.4, +1.6] pp). However, the paper does not conduct a formal equivalence test (e.g., TOST). The CI approach is adequate for the current framing -- the manuscript carefully avoids claiming 'equivalence' and instead says 'not significantly different' -- but a TOST with a pre-specified equivalence margin (e.g., 5pp) would strengthen the claim. Given the CI upper bound of 1.6pp, any reasonable equivalence margin >= 2.4pp would be rejected by TOST, providing stronger evidence. The current approach is acceptable but leaves the equivalence claim implicit rather than explicit.",
      "location": "manuscript.md:Abstract and Section 4.1",
      "recommendation": "The current CI-based approach is adequate given the careful language used. As an optional enhancement, consider adding a single sentence such as: 'A two one-sided tests (TOST) equivalence test with a 5 percentage-point margin rejects the null hypothesis of non-equivalence (p < 0.01), confirming that the models are practically equivalent within this margin.' This would formalize the implicit equivalence claim. This is a suggestion, not a requirement.",
      "resolution_required": false
    },
    {
      "id": "R3_F004",
      "severity": "minor",
      "category": "analysis",
      "description": "The permutation sensitivity analysis (5,000 trials, 0 flips) is described as excluding 'a true flip rate above 0.06% at 95% confidence'. The power analysis file (permutation_power.json) gives min_detectable_95 = 0.000599 = 0.0599%, which rounds to 0.06%. The manuscript correctly reports this figure. However, the claim '0.06%' is derived from the exact binomial upper confidence limit on 0 successes in 5,000 trials: 1 - 0.05^(1/5000) = 0.000599. This is the one-sided upper bound. The text uses the phrasing 'excludes a true flip rate above 0.06%' which is correct for a one-sided test, and appropriate here since we are testing the one-sided alternative that permutation does cause flips.",
      "location": "manuscript.md:Section 4.2, Permutation sensitivity paragraph",
      "recommendation": "No change needed. The statistical claim is correctly stated and verified against the data.",
      "resolution_required": false
    },
    {
      "id": "R3_F005",
      "severity": "major",
      "category": "analysis",
      "description": "The Bonferroni correction is applied across k=5 comparisons (ProsQA ID + 4 OOD test sets) with adjusted alpha = 0.01 (i.e., 0.05/5). This is correctly applied in the manuscript. However, there is an inconsistency between two data sources: statistical_analysis.json uses APPROXIMATE McNemar (chi-squared approximation from aggregate accuracy, producing estimated discordant counts) and finds DAG (p_corr=0.121) and Dense (p_corr=0.104) as NOT significant after Bonferroni correction, while mcnemar/results.json and mcnemar_verification.json use EXACT McNemar from per-sample paired data and find ALL four OOD comparisons significant. The manuscript correctly uses the exact test results. However, the existence of a contradictory result file (statistical_analysis.json) that was presumably generated by a different analysis pipeline is a reproducibility concern. The approximate method produced dramatically different p-values (DAG: approximate p=0.024 vs exact p=0.000291, a ~80x discrepancy) because the chi-squared McNemar approximation reconstructed discordant counts from aggregate accuracy rather than using per-sample data, producing much larger estimated discordant counts (b01=105.88, b10=142.38 vs actual b=235, c=162). The manuscript does not mention this discrepancy or explain why the approximate analysis gives qualitatively different results.",
      "location": "manuscript.md:Section 4.4, Table 5; papers/efficient_architecture_proof/results/statistical_analysis.json",
      "recommendation": "Add a brief footnote or methodological note acknowledging that an earlier approximate analysis (chi-squared McNemar from aggregate accuracy) produced qualitatively different significance conclusions for the DAG and Dense comparisons, and that the exact tests from per-sample data are the authoritative results. This strengthens transparency and helps readers who may encounter both result files in the repository. Alternatively, remove or clearly deprecate statistical_analysis.json from the results directory to avoid confusion.",
      "resolution_required": true
    },
    {
      "id": "R3_F006",
      "severity": "minor",
      "category": "analysis",
      "description": "The probing significance tests use 2,000 permutations per cell with the conservative estimator p = (count + 1) / (n_perms + 1). This gives a minimum achievable p-value of 1/2001 = 0.0004998, which the manuscript rounds to 0.0005. The Bonferroni threshold is 0.05/78 = 0.000641. Since the minimum achievable p (0.0005) is below the Bonferroni threshold (0.000641), the test can in principle detect significance -- but just barely. With only a 1.28x margin between the minimum p and the threshold, any cells that are 'just significant' cannot be distinguished from cells at the resolution floor. In practice, all significant cells in the data achieved p = 0.0005 (the floor), meaning zero permuted accuracies exceeded the observed accuracy in 2,000 permutations. This is strong evidence, but the near-coincidence of the floor and threshold means the test cannot rank cells by strength of evidence within the significant set. This is appropriately noted in Appendix A.1 which states 'minimum achievable p = 1/2001 = 0.0005, below the Bonferroni threshold of 0.05/78 = 0.000641'.",
      "location": "manuscript.md:Appendix A.1",
      "recommendation": "No change required. The methodology is sound and the limitation is appropriately documented. As an optional improvement, consider noting that increasing permutations to 5,000 or 10,000 would provide finer p-value resolution and a clearer separation between significant and non-significant cells.",
      "resolution_required": false
    },
    {
      "id": "R3_F007",
      "severity": "minor",
      "category": "analysis",
      "description": "The cross-corruption analysis (Table A1, Section 4.2) reports L2 distances of 202.65 (from the primary corruption experiment in corruption/results.json) and also references L2 approximately 203 in text. The cross-corruption data file (cross_corruption.json) reports slightly different L2 values: m3_m3noise: 202.8, m5_m3noise: 202.39. These small differences (< 0.4 L2 units) are expected from random noise sampling across separate experimental runs and do not affect any conclusions. However, the manuscript text states 'L2 approximately 203' for the M3-scale noise, which is consistent with all values. The reference to 202.65 specifically in the Experiment 1 description comes from the primary experiment, not the cross-corruption re-run.",
      "location": "manuscript.md:Section 4.2, paragraph 'The per-model noise calibration...'",
      "recommendation": "No change needed. The variation in L2 distances across runs is negligible and the approximate figure of 203 used in text correctly summarizes all measurements. The exact figure of 202.65 from the primary experiment is appropriately cited.",
      "resolution_required": false
    },
    {
      "id": "R3_F008",
      "severity": "suggestion",
      "category": "analysis",
      "description": "The transplantation experiment uses only 200 pairs, and the manuscript reports accuracy at the level of the full 200 pairs (e.g., M3 matched: 97.0%, M5 matched: 96.5%, M3 unmatched: 97.5%, M5 unmatched: 96.5%). No confidence intervals or statistical tests are reported for these transplantation accuracies. With n=200, the 95% CI for a proportion near 97% is approximately [93.6%, 98.8%], meaning the difference between matched and unmatched transplantation for M3 (97.0% vs 97.5%) is well within sampling variability. This is correctly interpreted in the text ('matching clean-input performance'), but the absence of CIs leaves the reader to infer this.",
      "location": "manuscript.md:Section 4.2, Cross-problem transplantation paragraph",
      "recommendation": "Consider adding binomial confidence intervals for the transplantation accuracies (e.g., 'M3 achieved 97.0% [93.6, 98.8] and 97.5% [94.2, 99.2] under matched and unmatched conditions, respectively'). This would make explicit that the differences are within sampling variability.",
      "resolution_required": false
    },
    {
      "id": "R3_F009",
      "severity": "suggestion",
      "category": "analysis",
      "description": "The 95% CI for the ProsQA accuracy difference is computed as a simple normal approximation based on discordant counts: diff +/- 1.96 * sqrt((b+c)/n^2). This gives [-2.4, +1.6]pp which is correct. However, for paired binary data with small discordant counts (b+c=26), the Newcombe or Agresti-Caffo interval may provide better coverage. Given the clear non-significance of the result, this does not affect any conclusions.",
      "location": "manuscript.md:Abstract and Section 4.1",
      "recommendation": "No change required. The normal approximation CI is standard and appropriate. The result is clearly non-significant regardless of CI method.",
      "resolution_required": false
    },
    {
      "id": "R3_F010",
      "severity": "major",
      "category": "analysis",
      "description": "The single-seed limitation is discussed extensively in Section 6 (Limitations), which is commendable. However, the abstract and conclusion make claims about the 'task-dependent generalization tradeoff' and state that 'M5 outperforms COCONUT on 3 of 4 test sets (all statistically significant)' without noting in those locations that these results are from a single training run. The McNemar tests correctly quantify within-sample significance (i.e., the models differ in their predictions on these specific test samples), but they do not account for training variability across seeds. A reader of only the abstract might interpret the OOD results as robust architectural differences rather than potentially seed-specific outcomes. The manuscript acknowledges this in Section 6 ('could widen or reverse under different random initializations') but the abstract and conclusion present the OOD results without this qualification.",
      "location": "manuscript.md:Abstract (lines 5-6) and Section 7 (Conclusion, lines 289-290)",
      "recommendation": "Add a brief qualification in the abstract, e.g., 'On out-of-distribution generalization (single training seed), M5 outperforms COCONUT on 3 of 4 test sets...' or 'though all results are from a single training seed (see Limitations).' Similarly, the conclusion should include a parenthetical noting the single-seed caveat when discussing OOD results. The in-distribution non-significance result is less affected by this concern because both models achieve near-ceiling accuracy.",
      "resolution_required": true
    },
    {
      "id": "R3_F011",
      "severity": "minor",
      "category": "analysis",
      "description": "The selectivity computation uses a 'best-layer' approach: for each model and each position, the selectivity is reported at a specific layer (M3: layer 0 for positions 0,1,3 and layer 12 for position 2; M5: layer 12 for all positions). This is clearly documented in Table 4's caption. However, when the manuscript states 'Both models show near-identical selectivity profiles' and 'Position 3 differs by only 0.3pp', this comparison uses different layers for each model (M3 layer 0 vs M5 layer 12). The maximum selectivity at position 3 is actually the same for both models: M3 achieves 52.3pp at layer 12 and M5 achieves 52.3pp at layer 12. If we compare at the same layer (12), the selectivity difference is 0.0pp, not 0.3pp. The 0.3pp difference reported in the manuscript (52.0pp for M3 at layer 0 vs 52.3pp for M5 at layer 12) arises from comparing at different layers where the information enters the network, which is the architecturally motivated comparison. This is a valid analytical choice, but the near-zero difference at the same layer (52.3pp vs 52.3pp at layer 12) would be even more striking evidence for the curriculum-driven claim.",
      "location": "manuscript.md:Section 4.3, Table 4 caption and body text",
      "recommendation": "Consider adding a note that at layer 12 (the final layer), both models achieve identical position-3 selectivity of 52.3pp. This strengthens the 'identical selectivity profiles' claim. The current comparison at architecturally-motivated layers is also valid and appropriately documented.",
      "resolution_required": false
    },
    {
      "id": "R3_F012",
      "severity": "minor",
      "category": "analysis",
      "description": "The manuscript reports the maximum absolute accuracy difference between M3 and M5 corruption curves as '1.0 percentage points (at 3 positions corrupted: 96.8% vs. 95.8%)'. Checking the data: forward corruption at 3 positions gives M3=96.8%, M5=95.8%, difference=1.0pp. However, at 6 positions corrupted, M3=2.4% vs M5=2.2%, which is a 0.2pp difference. The manuscript's claim that 'the maximum absolute accuracy difference at any corruption level is 1.0pp' is verified: looking at all corruption levels, the largest gap is indeed 1.0pp at 3 positions. This is correct. However, the claim applies only to forward corruption. For single-position corruption (Table A4), position 3 shows M3=57.6% vs M5=57.8%, a 0.2pp gap in the opposite direction. For reverse corruption, the maximum gap appears at 2 positions (M3=96.8% vs M5=96.0%, 0.8pp) and 3 positions (M3=96.8% vs M5=96.0%, 0.8pp). The 1.0pp maximum applies specifically to forward corruption and the text is placed in the forward corruption discussion, so it is contextually correct.",
      "location": "manuscript.md:Section 4.2, paragraph beginning 'Both models exhibit nearly identical degradation profiles'",
      "recommendation": "No change required. The claim is contextually correct for the forward corruption analysis being discussed.",
      "resolution_required": false
    },
    {
      "id": "R3_F013",
      "severity": "suggestion",
      "category": "analysis",
      "description": "The manuscript does not report effect sizes for the corruption experiment comparisons. While corruption curves are presented as descriptive comparisons and formal hypothesis tests would not be standard here, the lack of any quantitative measure of similarity between the M3 and M5 corruption profiles leaves the 'identical degradation profiles' claim qualitative. A simple summary statistic, such as the mean absolute difference across all corruption levels (which would be approximately 0.4pp) or a correlation coefficient between the two curves, would quantify this claim.",
      "location": "manuscript.md:Section 4.2",
      "recommendation": "Consider adding a sentence such as: 'The mean absolute difference between M3 and M5 forward corruption accuracies across all 7 conditions is 0.4 percentage points, and the Pearson correlation between the two profiles is r > 0.999.' This would provide a quantitative basis for the 'identical' characterization.",
      "resolution_required": false
    }
  ],
  "prior_findings_status": [
    {
      "original_id": "R2_F001",
      "status": "resolved",
      "assessment": "The prior finding noted that non-significant McNemar does not establish equivalence and recommended TOST or CI-based equivalence testing. The manuscript now reports '95% CI for difference: [-2.4, +1.6] percentage points' in the abstract and Section 4.1, provides full discordant pair counts (b=14, c=12), and carefully uses the language 'not significantly different' rather than 'equivalent'. The CI approach adequately addresses the concern. While a formal TOST is not conducted, the CI makes the equivalence evidence explicit. Resolved."
    },
    {
      "original_id": "R2_F002",
      "status": "resolved",
      "assessment": "The prior finding requested odds ratios with CIs for McNemar comparisons. Section 4.4 now reports OR with 95% Wald CIs for all five comparisons: ProsQA (OR=0.86 [0.40, 1.85]), 7-hop (OR=1.78 [1.43, 2.23]), 8-hop (OR=1.62 [1.29, 2.03]), DAG (OR=0.69 [0.56, 0.84]), Dense (OR=1.52 [1.22, 1.88]). All ORs verified against per-sample data. One minor rounding issue with the 8-hop lower CI bound (computed 1.30, reported 1.29) noted as R3_F001 above. Substantively resolved."
    },
    {
      "original_id": "R2_F004",
      "status": "resolved",
      "assessment": "The prior finding flagged that corruption analysis claimed 'identical' profiles without formal test. The manuscript now qualifies with 'nearly identical degradation profiles' and explicitly notes 'the maximum absolute accuracy difference between M3 and M5 at any corruption level is 1.0 percentage points (at 3 positions corrupted: 96.8% vs. 95.8%), within expected sampling variability for n = 500'. This is an appropriate qualification. Resolved."
    },
    {
      "original_id": "R2_F006",
      "status": "resolved",
      "assessment": "The prior finding raised concern about low power with only 26 discordant pairs for the ProsQA comparison. The manuscript now explicitly states '26 of 500 samples are discordant (14 where M3 alone is correct, 12 where M5 alone is correct)' in Section 4.1 and discusses the single-seed limitation extensively in Section 6. The abstract also includes the CI to make the uncertainty transparent. Resolved."
    },
    {
      "original_id": "R2_F012",
      "status": "resolved",
      "assessment": "The prior finding noted the evaluation pipeline discrepancy (training-time vs experiment pipeline producing slightly different accuracy estimates). Table 2 now includes a detailed footnote: 'Training-time evaluation at best epoch yielded slightly higher estimates for M3 (98.0%) and lower for M5 (95.6%), a discrepancy of 5 samples per model attributable to differences in the inference code path; we use the experiment-pipeline numbers for consistency with all subsequent analyses.' Section 6 (Limitations, Single seed) also references this discrepancy. Resolved."
    }
  ],
  "overall_assessment": "pass_with_conditions",
  "summary": "The manuscript demonstrates substantially improved statistical rigor compared to prior rounds. All five previously flagged findings (equivalence testing, odds ratios, corruption qualification, low power acknowledgment, pipeline discrepancy) have been adequately addressed. Cross-checking every reported number against the underlying data files confirms that accuracy values, contingency tables, p-values, selectivity metrics, and corruption profiles are correctly reported, with one minor rounding discrepancy in the 8-hop OR lower CI bound (1.29 vs computed 1.30). Two conditions require resolution: (1) the conflicting statistical_analysis.json file that shows DAG and Dense as non-significant should be addressed via a transparency note or file deprecation (R3_F005), and (2) the abstract and conclusion should include a single-seed qualification when presenting OOD generalization results (R3_F010). Neither condition requires new analysis -- only textual clarifications. The statistical methodology is sound throughout: exact McNemar tests with Bonferroni correction are appropriate for paired binary comparisons, the corrected permutation tests for probing significance are well-executed, and the selectivity computation is verified against raw data."
}
