{
  "reviewer": "Technical / Code Audit Reviewer",
  "checkpoint": "manuscript",
  "round": 4,
  "timestamp": "2026-02-13T22:00:00+00:00",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/output/manuscript.md",
    "papers/efficient_architecture_proof/code/coconut.py",
    "papers/efficient_architecture_proof/code/dataset.py",
    "papers/efficient_architecture_proof/code/exp_causal.py",
    "papers/efficient_architecture_proof/code/exp_corruption.py",
    "papers/efficient_architecture_proof/code/exp_ood.py",
    "papers/efficient_architecture_proof/code/exp_probing.py",
    "papers/efficient_architecture_proof/code/exp_utils.py",
    "papers/efficient_architecture_proof/code/find_best_epoch.py",
    "papers/efficient_architecture_proof/code/generate_figures.py",
    "papers/efficient_architecture_proof/code/generate_ood_data.py",
    "papers/efficient_architecture_proof/code/plot_selectivity_bars.py",
    "papers/efficient_architecture_proof/code/revision_tasks.py",
    "papers/efficient_architecture_proof/code/run.py",
    "papers/efficient_architecture_proof/code/statistical_analysis.py",
    "papers/efficient_architecture_proof/code/task2_selectivity_fix.py",
    "papers/efficient_architecture_proof/code/task4_5_gpu.py",
    "papers/efficient_architecture_proof/code/utils.py",
    "papers/efficient_architecture_proof/results/m3_test_eval.json",
    "papers/efficient_architecture_proof/results/m5_test_eval.json",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json",
    "papers/efficient_architecture_proof/results/per_sample_logprobs.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/statistical_analysis.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/experiments/corruption/results.json",
    "papers/efficient_architecture_proof/results/experiments/ood/results.json",
    "papers/efficient_architecture_proof/results/experiments/mcnemar/results.json",
    "papers/efficient_architecture_proof/results/experiments/probing/results.json"
  ],
  "findings": [
    {
      "id": "F001",
      "severity": "major",
      "category": "data_integrity",
      "description": "Undocumented accuracy discrepancy between two inference pipelines. The standard test evaluation (m3_test_eval.json) reports M3=98.0% (490/500) and M5=95.6% (478/500), while the per-sample correctness data used for corruption, OOD, and McNemar experiments (per_sample_correctness.json) reports M3=97.0% (485/500) and M5=96.6% (483/500). These differ by 5 samples per model in opposite directions: M3 drops 1pp while M5 rises 1pp. The manuscript uses 98.0%/95.6% in Table 2 (training replication) and 97.0%/96.6% in Tables 3, 5, A1, A2, A3, A4 (corruption and OOD experiments) without acknowledging or explaining the discrepancy. The two inference pipelines likely differ in answer extraction logic (model.generate vs get_processed_embeds + manual generation), but this is never documented. The different pipelines also narrow the M3-M5 gap from 2.4pp to 0.4pp, which materially affects the paper's claims about M5 closing the gap.",
      "location": "manuscript.md: Tables 2, 3, 5; m3_test_eval.json; per_sample_correctness.json; exp_utils.py (extract_answer vs run_inference)",
      "recommendation": "Document the two inference pipelines explicitly. Either reconcile them (use a single pipeline for all accuracy reporting) or add a footnote explaining why Table 2 and Table 5 ProsQA accuracies differ. Investigate root cause: likely differences in how get_processed_embeds + check_answer_from_corrupted handles answer extraction versus the standard run_inference path. The 5-sample divergence per model needs to be traced to specific samples.",
      "resolution_required": true
    },
    {
      "id": "F002",
      "severity": "minor",
      "category": "data_integrity",
      "description": "M5 peak probe accuracy reported inconsistently. Table 4 in the manuscript reports M5 peak probe accuracy as 57.1%, while Table A6 reports the same cell (layer 12, position 3) as 57.0%. The underlying data in probing/results.json shows 0.5705, which is 57.05%. Rounding to one decimal place gives 57.0%, not 57.1%. The value 57.1% in Table 4 is technically a rounding error.",
      "location": "manuscript.md: Table 4 vs Table A6; probing/results.json peak_accuracy=0.5705",
      "recommendation": "Change Table 4 to report 57.0% for M5 peak probe accuracy, consistent with Table A6 and the underlying data (0.5705 rounds to 57.0%, not 57.1%).",
      "resolution_required": false
    },
    {
      "id": "F003",
      "severity": "minor",
      "category": "data_integrity",
      "description": "Table 4 reports M5 peak probe accuracy as 57.1% and the text in Section 4.3 says 'matched-step probe accuracy reaches 55.4% for M3 and 57.0% for M5'. The text says 57.0% but the table says 57.1%. This internal inconsistency in the manuscript body is confusing.",
      "location": "manuscript.md: Table 4 line '57.1%' vs Section 4.3 text '57.0%'",
      "recommendation": "Reconcile the values. The data supports 57.0% (from 0.5705). Update Table 4 to 57.0%.",
      "resolution_required": false
    },
    {
      "id": "F004",
      "severity": "minor",
      "category": "code",
      "description": "The original exp_probing.py compute_selectivity function has a known bug where n_common was computed as min(all positions) = 12 (dominated by position 5's n=12), truncating all position data to 12 samples and producing artifactual 0.0 selectivity. This was correctly identified, documented, and fixed in task2_selectivity_fix.py which uses pairwise alignment. The probing/results.json still contains the buggy selectivity (all zeros), but selectivity_recomputed.json contains the corrected values. The manuscript correctly uses the corrected values and documents the bug in Appendix A.1. This finding is informational only -- the bug was handled properly.",
      "location": "exp_probing.py: compute_selectivity function; task2_selectivity_fix.py; probing/results.json vs selectivity_recomputed.json",
      "recommendation": "No action needed. The bug was correctly identified, fixed, and documented. Consider updating probing/results.json to include corrected selectivity values to avoid confusion for future readers of the data files.",
      "resolution_required": false
    },
    {
      "id": "F005",
      "severity": "minor",
      "category": "code",
      "description": "Figure 3 (plot_selectivity_bars.py) uses hardcoded selectivity values rather than reading from selectivity_recomputed.json. The hardcoded values (m3_selectivity = [-15.6, -10.6, 9.4, 52.0, 0.0], m5_selectivity = [-12.0, -14.6, 10.2, 52.3, 0.0]) are correct and match selectivity_recomputed.json exactly. However, hardcoding data values in figure generation code is a reproducibility concern: if the underlying experiment were re-run with different seeds, the figure code would need manual updates.",
      "location": "code/plot_selectivity_bars.py lines 21-24",
      "recommendation": "Consider modifying plot_selectivity_bars.py to read from selectivity_recomputed.json directly, improving reproducibility. Low priority since the hardcoded values are verified correct.",
      "resolution_required": false
    },
    {
      "id": "F006",
      "severity": "suggestion",
      "category": "code",
      "description": "The statistical_analysis.json file contains approximate McNemar test results computed from aggregate accuracy (not per-sample data), producing chi-squared approximations with a warning note. The manuscript correctly uses exact McNemar's test from mcnemar/results.json (which uses per-sample paired predictions via scipy.stats.binomtest). However, the approximate results in statistical_analysis.json give notably different p-values for some test sets -- e.g., DAG approximate p=0.024 vs exact p=0.00029, a 100-fold difference. The approximate analysis is misleading and could confuse readers of the data files. It also produces non-significant results for DAG and Dense under Bonferroni correction, while the exact test finds all four OOD sets significant.",
      "location": "statistical_analysis.json mcnemar section; mcnemar/results.json",
      "recommendation": "Either remove the approximate McNemar results from statistical_analysis.json or clearly label them as deprecated in favor of the exact per-sample results. The approximate method is invalid here because it reconstructs b and c from marginal accuracy rather than counting them directly from paired predictions.",
      "resolution_required": false
    },
    {
      "id": "F007",
      "severity": "suggestion",
      "category": "reproducibility",
      "description": "The exp_corruption.py permutation test generates 10 random permutations per sample but does not filter for identity permutations. With 6 thought positions, the probability of a random permutation being the identity is 1/6! = 1/720 = 0.14%, so in 5000 trials approximately 7 would be identity permutations by chance. These would trivially produce no flip, slightly biasing the flip rate toward zero. With 0 observed flips in 5000 trials, this does not materially affect the conclusion, but it is a minor methodological imprecision.",
      "location": "code/exp_corruption.py: permutation generation logic",
      "recommendation": "Add a filter to reject identity permutations (while loop or check). Very low priority given the negligible impact on results.",
      "resolution_required": false
    },
    {
      "id": "F008",
      "severity": "suggestion",
      "category": "code",
      "description": "The generate_ood_data.py script generates OOD test sets with specific properties. The DAG generation creates directed acyclic graphs by allowing join nodes ('bridge species') that merge paths. The manuscript describes DAGs as having 'convergent paths where multiple routes reach the same node'. The code implementation is correct and produces the described structure. The dense generation uses branching_factor range (5, 8) vs training range (2, 4). All generation uses seed 42 and ProsQA vocabulary (38 species, 17 person names), matching the manuscript claims.",
      "location": "code/generate_ood_data.py",
      "recommendation": "No action needed. Code matches manuscript description.",
      "resolution_required": false
    },
    {
      "id": "F009",
      "severity": "minor",
      "category": "data_integrity",
      "description": "The mcnemar_verification.json file independently reconstructs all McNemar contingency tables from per-sample prediction files and verifies they match both the saved results and manuscript claims. All 5 test sets pass all checks (contingency matches, p-values match, n matches). This is a strong positive indicator for data integrity. The verification was performed using scipy.stats.binomtest with two-sided alternative, matching the manuscript's description of 'exact McNemar's test (two-sided binomial test on the disagreement counts)'.",
      "location": "results/mcnemar_verification.json",
      "recommendation": "This is a positive finding. The independent verification confirms data integrity for all McNemar results.",
      "resolution_required": false
    },
    {
      "id": "F010",
      "severity": "major",
      "category": "statistical_implementation",
      "description": "The Bonferroni correction is described in the manuscript as using k=5 tests with adjusted alpha=0.01 (i.e., 0.05/5=0.01). However, the manuscript reports DAG p=0.001 in Table 5, which is the Bonferroni-corrected p-value (0.00145697 rounded to 3 decimal places), not the raw p-value. The manuscript is inconsistent in how p-values are presented: for 7-hop and 8-hop it reports '< 0.001' (both raw and Bonferroni-corrected are < 0.001), for DAG it reports 'p = 0.001' in the Bonf. column, and for Dense it reports '< 0.001'. The DAG Bonferroni-corrected p is actually 0.00146, which rounds to 0.001 at 3 decimal places but is properly 0.0015 at 4 decimal places. This is not wrong per se, but the rounding to exactly 0.001 could give readers the false impression that the result barely crosses the significance threshold, when in fact 0.00146 < 0.01 (the Bonferroni-adjusted alpha) with substantial margin.",
      "location": "manuscript.md: Table 5 DAG row; mcnemar/results.json DAG p_bonferroni=0.00145697",
      "recommendation": "Report DAG Bonferroni-corrected p as 0.0015 (4 decimal places) rather than 0.001 (3 decimal places) to avoid ambiguity. Alternatively, use consistent precision across all p-values in the table.",
      "resolution_required": false
    },
    {
      "id": "F011",
      "severity": "suggestion",
      "category": "code",
      "description": "The corruption experiment (exp_corruption.py) calibrates noise per-model using estimate_thought_embedding_stats, which computes the elementwise standard deviation across all thought positions and all samples, then generates isotropic Gaussian noise at that scale. The manuscript reports L2 distances of 202.65 for M3 and 4.09 for M5. The cross_corruption.json confirms these values (M3 noise L2 = 202.8, M5 noise L2 = 4.09). The small discrepancy (202.65 in manuscript vs 202.8 in data) likely reflects different random draws for the L2 distance estimation (which samples 1000 noise vectors). This is negligible.",
      "location": "manuscript.md Section 3.4 'L2 distance of 202.65'; cross_corruption.json noise_l2_mean=202.8",
      "recommendation": "Consider using the data-file value (202.8) in the manuscript for consistency, or note that 202.65 is an expected-value estimate while 202.8 is a sample mean. Very minor.",
      "resolution_required": false
    },
    {
      "id": "F012",
      "severity": "minor",
      "category": "data_integrity",
      "description": "The unmatched transplant results (unmatched_transplant.json) show M3 unmatched accuracy = 97.5%, which is higher than M3 matched transplant accuracy = 97.0% and M3 clean accuracy = 97.0% (from per_sample pipeline). This appears paradoxical: transplanting foreign thoughts should not improve accuracy above clean performance. The manuscript reports these numbers correctly (Table A2) but does not comment on this anomaly. The most likely explanation is that the 200 random unmatched pairs happened by chance to include more 'easy' recipient problems, or that the small sample size (200 pairs) produces noisy estimates. The difference is only 1 sample (195 vs 194 correct out of 200), well within sampling noise.",
      "location": "results/unmatched_transplant.json: m3 unmatched_accuracy=0.975 vs reference matched_m3_accuracy=0.97",
      "recommendation": "Add a brief note in the manuscript acknowledging that unmatched transplant accuracy (97.5%) slightly exceeds clean accuracy (97.0%) due to sampling noise over the 200-pair evaluation. This prevents readers from drawing incorrect conclusions about foreign thoughts improving performance.",
      "resolution_required": false
    },
    {
      "id": "F013",
      "severity": "suggestion",
      "category": "reproducibility",
      "description": "The training script (run.py) uses FSDP (Fully Sharded Data Parallelism) from Meta's original codebase. The manuscript states training was on 'a single NVIDIA H100 80GB GPU' with 'batch size 32, gradient accumulation over 4 steps on a single GPU, matching Meta's original 4-GPU configuration'. This is well-documented. However, the run.py code references configs that are not included in the repository (e.g., prosqa_m5_pause.yaml). The YAML configuration files that specify feedback_mode and curriculum parameters are not present in the code/ directory.",
      "location": "code/run.py; manuscript Section 3.3",
      "recommendation": "Include the YAML configuration files for M1, M3, and M5 in the code repository for full reproducibility. The manuscript mentions 'prosqa_m5_pause.yaml' but this file is not present in the codebase.",
      "resolution_required": false
    },
    {
      "id": "F014",
      "severity": "minor",
      "category": "code",
      "description": "The coconut.py implementation of M5 (pause_curriculum mode) uses a single nn.Parameter of shape (768,) that is cloned to fill all thought positions via _fill_pause_embeddings. The manuscript correctly describes this: 'a single learned embedding vector of 768 dimensions (nn.Parameter), repeated identically at all six thought positions'. The initialization clones from the <|latent|> token embedding, as documented in appendix_data.json (task3_pause_architecture). This is architecturally sound and matches all manuscript claims.",
      "location": "code/coconut.py: _fill_pause_embeddings method; manuscript Section 3.2",
      "recommendation": "No action needed. Implementation matches description.",
      "resolution_required": false
    },
    {
      "id": "F015",
      "severity": "suggestion",
      "category": "code",
      "description": "The probing experiment (exp_probing.py) uses RidgeClassifier from sklearn with default regularization (alpha=1.0). The manuscript describes this as 'RidgeClassifier with default regularization'. This is correct. However, the nonlinear probe uses the default MLPClassifier hyperparameters, and the manuscript notes in Appendix A.7 that 'the MLP training procedure (default scikit-learn MLPClassifier hyperparameters) may warrant further tuning to rule out convergence failure'. The probing results show all nonlinear probe accuracies as exactly 0.0 across all 78 cells for both models, which is unusual -- even random features should produce some above-chance accuracy. The 0.0 values may indicate MLP convergence failure rather than a genuine absence of nonlinear structure.",
      "location": "code/exp_probing.py: MLP probe implementation; probing/results.json nonlinear_probe_accuracy",
      "recommendation": "Investigate whether the MLP probes are failing to converge. All 156 cells (78 per model) showing exactly 0.0 is suspicious. Try adjusting learning rate, max_iter, or hidden layer size. If the 0.0 values are confirmed as convergence failures, either fix the MLP training or remove the nonlinear probe claim from the paper (currently 'Cells where MLP > linear: 0/78' in Table 4). The manuscript already flags this in A.7 which is good practice, but the table entry could be misleading.",
      "resolution_required": false
    },
    {
      "id": "F016",
      "severity": "minor",
      "category": "data_integrity",
      "description": "The manuscript reports M3 clean accuracy in Table 3 as 97.0% and in Table A2 as 97.0%, both sourced from the corruption/OOD pipeline. M5 clean is 96.6% in both tables. The manuscript reports the M5-M3 difference as '-0.4 pp' in Table 5, which is correct (96.6 - 97.0 = -0.4). However, Table 2 reports M3=98.0% and M5=95.6%, giving a gap of 2.4pp. The paper uses the larger gap (2.4pp) for narrative purposes ('closing 85% of the gap') and the smaller gap (0.4pp) for statistical testing. While both are valid measurements from different pipelines, the selective use of whichever gap serves the narrative better is a concern.",
      "location": "manuscript.md: Table 2 vs Table 5 ProsQA rows",
      "recommendation": "Address the dual-pipeline issue (see F001). If using Table 2 values for the '85% of the gap' claim, explicitly note that the corruption pipeline produces different accuracies and a much smaller gap (0.4pp). Alternatively, use one pipeline consistently for all in-distribution accuracy claims.",
      "resolution_required": true
    },
    {
      "id": "F017",
      "severity": "suggestion",
      "category": "reproducibility",
      "description": "The code repository does not include a requirements.txt or environment specification listing exact package versions for PyTorch, scikit-learn, scipy, numpy, and matplotlib. The manuscript does not specify package versions used. For full reproducibility, exact versions should be documented.",
      "location": "code/ directory (missing requirements.txt or environment.yml)",
      "recommendation": "Add a requirements.txt or conda environment.yml specifying exact package versions used for all experiments.",
      "resolution_required": false
    },
    {
      "id": "F018",
      "severity": "minor",
      "category": "code",
      "description": "The revision_tasks.py script is a combined version of task2_selectivity_fix.py and task4_5_gpu.py functionality. Both standalone scripts and the combined script exist in the codebase. This duplication introduces a risk of code divergence. The cross_corruption.json and unmatched_transplant.json results could have been generated by either script.",
      "location": "code/revision_tasks.py vs code/task2_selectivity_fix.py + code/task4_5_gpu.py",
      "recommendation": "Consolidate into a single script or document which script generated which result file. Low priority since both versions appear to implement the same logic.",
      "resolution_required": false
    },
    {
      "id": "F019",
      "severity": "suggestion",
      "category": "data_integrity",
      "description": "The per_sample_logprobs.json contains per-sample log-probabilities for all model-testset combinations (10 arrays of 500-1000 floats each). These are not referenced anywhere in the manuscript. They may have been collected for potential logprob-based analyses that were ultimately not used. Their presence is fine for archival purposes but they consume 184KB of storage without documented purpose.",
      "location": "results/per_sample_logprobs.json",
      "recommendation": "Either document the purpose of per_sample_logprobs.json or note that it is archival data collected but not used in the analysis. No need to remove.",
      "resolution_required": false
    },
    {
      "id": "F020",
      "severity": "suggestion",
      "category": "code",
      "description": "The utils.py file from Meta's original codebase contains set_seed which sets both torch.manual_seed and np.random.seed, as well as PYTHONHASHSEED, cudnn.deterministic=True, and cudnn.benchmark=False. The exp_utils.py has its own set_seed function. Both set the same seeds in the same way, but the duplication means changes to one do not propagate to the other.",
      "location": "code/utils.py vs code/exp_utils.py set_seed functions",
      "recommendation": "Have exp_utils.py import set_seed from utils.py rather than reimplementing it. Low priority since both implementations are identical.",
      "resolution_required": false
    },
    {
      "id": "F021",
      "severity": "minor",
      "category": "data_integrity",
      "description": "The corruption results (corruption/results.json) contain reverse corruption values for M3: [0.968, 0.968, 0.968, 0.574, 0.156, 0.024]. The manuscript's Table A3 reports reverse corruption for M3 as: 97.0% (1 pos), 96.8% (2 pos), 96.8% (3 pos), 57.4% (4 pos), 15.6% (5 pos), 2.4% (6 pos). The first value in the JSON array (0.968) maps to '1 position corrupted from end' = 96.8%. But Table A3 shows the first entry as 97.0%. This 97.0% actually appears to be the clean accuracy, not 1-position reverse corruption. Looking more carefully: Table A3 row '1 (pos 5)' = 97.0% for M3 matches the clean accuracy, suggesting that corrupting only position 5 has no effect. The JSON data for reverse corruption starts with 1 position corrupted. The value 0.968 for M3 reverse[0] means corrupting position 5 gives 96.8%, but the manuscript says 97.0%. Let me verify: the JSON reverse values are [0.968, 0.968, 0.968, 0.574, 0.156, 0.024]. If these map to corrupting 1, 2, 3, 4, 5, 6 positions from the end, then 1 pos from end = 96.8%, not 97.0%. The manuscript reports 97.0%. This is a 0.2pp discrepancy for M3 reverse 1-position.",
      "location": "manuscript.md Table A3 row '1 (pos 5)' M3=97.0% vs corruption/results.json m3.reverse[0]=0.968",
      "recommendation": "Verify the exact value for M3 with 1 position corrupted from the end (position 5 only). The JSON says 96.8% but Table A3 says 97.0%. If 97.0% is the clean accuracy incorrectly placed in this row, update either the table or the JSON.",
      "resolution_required": false
    },
    {
      "id": "F022",
      "severity": "minor",
      "category": "statistical_implementation",
      "description": "The manuscript states McNemar's test is computed as 'exact McNemar's test (two-sided binomial test on the disagreement counts)' and implements it as binomtest(min(b,c), b+c, 0.5, two-sided). This is the standard exact form of McNemar's test. The implementation in mcnemar_verification.json confirms scipy.stats.binomtest is used. The Bonferroni correction multiplies raw p-values by k=5, capped at 1.0. This is correct. The adjusted alpha threshold of 0.01 (0.05/5) is correctly applied. All implementation details match the manuscript description.",
      "location": "mcnemar_verification.json; manuscript Section 3.4",
      "recommendation": "No action needed. Statistical implementation is correct.",
      "resolution_required": false
    },
    {
      "id": "F023",
      "severity": "minor",
      "category": "code",
      "description": "The probing experiment reports 0 significant cells out of 78 for both models (all permutation p-values = 1.0). The permutation test uses 10,000 shuffles. With all p-values being exactly 1.0 (not approximately 1.0 but exactly 1.0), this means that no shuffled probe ever exceeded the observed accuracy. For cells with observed accuracy near chance (e.g., 3-5%), this could indicate that the chance-level accuracy is at or above the true probe accuracy, which is expected. But for cells with higher accuracy (e.g., M3 layer 0 position 3 = 55.4%), having all 10,000 permutation shuffles produce accuracy below 55.4% would normally yield a very small p-value (e.g., < 0.0001), not p = 1.0. The fact that ALL 78 cells show p = 1.0 suggests a possible bug in the permutation test implementation where the comparison direction is inverted, or the labels are shuffled incorrectly.",
      "location": "probing/results.json: permutation_p_values all 1.0; code/exp_probing.py permutation test",
      "recommendation": "Investigate the permutation test implementation. With peak probe accuracy of 55.4% for M3 at (0,3), the probability of all 10,000 permuted classifiers exceeding 55.4% is essentially zero. The p-value should be < 0.0001 at this cell, not 1.0. This suggests the permutation p-values are computed incorrectly (e.g., counting fraction of permuted results GREATER than observed, rather than greater than or equal to). If confirmed as a bug, the 'n_significant_cells: 0' claim in the manuscript (Table 4) would need revision -- some cells almost certainly have significant probing accuracy.",
      "resolution_required": true
    },
    {
      "id": "F024",
      "severity": "suggestion",
      "category": "reproducibility",
      "description": "The manuscript references 'Code, configurations, and experiment scripts are available at https://github.com/bmarti44/research-pipeline' but the actual code is in the local repository at papers/efficient_architecture_proof/code/. The GitHub URL should be verified to contain all necessary code files.",
      "location": "manuscript.md: final paragraph of Section 7",
      "recommendation": "Verify the GitHub repository URL is correct and contains all code and configuration files needed for reproduction.",
      "resolution_required": false
    },
    {
      "id": "F025",
      "severity": "suggestion",
      "category": "data_integrity",
      "description": "The probing results for positions 4 and 5 show 0.0% accuracy across all layers for both models. The manuscript attributes this to 'insufficient samples (n=81 and n=12)'. The n_samples_per_position in probing/results.json confirms n=81 for position 4 and n=12 for position 5. However, n=81 is not necessarily insufficient for a RidgeClassifier probe -- scikit-learn's RidgeClassifier with 5-fold CV should work with 81 samples (16 per fold minimum). The 0.0% accuracy at position 4 for all layers is suspicious and may indicate a labeling issue: if the labels for positions 4 and 5 have very high cardinality relative to sample count, the probe may systematically fail. The manuscript appropriately flags position 5 (n=12) as unreliable but does not flag position 4 (n=81).",
      "location": "probing/results.json: positions 4-5 all zeros; manuscript Table A5, A6",
      "recommendation": "Investigate why position 4 (n=81) produces exactly 0.0% probe accuracy across all 13 layers for both models. Check label cardinality at position 4 -- if there are more classes than samples per fold, the RidgeClassifier may trivially fail. Consider reporting chance-level baselines for each position.",
      "resolution_required": false
    }
  ],
  "overall_assessment": "pass_with_conditions",
  "summary": "The codebase and data files are generally well-organized, with strong internal verification (mcnemar_verification.json independently confirms all McNemar results). The vast majority of manuscript claims trace correctly to underlying data: all OOD accuracy values (Table 5), corruption curves (Table 3), selectivity values (Table 4, Figure 3), cross-corruption (Table A1), and unmatched transplant (Table A2) match their source JSON files exactly. The selectivity bug was properly identified, fixed, and documented. Three issues require attention: (1) an undocumented accuracy discrepancy between two inference pipelines that materially affects the M3-M5 gap narrative, (2) the selective use of whichever pipeline gap serves the argument better, and (3) a likely bug in the permutation test producing all p-values = 1.0 when some cells should show highly significant probing accuracy.",

  "cross_reference_summary": {
    "verified_correct": [
      "Table 3 corruption values match corruption/results.json exactly",
      "Table 5 OOD accuracy values match ood/results.json exactly",
      "Table 5 McNemar b, c values match mcnemar/results.json exactly",
      "Table 5 McNemar p-values match mcnemar_verification.json exactly",
      "Table A1 cross-corruption values match cross_corruption.json exactly",
      "Table A2 transplant values match unmatched_transplant.json exactly",
      "Figure 3 selectivity bars match selectivity_recomputed.json exactly",
      "Table 4 M3 peak probe accuracy 55.4% matches probing/results.json (0.5537)",
      "Table 4 Position 3 selectivity +52.0pp (M3) and +52.3pp (M5) match selectivity_recomputed.json",
      "Table 4 Position 2 selectivity +9.4pp (M3) and +10.2pp (M5) match selectivity_recomputed.json",
      "Table 4 Positions 0-1 anti-selectivity values match selectivity_recomputed.json",
      "Table 4 Nonlinear probe advantage 0/78 matches probing/results.json (all zeros)",
      "Table A5 M3 linear probe grid values match probing/results.json linear_probe_accuracy",
      "Table A6 M5 linear probe grid values match probing/results.json linear_probe_accuracy",
      "Appendix A.4 permutation power analysis matches permutation_power.json",
      "Per-sample correctness counts (per_sample_correctness.json) reproduce all McNemar contingency tables",
      "McNemar verification (mcnemar_verification.json) independently confirms all manuscript statistics",
      "OOD dataset generation parameters (seed 42, 1000 samples, vocabulary) documented in code/generate_ood_data.py",
      "M3 replication accuracy 98.0% matches Hao et al. ~97% within reported tolerance"
    ],
    "discrepancies_found": [
      "Table 2 M3=98.0% vs Table 5 ProsQA M3=97.0% (different inference pipelines, 5 samples differ)",
      "Table 2 M5=95.6% vs Table 5 ProsQA M5=96.6% (different inference pipelines, 5 samples differ, opposite direction)",
      "Table 4 M5 peak probe accuracy 57.1% vs probing data 57.05% vs Table A6 57.0% (rounding inconsistency)",
      "Manuscript L2=202.65 vs cross_corruption.json L2=202.8 (different random draws, negligible)",
      "Table A3 M3 reverse 1-pos = 97.0% vs corruption/results.json reverse[0] = 96.8% (0.2pp discrepancy)",
      "Permutation p-values all 1.0 despite peak accuracy 55.4% (likely implementation bug)"
    ]
  },

  "code_correctness_assessment": {
    "model_implementation": "PASS. coconut.py correctly implements both COCONUT (continuous mode with multi-pass KV cache loop) and M5 (pause_curriculum mode with single forward pass and nn.Parameter embedding). The feedback_mode branching logic is clean. The _fill_pause_embeddings method correctly clones the pause embedding to all thought positions.",
    "corruption_implementation": "PASS. exp_corruption.py correctly implements forward, reverse, single-position corruption, permutation (10 random non-identity-filtered permutations), partial permutation, and cross-problem transplant (200 hop-count-matched pairs). Noise calibration via estimate_thought_embedding_stats is methodologically sound. check_answer_from_corrupted correctly substitutes noise vectors at specified positions.",
    "probing_implementation": "PASS WITH CAVEAT. Linear probe implementation using RidgeClassifier with 5-fold CV is correct. Hidden state extraction at all 13 layers x 6 positions is correct. Selectivity computation was buggy (n_common truncation) but was fixed in task2_selectivity_fix.py. CAVEAT: Permutation test p-values (all 1.0) are likely incorrect. Nonlinear MLP probes (all 0.0) may indicate convergence failure rather than genuine absence of nonlinear structure.",
    "statistical_implementation": "PASS. Exact McNemar's test correctly implemented via scipy.stats.binomtest. Bonferroni correction correctly applied with k=5. All contingency tables verified from per-sample data.",
    "figure_generation": "PASS. plot_selectivity_bars.py hardcodes verified-correct values. generate_figures.py produces all 7 figures from data files.",
    "ood_generation": "PASS. generate_ood_data.py correctly generates OOD test sets with proper vocabulary, seed, and structural parameters.",
    "cross_corruption": "PASS. task4_5_gpu.py correctly implements the M3-magnitude noise on M5 experiment. The three conditions (M3+M3noise, M5+M5noise, M5+M3noise) are correctly set up with noise std from each model's actual thought statistics.",
    "unmatched_transplant": "PASS. task4_5_gpu.py correctly implements random donor-recipient pairing with no hop-count matching. 200 pairs with seed 42."
  },

  "data_integrity_assessment": {
    "provenance": "GOOD. All result JSON files contain sufficient metadata to trace their origin. m3_test_eval.json and m5_test_eval.json specify checkpoint names. mcnemar_verification.json contains full verification trail. appendix_data.json provides a consolidated summary of all revision tasks.",
    "consistency": "MOSTLY CONSISTENT. The vast majority of cross-references check out. The main consistency issue is the dual-pipeline accuracy discrepancy (F001/F016). All other cross-references between manuscript tables, figure code, and result JSON files are exact matches.",
    "completeness": "COMPLETE. All manuscript claims can be traced to specific data files. The per_sample_correctness.json file enables independent verification of all McNemar statistics. The probing/results.json contains full 13x6 grids for both models. The selectivity_recomputed.json contains corrected selectivity values.",
    "verification_artifacts": "STRONG. The mcnemar_verification.json is an exemplary verification artifact: it independently reconstructs all statistics from raw data and confirms matches with both saved results and manuscript claims. The permutation_power.json provides rigorous statistical backing for the 0-flip permutation result."
  },

  "reproducibility_assessment": {
    "seeds_documented": "YES. Training seed 0, OOD generation seed 42, transplant pairing seed 42 are all documented.",
    "hardware_documented": "YES. Single NVIDIA H100 80GB GPU, fp32 precision.",
    "missing_for_reproduction": [
      "YAML configuration files (prosqa_m5_pause.yaml, etc.) not included in code directory",
      "Package version requirements (PyTorch, scikit-learn, scipy versions) not specified",
      "Training checkpoints not available (expected, as they are large binary files)",
      "Pretrained GPT-2 model source clearly specified (openai-community/gpt2)"
    ]
  }
}
