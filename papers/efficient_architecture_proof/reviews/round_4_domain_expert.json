{
  "reviewer": "Domain Expert (LLM/COCONUT Specialist)",
  "checkpoint": "v4.0_scaleup",
  "round": 4,
  "timestamp": "2026-02-05T15:30:00Z",
  "files_reviewed": [
    "results/v4.0_scaleup/baseline_2000/seed_*/baseline_results.json",
    "results/v4.0_scaleup/coconut_warmstart/seed_*/coconut_only_results.json",
    "results/v4.0_scaleup/full_abc_warmstart/seed_*/full_abc_results.json",
    "results/abc_study_v3/summary.json",
    "code/models/coconut_full.py",
    "COCONUT_IMPLEMENTATION_COMPLETE.md",
    "LATENT_REASONING_RESEARCH.md",
    "RESEARCH_SYNTHESIS.md"
  ],
  "domain_context": {
    "original_coconut_paper": {
      "model": "GPT-2 (124M parameters)",
      "training": "Warm-start from pre-trained LM checkpoint",
      "key_results": {
        "ProntoQA": "99.8% (vs 98.8% CoT)",
        "ProsQA": "97.0% (vs 77.5% CoT)",
        "GSM8K": "34.1% (vs 42.9% CoT, UNDERPERFORMS)"
      },
      "mechanism": "Hidden state from position (p-1) replaces embedding at position (p)",
      "training_approach": "Multi-stage curriculum: progressively replace CoT steps with continuous thoughts"
    },
    "this_study": {
      "model": "Custom transformer (38M parameters)",
      "training": "Warm-start from task-trained baseline (1000 steps LM)",
      "task": "Multi-step arithmetic (5-9 steps)",
      "architecture_additions": "MoD (Mixture-of-Depths) + Hierarchical Memory"
    }
  },
  "findings": [
    {
      "id": "DE001",
      "severity": "major",
      "category": "domain_knowledge",
      "title": "Task Choice Conflicts with COCONUT Literature",
      "description": "The original COCONUT paper explicitly shows that latent reasoning UNDERPERFORMS on arithmetic tasks (GSM8K: 34.1% vs 42.9% CoT). This study tests on 'multi-step arithmetic (5-9 steps)', which is precisely the domain where COCONUT is known to struggle. The paper states: 'COCONUT excels at logical/search tasks (BFS capability) but struggles with arithmetic. This may be because math requires precise symbolic manipulation that benefits from discrete token space.'",
      "location": "RESEARCH_SYNTHESIS.md:22-23, results/abc_study_v3/summary.json",
      "recommendation": "Either (a) acknowledge this is testing COCONUT in an adverse domain and interpret results accordingly, or (b) switch to a task where COCONUT has demonstrated strength (e.g., ProsQA-style logical reasoning, symbolic planning)",
      "resolution_required": true,
      "evidence": [
        "GSM8K (original paper): CoT 42.9%, COCONUT 34.1% (-8.8pp)",
        "This study tests arithmetic, not logical reasoning"
      ]
    },
    {
      "id": "DE002",
      "severity": "major",
      "category": "domain_knowledge",
      "title": "38M Scale is Smaller than Original COCONUT Validation",
      "description": "The original COCONUT paper used GPT-2 at 124M parameters (later scaled to 7B in ablations). This study uses 38M parameters, which is 3.3x smaller than the smallest validated COCONUT scale. While COCONUT's core mechanism may work at smaller scales, there is no published evidence that 38M is sufficient for effective latent reasoning. The architecture may lack the representation capacity needed for continuous thought to emerge.",
      "location": "results/v4.0_scaleup/*/config.json",
      "recommendation": "Explicitly acknowledge that this study operates below the validated COCONUT scale. Consider adding a scale-up experiment to at least 125M parameters for comparison.",
      "resolution_required": true,
      "evidence": [
        "Original COCONUT: GPT-2 124M parameters",
        "This study: 38M parameters (3.3x smaller)",
        "Scale-dependent effects are common in transformer architectures"
      ]
    },
    {
      "id": "DE003",
      "severity": "critical",
      "category": "domain_knowledge",
      "title": "Warm-Start Approach Differs Fundamentally from COCONUT",
      "description": "The original COCONUT paper warm-starts from a PRE-TRAINED language model (GPT-2 trained on web text). This study warm-starts from a task-trained baseline (1000 steps on the same arithmetic task). This is a fundamental difference: COCONUT's latent reasoning leverages representations learned during pre-training on diverse text, not representations specific to the target task. The warm-start from task-trained baseline provides no linguistic/reasoning scaffold from pre-training.",
      "location": "results/v4.0_scaleup/coconut_warmstart/seed_42/coconut_only_results.json:19-22",
      "recommendation": "Either (a) use a pre-trained model checkpoint as the warm-start (e.g., GPT-2 small, TinyLlama), or (b) clearly state this methodological deviation and discuss how it might affect latent reasoning emergence.",
      "resolution_required": true,
      "evidence": [
        "Original COCONUT: warm-start from pre-trained GPT-2",
        "This study: warmstart from baseline trained 1000 steps on arithmetic",
        "warmstart field: 'warmstart_from_condition': 'baseline'"
      ]
    },
    {
      "id": "DE004",
      "severity": "major",
      "category": "analysis",
      "title": "Improvement Magnitude is Inconsistent with COCONUT Literature",
      "description": "COCONUT shows massive improvements on tasks where it succeeds (+19.5pp on ProsQA) and significant degradation on arithmetic (-8.8pp on GSM8K). The reported improvements here (-5.2% COCONUT alone, -7.2% full ABC) are anomalous: they suggest improvement on arithmetic, contradicting the literature. This could indicate: (a) a different mechanism is driving improvement, (b) the arithmetic task is simple enough to not require discrete reasoning, or (c) measurement/implementation issues.",
      "location": "results/v4.0_scaleup/",
      "recommendation": "Investigate whether the improvement is truly from latent reasoning or from other factors (e.g., regularization effect of additional forward passes, MoD routing, memory augmentation).",
      "resolution_required": false,
      "evidence": [
        "COCONUT on GSM8K: -8.8pp (HURTS)",
        "This study COCONUT on arithmetic: -5.2% val_ppl (HELPS)",
        "This is qualitatively opposite to expectations"
      ]
    },
    {
      "id": "DE005",
      "severity": "minor",
      "category": "domain_knowledge",
      "title": "Synergy Claim Lacks Mechanistic Grounding",
      "description": "The claim that MoD + Memory add -2.1% on top of COCONUT (p=0.024) is statistically significant but lacks a mechanistic explanation. Why would sparse computation (MoD) and external memory enhance latent reasoning? The literature provides no theoretical basis for this synergy. Alternative explanation: the additional -2.1% could be from (a) regularization, (b) ensemble-like effects, or (c) statistical noise given the small sample size (n=3 seeds).",
      "location": "manuscript sections",
      "recommendation": "Provide a mechanistic hypothesis for why MoD and Memory might synergize with COCONUT, or acknowledge this as an unexpected empirical finding requiring further investigation.",
      "resolution_required": false,
      "evidence": [
        "COCONUT alone: -5.2% (val_ppl improvement)",
        "Full ABC: -7.2% (val_ppl improvement)",
        "Synergy: -2.1% additional (p=0.024)",
        "No theoretical basis in literature"
      ]
    },
    {
      "id": "DE006",
      "severity": "major",
      "category": "analysis",
      "title": "Scale-Dependent Activation is Not Novel",
      "description": "The observation that COCONUT hurts at 7.5M but helps at 38M is consistent with general scaling literature, not specific to COCONUT. Many architectural innovations only emerge at sufficient scale. The paper 'Scaling Laws Under the Microscope' (EMNLP 2022) and 'Scaling Laws Are Unreliable for Downstream Tasks' (2025) both discuss this phenomenon. This is not strong evidence of COCONUT-specific scale-dependence.",
      "location": "RESEARCH_SYNTHESIS.md, results/abc_study_v3/summary.json",
      "recommendation": "Either (a) test at multiple intermediate scales (10M, 20M, 30M, 50M) to establish a precise activation threshold, or (b) frame this as consistent with general scaling literature rather than COCONUT-specific.",
      "resolution_required": false,
      "evidence": [
        "7.5M scale: COCONUT hurts (+3.0% worse)",
        "38M scale: COCONUT helps (-5.2% better)",
        "This pattern is common across many architectures"
      ]
    },
    {
      "id": "DE007",
      "severity": "minor",
      "category": "implementation",
      "title": "COCONUT Implementation Uses Fixed Iterations Instead of Curriculum",
      "description": "The implementation uses a fixed number of iterations (n_iterations = max_n_latent = 8) instead of the curriculum-based approach in the original COCONUT paper. The original paper progressively increases the number of continuous thoughts during training (Stage k replaces k CoT steps with k latent tokens). This study's fixed-iteration approach may limit the model's ability to learn when and how many iterations are needed.",
      "location": "code/models/coconut_full.py:346, COCONUT_IMPLEMENTATION_COMPLETE.md",
      "recommendation": "Implement curriculum training with progressively increasing latent tokens as in the original paper, or clearly document this deviation and its potential impact.",
      "resolution_required": false,
      "evidence": [
        "Original: Stage k uses k latent tokens",
        "This implementation: Always max_n_latent iterations"
      ]
    },
    {
      "id": "DE008",
      "severity": "major",
      "category": "domain_knowledge",
      "title": "Generalization Claims are Premature",
      "description": "The results are from a single task type (multi-step arithmetic) at a single scale (38M) with a single dataset. Generalization to other tasks, scales, or datasets cannot be claimed. The COCONUT paper itself shows dramatic task-dependence (97% on ProsQA vs 34% on GSM8K). Any generalization claims require testing on multiple task types.",
      "location": "manuscript sections, results/",
      "recommendation": "Test on at least one additional task type (e.g., logical reasoning like ProntoQA, or text generation) before making any generalization claims.",
      "resolution_required": true,
      "evidence": [
        "Only one task tested: multi-step arithmetic",
        "COCONUT paper shows massive task variance",
        "No logical reasoning or language modeling evaluation"
      ]
    },
    {
      "id": "DE009",
      "severity": "minor",
      "category": "analysis",
      "title": "FLOP-Normalized Comparison Not Shown",
      "description": "COCONUT requires 5x forward passes (n_forward_passes: 5 in the results). The study shows absolute perplexity improvement but does not show FLOP-normalized improvement. A fair comparison should ask: 'Given the same compute budget, which approach is better?' The 5x overhead may not be justified by the -7.2% improvement.",
      "location": "results/v4.0_scaleup/*/config.json, flops_per_step fields",
      "recommendation": "Add FLOP-normalized comparison. If 5x compute only yields -7.2% improvement, compare to simply training a 5x larger baseline for the same FLOPs.",
      "resolution_required": false,
      "evidence": [
        "Baseline: 10.18B FLOPs/step, 1 forward pass",
        "COCONUT/ABC: 50.88B FLOPs/step, 5 forward passes",
        "5x compute for -7.2% improvement"
      ]
    },
    {
      "id": "DE010",
      "severity": "suggestion",
      "category": "domain_knowledge",
      "title": "Compare to Simpler Baselines",
      "description": "Before claiming COCONUT-specific benefits, compare to simpler baselines that also use extra compute: (a) ensemble of 5 forward passes with majority voting, (b) 5x longer training for baseline, (c) repeat predictions and average. These baselines would help isolate whether the benefit is from latent reasoning specifically or just from additional computation.",
      "location": "experimental design",
      "recommendation": "Add at least one compute-matched baseline (e.g., 5x training steps for baseline, or ensemble) to isolate the COCONUT contribution.",
      "resolution_required": false,
      "evidence": [
        "5x compute is a major confounder",
        "No compute-matched baseline provided"
      ]
    }
  ],
  "theoretical_analysis": {
    "scale_dependent_activation_hypothesis": {
      "explanation": "COCONUT's latent reasoning requires sufficient model capacity to learn the mapping from hidden states to meaningful reasoning steps. At small scales (7.5M), the model lacks the representation capacity to distinguish between different continuous thought states, leading to degradation. At 38M, the model may have just enough capacity for basic continuous thought to emerge, but this is still well below the validated COCONUT scale (124M).",
      "supporting_literature": [
        "Emergent Abilities of Large Language Models (Wei et al., 2022)",
        "Scaling Laws Under the Microscope (EMNLP 2022)",
        "COCONUT original paper's use of 124M GPT-2"
      ],
      "concerns": [
        "38M is still below validated COCONUT scale",
        "Task choice (arithmetic) is adverse for COCONUT",
        "Warm-start approach differs fundamentally"
      ]
    },
    "synergy_mechanistic_hypothesis": {
      "possible_explanations": [
        "MoD reduces noise in latent states by skipping irrelevant tokens, making continuous thought cleaner",
        "Memory provides external context that reduces the reasoning burden on latent iterations",
        "The combination acts as implicit regularization, preventing overfitting"
      ],
      "concerns": [
        "No theoretical basis in literature",
        "Could be statistical artifact (n=3 seeds, p=0.024 is marginal)",
        "Alternative explanations not ruled out"
      ]
    },
    "arithmetic_vs_logical_reasoning": {
      "why_coconut_fails_on_arithmetic": "Arithmetic requires precise symbolic manipulation where each digit matters. Continuous representations may blur digit boundaries, leading to carry/borrow errors. Discrete token spaces preserve exact numerical values.",
      "why_this_study_might_differ": [
        "Simpler arithmetic (5-9 steps) vs GSM8K word problems",
        "Smaller numbers might have more distinct representations",
        "The 'arithmetic' here may actually be pattern matching"
      ],
      "recommendation": "Examine actual errors to see if they are arithmetic errors or reasoning errors"
    }
  },
  "overall_assessment": "pass_with_conditions",
  "summary": "The v4.0 scale-up experiment shows promising results but has fundamental methodological concerns from a domain perspective. The choice to test COCONUT on arithmetic (where the original paper shows it underperforms) is puzzling and requires justification. The warm-start from task-trained baseline rather than pre-trained LM is a significant deviation from the COCONUT methodology. The reported improvements (-5.2% to -7.2%) are qualitatively opposite to expectations from the literature. While the scale-up from 7.5M to 38M showing activation is interesting, it is still well below the validated COCONUT scale (124M). The synergy claims lack mechanistic grounding. These concerns do not invalidate the results but significantly limit the conclusions that can be drawn about COCONUT specifically.",
  "conditions_for_proceeding": [
    "Acknowledge task choice conflicts with COCONUT literature",
    "Document warm-start methodology deviation",
    "Add FLOP-normalized comparisons",
    "Limit generalization claims to the specific setup tested"
  ]
}
