{
  "reviewer": "Statistician",
  "timestamp": "2026-02-13T23:45:00Z",
  "files_reviewed": [
    "papers/efficient_architecture_proof/manuscript/output/manuscript.md",
    "papers/efficient_architecture_proof/results/statistical_analysis.json",
    "papers/efficient_architecture_proof/results/selectivity_recomputed.json",
    "papers/efficient_architecture_proof/results/permutation_power.json",
    "papers/efficient_architecture_proof/results/unmatched_transplant.json",
    "papers/efficient_architecture_proof/results/cross_corruption.json",
    "papers/efficient_architecture_proof/results/appendix_data.json",
    "papers/efficient_architecture_proof/results/mcnemar_verification.json",
    "papers/efficient_architecture_proof/results/m3_test_eval.json",
    "papers/efficient_architecture_proof/results/m5_test_eval.json",
    "papers/efficient_architecture_proof/results/experiments/corruption/results.json",
    "papers/efficient_architecture_proof/results/experiments/probing/results.json",
    "papers/efficient_architecture_proof/results/experiments/ood/results.json",
    "papers/efficient_architecture_proof/results/experiments/mcnemar/results.json",
    "papers/efficient_architecture_proof/results/per_sample_correctness.json",
    "papers/efficient_architecture_proof/results/per_sample_logprobs.json"
  ],
  "findings": [
    {
      "id": "S001",
      "severity": "critical",
      "category": "numerical_accuracy",
      "description": "Internal accuracy discrepancy between Table 2 and Table 5 / experiment files for both M3 and M5. Table 2 reports M3 test accuracy as 98.0% (490/500 correct, best epoch 49, from m3_test_eval.json) and M5 as 95.6% (478/500, epoch 43, from m5_test_eval.json). However, Table 5 reports M3 ProsQA (ID) accuracy as 97.0% and M5 as 96.6%, verified by the McNemar contingency table (a=471, b=14, c=12, d=3; M3 correct = 471+14 = 485/500 = 97.0%, M5 correct = 471+12 = 483/500 = 96.6%). All experiment files (corruption clean baselines, OOD results, per-sample data) consistently use 97.0%/96.6%, not 98.0%/95.6%. These two sets of numbers cannot both be correct for the same models on the same 500-sample test set. The discrepancy is 5 correct answers for M3 (490 vs 485) and 5 for M5 (478 vs 483). This likely reflects different evaluation checkpoints, different decoding procedures, or a bug in one pipeline.",
      "location": "manuscript.md: Table 2 (line 124-128) vs Table 5 (line 202-208)",
      "evidence": "m3_test_eval.json: 490/500=0.98; mcnemar_verification.json prosqa contingency: a=471,b=14,c=12,d=3 -> M3=485/500=0.97; ood/results.json: m3.prosqa_test=0.97; corruption/results.json: m3.clean_accuracy=0.97. m5_test_eval.json: 478/500=0.956; contingency -> M5=483/500=0.966; ood/results.json: m5.prosqa_test=0.966",
      "recommendation": "Determine which accuracy numbers are correct and reconcile all tables. If different checkpoints or evaluation procedures were used, document this explicitly. If one is erroneous, correct it. The corruption experiments, probing, McNemar tests, and OOD evaluations all use the 97.0%/96.6% numbers, so those appear to be from the evaluation pipeline actually used in experiments. The 98.0%/95.6% numbers from m3_test_eval.json and m5_test_eval.json may be from a separate evaluation pass. All downstream claims (including the 85% gap closure) depend on which numbers are correct."
    },
    {
      "id": "S002",
      "severity": "major",
      "category": "numerical_accuracy",
      "description": "The 85% gap closure claim is numerically inaccurate under both possible accuracy scenarios. Using Table 2 numbers (M3=98.0%, M5=95.6%, M1=83.0%): (95.6-83.0)/(98.0-83.0) = 12.6/15.0 = 84.0%, which rounds to 84% not 85%. Using the experiment-consistent numbers (M3=97.0%, M5=96.6%, M1=83.0%): (96.6-83.0)/(97.0-83.0) = 13.6/14.0 = 97.1%, which would be a dramatically different claim. The paper consistently uses '85%' in the abstract, introduction, and conclusion.",
      "location": "manuscript.md: abstract (line 6), introduction (line 16), conclusion (line 287)",
      "evidence": "(95.6-83.0)/(98.0-83.0) = 84.0%, not 85%. (96.6-83.0)/(97.0-83.0) = 97.1%.",
      "recommendation": "If Table 2 numbers are authoritative, change '85%' to '84%' throughout. If experiment-pipeline numbers (97.0%/96.6%) are authoritative, the gap closure is 97%, which fundamentally changes the framing. Resolve the S001 discrepancy first, then update the gap closure percentage accordingly."
    },
    {
      "id": "S003",
      "severity": "major",
      "category": "effect_sizes",
      "description": "No standardized effect sizes or confidence intervals are reported for any comparison. The statistical_analysis.json file stores a field labeled 'd' that is simply the raw proportion difference (e.g., d=-0.094 for 7-hop M3 vs M5, which is just 0.66-0.754), not Cohen's d or any standardized measure. For binary outcomes with paired data, appropriate effect sizes would include odds ratios from the McNemar contingency tables (with CIs), Cramer's V, or Cohen's h for proportion differences. The manuscript reports raw percentage-point differences but never accompanies them with confidence intervals.",
      "location": "manuscript.md: Table 5 (line 202-208), Section 4.4 (line 196-210)",
      "evidence": "statistical_analysis.json effect_sizes section: 'd' values are raw proportion differences (e.g., m3_vs_m5.ood_7hop.d = -0.094 = 0.66-0.754). No CIs computed. No odds ratios. No Cohen's h.",
      "recommendation": "For each McNemar comparison, report odds ratios with 95% CIs computed from the contingency table: OR = c/b, CI via the exact conditional method or via log(OR) +/- 1.96*sqrt(1/b + 1/c). For proportion differences, report 95% CIs using the Newcombe method or bootstrap. For the probing comparisons, report bootstrap CIs on selectivity differences. This is standard practice for any quantitative paper and a basic requirement for publication."
    },
    {
      "id": "S004",
      "severity": "major",
      "category": "corrections",
      "description": "Bonferroni correction for OOD McNemar tests is too conservative, and the manuscript description of adjusted alpha is misleading. The manuscript states 'Bonferroni correction for the five comparisons (adjusted alpha = 0.01)' but does not use an adjusted alpha threshold. Instead, it multiplies p-values by k=5 and reports the corrected p-values in the table. This is equivalent, but the prose creates confusion by suggesting a threshold of 0.01. More importantly, Bonferroni is known to be overly conservative for correlated tests. The five test sets overlap in their training distribution and share the same models, so the tests are not independent. Holm-Bonferroni (step-down) would be more powerful while maintaining familywise error rate control, as the paper's own analysis_plan likely intended.",
      "location": "manuscript.md: Section 3.4 last paragraph (line 114), Table 5 (line 200-208)",
      "evidence": "Paper states 'adjusted alpha = 0.01' (0.05/5) but then reports Bonferroni-corrected p-values rather than comparing raw p-values to 0.01. For DAG: p_bonf = 0.001457 reported as 0.001, which is both imprecise and inconsistent with the '<0.001' format used for other tests.",
      "recommendation": "1) Use Holm-Bonferroni instead of Bonferroni, which would not change any conclusions but demonstrates methodological sophistication. 2) Be consistent: either report adjusted p-values OR report raw p-values against adjusted alpha, not a mixture. 3) Report the DAG Bonferroni p-value as 0.0015 rather than rounding to 0.001, since the paper uses 3 significant figures elsewhere."
    },
    {
      "id": "S005",
      "severity": "major",
      "category": "methodology",
      "description": "Table 4 caption incorrectly states that selectivity values for M3 positions 0 and 1 are reported 'at each model's peak probe accuracy layer: layer 0 for M3 positions 0, 1, 3.' The data shows that M3's peak probe accuracy for position 0 occurs at layer 8 (13.0% accuracy), not layer 0 (8.6%). Position 1 peaks at layer 12 (10.0%), not layer 0 (8.6%). Only position 3 genuinely peaks at layer 0 (55.4%). The selectivity values in the table (-15.6pp and -10.6pp for positions 0 and 1) are indeed from layer 0, but the caption's description of why layer 0 was chosen is factually incorrect.",
      "location": "manuscript.md: Table 4 caption (line 166-167)",
      "evidence": "probing/results.json: M3 diagonal_peak_layers = [8, 12, 12, 0], meaning pos 0 peaks at layer 8, pos 1 at layer 12, pos 2 at layer 12, pos 3 at layer 0. M3 probe accuracy at (layer 0, pos 0) = 8.6% vs (layer 8, pos 0) = 13.0%. M3 probe accuracy at (layer 0, pos 1) = 8.6% vs (layer 12, pos 1) = 10.0%.",
      "recommendation": "Correct the caption to accurately describe the layer selection rationale. If layer 0 was chosen because it's where recycled hidden states are injected (which is a valid architectural motivation), say that explicitly instead of claiming it's the peak accuracy layer. Alternatively, report selectivity at the actual peak layers (which would show near-zero selectivity for pos 0, changing the anti-selectivity narrative)."
    },
    {
      "id": "S006",
      "severity": "major",
      "category": "missing_analysis",
      "description": "No statistical test is used to formally compare corruption degradation profiles between M3 and M5. The paper claims 'identical degradation profiles' but this is asserted by visual inspection of parallel curves rather than tested. For each corruption level, the M3-M5 accuracy differences could be tested, or a more appropriate approach would be to fit parallel regression lines and test for a model x corruption interaction. With n=500 per condition, even small differences could be statistically significant. The current treatment amounts to informal 'looks similar' reasoning.",
      "location": "manuscript.md: Section 4.2 (lines 136-161)",
      "evidence": "Table 3 shows small numerical differences at each corruption level (e.g., 96.8% vs 96.4%, 57.4% vs 57.2%) that are never tested for significance.",
      "recommendation": "Fit a logistic regression with predictors {model, corruption_level, model*corruption_level} and test the interaction term. If the interaction is non-significant, this formally supports 'identical degradation profiles.' Alternatively, compute McNemar tests at each corruption level to compare the two models and report per-level significance."
    },
    {
      "id": "S007",
      "severity": "major",
      "category": "power",
      "description": "The single-seed limitation is acknowledged in Limitations but is more damaging than the discussion suggests. All reported accuracy numbers, effect sizes, and statistical tests are conditional on one random initialization. The 2.4pp test gap (or 0.4pp gap, depending on which numbers are correct) between M3 and M5 could easily reverse under a different seed. The OOD advantages for M5 (+7-9pp) could similarly be seed-dependent. Without multi-seed replication, it is impossible to compute proper confidence intervals or establish that any observed difference is robust. This is not merely a limitation -- it undermines the paper's central claim that curriculum explains COCONUT's performance, because the M5 result is a single draw from a stochastic training process.",
      "location": "manuscript.md: Section 6 Limitations, 'Single seed' paragraph (line 277)",
      "evidence": "statistical_analysis.json: n_seeds=1 throughout. All CI fields are null. paired_tests note: 'Paired tests require >= 2 seeds. Only 1 seed available.'",
      "recommendation": "At minimum, run 3-5 seeds per model to establish stability of the key findings. Report mean +/- SD across seeds. Compute proper paired t-tests or Wilcoxon signed-rank tests on seed-level accuracy. If additional seeds are not feasible, downgrade claims from 'M5 closes 85% of the gap' to 'In this single-seed replication, M5 closed X% of the gap' and explicitly state that robustness across seeds is unknown."
    },
    {
      "id": "S008",
      "severity": "minor",
      "category": "numerical_accuracy",
      "description": "M5 peak probe accuracy is reported inconsistently: Table 4 says 57.1% while the text in Section 4.3 says 57.0%. The backing data shows 0.5704697986577181, which is 57.0% when rounded to one decimal place (57.05% rounds to 57.1% under round-half-up convention, 57.0% under round-half-down or truncation).",
      "location": "manuscript.md: Table 4 (line 170) says 57.1%, text (line 168) says 57.0%",
      "evidence": "probing/results.json: m5.peak_accuracy = 0.5705 -> 57.05%. statistical_analysis.json: m5.peak_accuracy.mean = 0.5705",
      "recommendation": "Pick one rounding convention and apply it consistently. 57.0% or 57.1% are both defensible, but the table and text must agree."
    },
    {
      "id": "S009",
      "severity": "minor",
      "category": "test_selection",
      "description": "McNemar's test is correctly chosen for paired binary outcomes, and the exact (binomial) variant is appropriate given the moderate sample sizes. The justification for not using Wilcoxon or other continuous-outcome tests (different logit calibration across architectures) is sound. The test is well-specified and correctly implemented as verified by mcnemar_verification.json.",
      "location": "manuscript.md: Section 3.4 (line 114)",
      "evidence": "mcnemar_verification.json: all 5 tests pass verification, contingency tables reconstruct correctly, exact p-values match.",
      "recommendation": "No change needed. This is a positive finding from the review."
    },
    {
      "id": "S010",
      "severity": "minor",
      "category": "numerical_accuracy",
      "description": "DAG Bonferroni p-value is reported as '0.001' in Table 5, but the exact value is 0.001457 (5 * 0.000291). This rounds to 0.0015 at 4 significant figures, not 0.001. While not consequential for significance (still < 0.01), the rounding creates an impression of greater significance than warranted and is inconsistent with the precision used for other p-values in the table.",
      "location": "manuscript.md: Table 5, DAG row (line 207)",
      "evidence": "mcnemar/results.json: dag p_exact=0.00029139, p_bonferroni=0.00145697. mcnemar_verification.json: verified_p_bonferroni=0.001457. appendix_data.json: dag_bonferroni_p=0.001457, dag_bonferroni_rounded=0.001",
      "recommendation": "Report as 0.0015 or as '< 0.002' to maintain consistent precision. The appendix_data.json file itself notes this rounding (dag_bonferroni_rounded=0.001), suggesting awareness of the approximation."
    },
    {
      "id": "S011",
      "severity": "minor",
      "category": "assumptions",
      "description": "McNemar's exact test (binomial test on discordant pairs) makes no distributional assumptions beyond independence of observations, which is satisfied here since each test sample is evaluated independently. The test is appropriate. However, the independence assumption across the five test sets could be questioned: the same trained models are evaluated on all five, introducing correlation. Bonferroni correction handles this conservatively, but a multivariate approach could be more efficient.",
      "location": "manuscript.md: Section 3.4 (line 114)",
      "evidence": "The same M3 and M5 model weights produce predictions on all 5 test sets.",
      "recommendation": "Acknowledge in the methods section that the five tests are correlated through shared model parameters, and note that Bonferroni is conservative for this reason. No change to conclusions needed."
    },
    {
      "id": "S012",
      "severity": "minor",
      "category": "methodology",
      "description": "The probing selectivity metric (matched probe accuracy minus max cross-position probe accuracy) is a reasonable measure of position-specificity but has no established statistical test. The 0.3pp difference between M3 and M5 at position 3 is described as 'smaller than the typical standard deviation of 5-fold cross-validation estimates at this sample size (n=298), though we do not report per-fold variance.' Reporting per-fold variance and a formal comparison (e.g., paired permutation test on CV folds) would strengthen this claim.",
      "location": "manuscript.md: Section 4.3 (line 178)",
      "evidence": "No per-fold variance reported anywhere in the data files. The selectivity comparison between models is informal.",
      "recommendation": "Report per-fold standard deviations for the key probing results (at minimum, position 3 for both models). Compute a permutation test or bootstrap confidence interval for the M3-M5 selectivity difference to formally establish that 0.3pp is not significantly different from zero."
    },
    {
      "id": "S013",
      "severity": "minor",
      "category": "interpretation",
      "description": "The manuscript describes the permutation experiment as providing 'strong evidence that permutation does not meaningfully alter model outputs' and excludes 'a true flip rate above 0.06% at 95% confidence.' The power analysis in permutation_power.json is correctly computed: with 5000 trials and 0 flips, the 95% upper bound is 0.000599. However, the analysis considers only prediction flips (changed answers), not accuracy changes. If permutation degraded the quality of intermediate representations but the model could still recover the correct answer through other mechanisms, flips would not occur. The permutation experiment tests order-dependence of the final answer, not order-dependence of intermediate representations.",
      "location": "manuscript.md: Section 4.2 'Permutation sensitivity' (line 158)",
      "evidence": "permutation_power.json: n_trials=5000, observed_flips=0, min_detectable_95=0.000599",
      "recommendation": "Clarify that the permutation test measures order-dependence of the final prediction, not of intermediate computations. The distinction matters because thought tokens could carry order-sensitive intermediate states that nonetheless produce the same final answer. This subtlety is partially addressed by the probing experiment but should be noted here."
    },
    {
      "id": "S014",
      "severity": "suggestion",
      "category": "missing_analysis",
      "description": "A Bayesian analysis would strengthen the paper's negative claims. Several key conclusions rest on null results (no difference between M3 and M5 on corruption profiles, permutation sensitivity, in-distribution accuracy, selectivity profiles). Frequentist tests can only fail to reject the null; they cannot provide evidence for it. Bayes factors for the McNemar comparison on ProsQA (ID), for the corruption profile equivalence, and for the selectivity difference would quantify the evidence favoring the null over the alternative.",
      "location": "manuscript.md: throughout Results (Section 4)",
      "evidence": "statistical_analysis.json: bayesian fields not computed.",
      "recommendation": "Compute Bayes factors for the key null comparisons: (1) McNemar on ProsQA in-distribution (currently p=0.845, suggesting null, but BF would quantify this); (2) corruption profile equivalence (e.g., a Bayesian test of the interaction in a logistic model); (3) selectivity difference at position 3 (0.3pp, which would likely yield BF > 3 favoring the null). Even reporting BF10 for the ProsQA comparison would add substantial evidential value."
    },
    {
      "id": "S015",
      "severity": "suggestion",
      "category": "missing_analysis",
      "description": "No formal equivalence test (TOST procedure) is used for the key null claims. The paper's central argument is that M3 and M5 are functionally equivalent on several diagnostics. A two one-sided tests (TOST) procedure with a prespecified equivalence margin would provide affirmative evidence of equivalence rather than merely failing to find a difference. For example, on ProsQA accuracy: defining a margin of +/-3pp, a TOST test could formally demonstrate equivalence.",
      "location": "manuscript.md: Section 4.1 (line 120), Section 4.2 (line 134)",
      "evidence": "N/A -- TOST not conducted",
      "recommendation": "For the in-distribution accuracy comparison and the corruption profile comparison, conduct TOST with a clinically/practically meaningful equivalence margin (e.g., +/-3pp for accuracy, +/-5pp for selectivity). This directly addresses the logical gap between 'not significantly different' and 'equivalent.'"
    },
    {
      "id": "S016",
      "severity": "suggestion",
      "category": "interpretation",
      "description": "The paper claims M1 'performs near chance on all OOD test sets (8.2%-28.2%)' but does not specify what chance-level performance is. For a two-choice task, chance is 50%. M1's 28.2% on DAG and 14.1% on dense are actually below chance, which requires explanation. If the model outputs answers in a format that sometimes fails to parse, or if there is a systematic bias, this should be discussed. 8.2% on 8-hop is dramatically below the 50% chance baseline.",
      "location": "manuscript.md: Section 4.4 last paragraph (line 218)",
      "evidence": "Table 5: M1 DAG=28.2%, M1 Dense=14.1%, M1 8-hop=8.2%. Two-choice task chance = 50%.",
      "recommendation": "Remove or qualify the 'near chance' characterization. M1's below-chance performance suggests systematic incorrect answering (e.g., consistently choosing the distractor, parsing failures, or the CoT directing the model to wrong paths). Discuss why performance is below 50% -- this is informative about how explicit CoT fails under distribution shift."
    },
    {
      "id": "S017",
      "severity": "suggestion",
      "category": "missing_analysis",
      "description": "The transplant experiment (200 pairs) has no formal statistical test. The paper reports M3=97.0% and M5=96.5% under matched transplant, and M3=97.5% and M5=96.5% under unmatched transplant, comparing informally to clean baselines. A formal test (e.g., McNemar comparing clean vs transplanted accuracy on the same 200 samples, or a binomial test of the transplant success rate) would strengthen the claim that transplant accuracy is not degraded.",
      "location": "manuscript.md: Section 4.2 'Cross-problem transplantation' (line 160)",
      "evidence": "unmatched_transplant.json: m3_unmatched=0.975, m5_unmatched=0.965, n=200",
      "recommendation": "Conduct a paired McNemar test comparing each model's accuracy on the 200 test samples under clean vs. transplant conditions. Alternatively, report the transplant success rate with an exact binomial CI."
    },
    {
      "id": "S018",
      "severity": "suggestion",
      "category": "methodology",
      "description": "The nonlinear probe results (0/78 cells showing MLP advantage over linear probes) are reported but the MLP probes used default scikit-learn hyperparameters, as acknowledged in Appendix A.7. With default MLPClassifier settings and potentially insufficient training iterations, the null result may reflect convergence failure rather than absence of nonlinear structure. This limitation is partially acknowledged but could be addressed more rigorously.",
      "location": "manuscript.md: Appendix A.7 (line 389-391)",
      "evidence": "probing/results.json: all nonlinear_probe_accuracy entries are 0.0 for both models across all 78 cells.",
      "recommendation": "The fact that ALL 78 cells show exactly 0.0 for nonlinear probes (rather than some being positive and comparable to linear) strongly suggests a bug or convergence failure in the MLP probe training, not a genuine null result. Zero accuracy on a classification task (even a difficult multiclass one) is suspicious -- random performance should be nonzero. Investigate whether the MLP probes were trained correctly. If they genuinely returned 0.0 accuracy, this needs explanation."
    },
    {
      "id": "S019",
      "severity": "minor",
      "category": "numerical_accuracy",
      "description": "L2 distance for M3 noise is reported as 202.65 in the manuscript and corruption/results.json, but as 202.8 in cross_corruption.json. This minor discrepancy likely reflects different random seeds for noise generation across experiment runs, but should be acknowledged or reconciled.",
      "location": "manuscript.md: Section 4.2 (line 101)",
      "evidence": "corruption/results.json: replacement_l2_distance=202.65. cross_corruption.json: m3_noise_l2_mean=202.8",
      "recommendation": "Note in the methods that L2 distance varies slightly across noise draws (~202-203 for M3-scale noise) and report the range rather than a single value, or use the same noise seed for both experiments."
    },
    {
      "id": "S020",
      "severity": "minor",
      "category": "numerical_accuracy",
      "description": "The old approximate McNemar results in statistical_analysis.json (computed from aggregate accuracy, not per-sample data) are starkly different from the correct per-sample exact tests in mcnemar/results.json and mcnemar_verification.json. For DAG: approximate p_corrected=0.121 (not significant) vs exact p_corrected=0.00146 (significant). The manuscript correctly uses the exact tests, but the statistical_analysis.json file contains misleading approximate results with a warning flag. This creates an audit trail inconsistency.",
      "location": "statistical_analysis.json vs mcnemar/results.json",
      "evidence": "statistical_analysis.json DAG: chi2=5.076, p=0.0243, p_corrected=0.1213, significant=false. mcnemar/results.json DAG: exact p=0.00029, p_bonf=0.00146, significant=true.",
      "recommendation": "Update statistical_analysis.json to use the correct exact McNemar results, or add a clear deprecation notice. The current state where the summary file says 'not significant' while the detailed file says 'significant' could mislead anyone examining the data."
    },
    {
      "id": "S021",
      "severity": "major",
      "category": "interpretation",
      "description": "The claim that 'three converging experiments fail to distinguish the systems' conflates in-distribution and out-of-distribution evidence. The OOD experiment clearly distinguishes M3 and M5 with large, significant differences on all four OOD sets. The claim of indistinguishability applies only to in-distribution evaluation and the corruption/probing experiments (which are also in-distribution). The abstract and conclusion should more carefully scope this claim to in-distribution behavior.",
      "location": "manuscript.md: abstract (line 6), Section 5.1 (line 224-237)",
      "evidence": "Table 5: All four OOD comparisons are significant with 7-9pp differences.",
      "recommendation": "Revise the abstract from 'Three converging experiments fail to distinguish the systems' to something like 'Three converging in-distribution experiments fail to distinguish the systems, while OOD evaluation reveals a systematic task-dependent generalization tradeoff.' The current phrasing in the abstract is misleading because it implies all experiments showed no difference."
    },
    {
      "id": "S022",
      "severity": "minor",
      "category": "methodology",
      "description": "The selectivity metric uses max over cross-position accuracies rather than mean. This choice maximizes the contrast (yielding higher selectivity) but the max is more sensitive to noise at individual positions. Using the mean cross-position accuracy as the control baseline would be more robust, though it would also reduce selectivity values. The choice is not inherently wrong but should be justified.",
      "location": "manuscript.md: Section 3.4 Experiment 2 (line 105)",
      "evidence": "selectivity(l,t) = probe_acc(target=step_t) - max_{s!=t} probe_acc(target=step_s). The max operation selects the single highest alternative as the control.",
      "recommendation": "Briefly justify why max is used rather than mean. The max gives a conservative estimate of selectivity (if a position encodes step_t better than even the best alternative, it truly encodes step_t specifically). This is actually the right choice -- just make the reasoning explicit."
    },
    {
      "id": "S023",
      "severity": "suggestion",
      "category": "missing_analysis",
      "description": "The probing results for positions 4 (n=81) and 5 (n=12) show 0.0% accuracy across all layers and both models. With 38+ entity classes and n=81, random-level accuracy should be around 1/38 = 2.6%. The 0.0% results suggest either the probe fails to converge at low n, the label set is extremely large relative to n, or there is a data issue. Position 5 (n=12) with dozens of classes is expected to produce 0.0%, but position 4 (n=81) deserves investigation.",
      "location": "manuscript.md: Table A5, A6 (lines 353-387), Section 4.3 (line 164)",
      "evidence": "All probing accuracy values for positions 4 and 5 are 0.0 in probing/results.json for both models and both linear and nonlinear probes.",
      "recommendation": "Check whether the probing code handles small-n positions correctly. For position 4 (n=81 samples, 5-fold CV = ~65 train / ~16 test), the number of unique entity labels at step 4 may exceed the training set size, making classification impossible. Report the number of unique target classes per position to contextualize the 0.0% results."
    }
  ],
  "numerical_verification": {
    "claims_checked": 47,
    "claims_verified": 40,
    "discrepancies": [
      {
        "claim": "M3 test accuracy = 98.0% (Table 2)",
        "data_value": "97.0% in all experiment files, 98.0% only in m3_test_eval.json",
        "severity": "critical",
        "finding_ref": "S001"
      },
      {
        "claim": "M5 test accuracy = 95.6% (Table 2)",
        "data_value": "96.6% in all experiment files, 95.6% only in m5_test_eval.json",
        "severity": "critical",
        "finding_ref": "S001"
      },
      {
        "claim": "M5 closes 85% of the gap",
        "data_value": "84.0% using Table 2 numbers, 97.1% using experiment numbers",
        "severity": "major",
        "finding_ref": "S002"
      },
      {
        "claim": "M5 Peak probe accuracy = 57.1% (Table 4)",
        "data_value": "57.0% in text (line 168); data = 0.5705 = 57.05%",
        "severity": "minor",
        "finding_ref": "S008"
      },
      {
        "claim": "DAG Bonferroni p = 0.001 (Table 5)",
        "data_value": "0.001457 (mcnemar_verification.json)",
        "severity": "minor",
        "finding_ref": "S010"
      },
      {
        "claim": "Layer 0 is peak probe accuracy layer for M3 positions 0, 1 (Table 4 caption)",
        "data_value": "Peak for pos 0 is layer 8 (13.0%), peak for pos 1 is layer 12 (10.0%)",
        "severity": "major",
        "finding_ref": "S005"
      },
      {
        "claim": "L2 distance for M3 noise = 202.65",
        "data_value": "202.65 in corruption/results.json, 202.8 in cross_corruption.json",
        "severity": "minor",
        "finding_ref": "S019"
      }
    ]
  },
  "overall_assessment": "revise",
  "summary": "The paper makes a compelling case for its central thesis that COCONUT's training curriculum, rather than its hidden-state recycling mechanism, drives in-distribution performance on ProsQA. The experimental design is creative and multi-faceted, the McNemar tests are correctly implemented and verified, and the permutation power analysis is sound. However, several issues require resolution before the manuscript can be considered statistically rigorous.\n\nThe most critical issue is the internal accuracy discrepancy between Table 2 (which provides the numbers for the paper's flagship '85% gap closure' claim) and Table 5 / all experiment files (which provide the numbers actually used in all analyses). M3 is reported as 98.0% in Table 2 but 97.0% in every experiment; M5 is 95.6% in Table 2 but 96.6% in every experiment. These cannot both be correct for the same model on the same data. This discrepancy cascades into the gap closure calculation and potentially undermines the quantitative framing of the entire paper. If the experiment-pipeline numbers (97.0%/96.6%) are correct, M5 closes 97% of the gap -- a substantially stronger result than reported.\n\nBeyond this critical issue, the paper lacks standardized effect sizes and confidence intervals for all comparisons, does not formally test the equivalence claims (corruption profile identity, selectivity equivalence) with appropriate methods (TOST, Bayes factors), and has a misleading Table 4 caption regarding layer selection for selectivity reporting. The single-seed limitation is severe for a paper making strong claims about curriculum-driven performance. The nonlinear probe results (all zeros across 156 cells) appear suspicious and warrant investigation. These issues are addressable through additional analyses and corrections rather than new experiments, with the exception of the multi-seed recommendation which would require additional compute."
}
