======================================================================
OUTPUT FRICTION IN TOOL CALLING — ANALYSIS REPORT
======================================================================
Generated: 2026-02-03 14:29:48
Data file: experiments/results/signal_detection_20260203_121413.json

======================================================================
DETECTION RATES (LLM Judge — Both Conditions)
======================================================================

NL detection:         87.0% (322/370)
Structured detection: 81.4% (301/370)
Difference:           +5.7pp
McNemar p:            0.005
Sign test:            8 vs 3, p = 0.227

→ Format does not affect detection.

======================================================================
OUTPUT FRICTION (Structured Condition Only) — PRIMARY FINDING
======================================================================

Structured detection (judge):    81.4% (301/370)
Structured compliance (XML):     69.2% (256/370)
Output friction gap:             12.2pp (60 silent failures)

→ 1 in 6 detections fails to produce a tool call.

======================================================================
SILENT FAILURE ANALYSIS
======================================================================

Silent failures (judge=Yes, XML=No): 60/370

These are trials where the model detected the signal
(per LLM judge) but failed to produce XML for the parser.
From a production system's perspective, these are missed
tool calls — the signal was understood but not acted upon.

======================================================================
FALSE POSITIVE RATES (Control Scenarios)
======================================================================

Control scenarios: 230

                        JUDGE           REGEX
--------------------------------------------------
  NL FP rate:          0.0%            0.4%
  ST FP rate:          2.2%            0.0%

======================================================================
AMBIGUITY BREAKDOWN
======================================================================

              Detection(judge)  Compliance(XML)  Friction
------------------------------------------------------------
EXPLICIT      100.0%              100.0%             +0.0pp  (n=150)
IMPLICIT      68.6%              48.2%             +20.5pp  (n=220)

→ Does friction increase with ambiguity?
   Yes: EXPLICIT (+0.0pp) < IMPLICIT (+20.5pp)

======================================================================
MEASUREMENT COMPARISON (Methodological)
======================================================================

This comparison shows why cross-condition analysis is problematic.
The within-condition gap (Section 2) is the appropriate metric.

                Regex scoring    Judge scoring
--------------------------------------------------
NL recall:      76.2%            87.0%
ST recall:      69.2%            81.4%
NL-ST gap:      +7.0pp           +5.7pp
McNemar p:      0.0006          0.0046

→ Cross-condition gap was measurement artifact.
   Within-condition gap (Section 2) is the real finding.

======================================================================
JUDGE VALIDATION
======================================================================

Judge-human κ:           [PENDING ANNOTATION]
Agreement rate:          [PENDING ANNOTATION]

To complete validation:
  1. Annotate: experiments/results/validation_annotation_*.csv
  2. Run: python experiments/compute_agreement.py <annotated.csv> <key.csv>
  3. Target: κ ≥ 0.80 for substantial agreement

======================================================================
END OF REPORT
======================================================================